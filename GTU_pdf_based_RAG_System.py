# -*- coding: utf-8 -*-
"""gtupdfex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iVXo9ZSpLOAZQnweWRMEk2tKRzGolF8F
"""

!pip install PyMuPDF pdfplumber pytesseract pdf2image beautifulsoup4 chromadb faiss-cpu langchain

"""### **Read the pdf file !!**"""

import fitz

def extract_text_from_pdf(path):
    doc = fitz.open(path)
    text = ""
    for page in doc:
        text += page.get_text("text") + "\n"
    return text

path = "/content/gtu_study_materialDataset.pdf"
text_data = extract_text_from_pdf(path)
text_data

"""# **Extract Image from PDF :**"""

import fitz
import os

def extract_images_from_pdf(path, output_folder="Images"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(path)
    image_count = 0

    for page_num in range(len(doc)):
        for img_index, img in enumerate(doc[page_num].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            image_filename = os.path.join(output_folder, f"page{page_num+1}_img{img_index+1}.{image_ext}")
            with open(image_filename, "wb") as img_file:
                img_file.write(image_bytes)

            image_count += 1

    print(f"✅ Extracted {image_count} images and saved in '{output_folder}' folder.")

path = "/content/gtu_study_materialDataset.pdf"
extract_images_from_pdf(path)

def extract_links_from_pdf(path):
    doc = fitz.open(path)
    links = []
    for page in doc:
        links += [link["uri"] for link in page.get_links() if "uri" in link]
    return links



links_data = extract_links_from_pdf(path)


def save_links_to_txt(links, filename="extracted_links.txt"):
    with open(filename, "w", encoding="utf-8") as file:
        for link in links:
            file.write(link + "\n")
    print(f"✅ Links saved to {filename}")

# Save links
save_links_to_txt(links_data)

links_data

"""# **Clean the Data :**"""

import re

def clean_text(text_data):
    text_data = re.sub(r'[\uf0b7●•▪▶]', '', text_data)
    text_data = re.sub(r'\s+', ' ', text_data).strip()
    text_data = re.sub(r'\s+([.,!?;:])', r'\1', text_data)

    return text_data

cleaned_text = clean_text(text_data)
print(cleaned_text)

"""# **Split Data into chunks :**"""

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=40,
     length_function=len
    )

document = text_splitter.create_documents([cleaned_text])
document

"""# **Convert Text Chunks into Embeddings**"""

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embedding = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004",google_api_key="AIzaSyBW_ps9Mjmx kjvnqKQyDnj6FS3m9N-4sok")
embedding.client.timeout = 120

from langchain_chroma import Chroma

db = Chroma.from_documents(document, embedding)

db

from sentence_transformers import SentenceTransformer

# ✅ Load SentenceTransformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# ✅ Example documents
documents = [
    Document(page_content="This is the first document."),
    Document(page_content="Here is another document."),
    Document(page_content="ChromaDB is useful for storing vectors."),
]

# ✅ Generate embeddings for each document
embeddings = [model.encode(doc.page_content).tolist() for doc in document]

# ✅ Extract texts and metadata
texts = [doc.page_content for doc in documents]
metadatas = [{"source": f"doc_{i}"} for i in range(len(document))]



