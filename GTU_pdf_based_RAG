{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRvkkvTmNl0y",
        "outputId": "38421bc8-7eb3-4878-e74d-d0ddf7d94ba2",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.10.6)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.47)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.18)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.29.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=a4211505f27b25e85c78f00ed150ef2300e180d5e87217fb7a4aff9c535af1f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pytesseract, pyproject_hooks, pypdfium2, PyMuPDF, pdf2image, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, faiss-cpu, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, build, pdfminer.six, onnxruntime, kubernetes, fastapi, pdfplumber, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "Successfully installed PyMuPDF-1.25.4 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 durationpy-0.9 faiss-cpu-1.10.0 fastapi-0.115.12 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-util-http-0.52b1 overrides-7.7.0 pdf2image-1.17.0 pdfminer.six-20250327 pdfplumber-0.11.6 posthog-3.23.0 pypdfium2-4.30.1 pypika-0.48.9 pyproject_hooks-1.2.0 pytesseract-0.3.13 python-dotenv-1.1.0 starlette-0.46.1 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF pdfplumber pytesseract pdf2image beautifulsoup4 chromadb faiss-cpu langchain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO7Ei1VxSsJB"
      },
      "source": [
        "### **Read the pdf file !!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "Z2vvsM3OSOLl",
        "outputId": "0016038e-82ad-496c-faf7-970eaa13bab2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nCloud Computing \\n\\uf0b7 \\nCloud computing is the delivery of on-demand computing services, from applications to storage and \\nprocessing power, typically over the internet and on a pay-as-you-go basis. \\nIntroduction to Cloud Computing \\n \\nFig. Cloud Computing \\n\\uf0b7 \\nCloud Computing is the delivery of computing services such as servers, storage, databases, networking, \\nsoftware, analytics, intelligence, and more, over the Cloud (Internet). \\n\\uf0b7 \\nCloud Computing provides an alternative to the on-premises datacenter.  \\n\\uf0b7 \\nWith an on-premises datacenter, we have to manage everything, such as purchasing and installing \\nhardware, virtualization, installing the operating system, and any other required applications, setting up \\nthe network, configuring the firewall, and setting up storage for data.  \\n\\uf0b7 \\nAfter doing all the set-up, we become responsible for maintaining it through its entire lifecycle. \\n\\uf0b7 \\nBut if we choose Cloud Computing, a cloud vendor is responsible for the hardware purchase and \\nmaintenance.  \\n\\uf0b7 \\nThey also provide a wide variety of software and platform as a service.  \\n\\uf0b7 \\nWe can take any required services on rent.  \\n\\uf0b7 \\nThe cloud computing services will be charged based on usage. \\n\\uf0b7 \\nThe cloud environment provides an easily accessible online portal that makes handy for the user to \\nmanage the compute, storage, network, and application resources. \\nCharacteristics (Features) of Cloud Computing \\nThe five essential characteristics of cloud computing: \\n1. On-demand self-service: A consumer can separately provision computing capabilities, such as server \\ntime and network storage, as needed automatically without requiring human interaction with each \\nservice provider. \\n2. Broad network access: Capabilities are available over the network and accessed through standard \\nmechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, \\ntablets, laptops and workstations). \\n3. Resource pooling: The provider\\'s computing resources are pooled to serve multiple consumers using a \\nmulti-tenant model, with different physical and virtual resources dynamically assigned and reassigned \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n2 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\naccording to consumer demand. There is a sense of location independence in that the customer \\ngenerally has no control or knowledge over the exact location of the provided resources but may be able \\nto specify location at a higher level of abstraction (e.g., country, state or datacenter). Examples of \\nresources include storage, processing, memory and network bandwidth. \\n4. Rapid elasticity: Capabilities can be elastically provisioned and released, in some cases automatically, to \\nscale rapidly outward and inward matching with demand. To the consumer, the capabilities available for \\nprovisioning often appear to be unlimited and can be appropriated in any quantity at any time. \\n5. Measured service: Cloud systems automatically control and optimize resource use by leveraging a \\nmetering capability at some level of abstraction appropriate to the type of service (e.g., storage, \\nprocessing, bandwidth and active user accounts). Resource usage can be monitored, controlled and \\nreported, providing transparency for the provider and consumer. \\nAdvantages of Cloud Computing \\n\\uf0b7 \\nCost: It reduces the huge capital costs of buying hardware and software. \\n\\uf0b7 \\nSpeed: Resources can be accessed in minutes, typically within a few clicks. \\n\\uf0b7 \\nScalability: We can increase or decrease the requirement of resources according to the business \\nrequirements. \\n\\uf0b7 \\nProductivity: While using cloud computing, we put less operational effort. We do not need to apply \\npatching, as well as no need to maintain hardware and software. So, in this way, the IT team can be \\nmore productive and focus on achieving business goals. \\n\\uf0b7 \\nReliability: Backup and recovery of data are less expensive and very fast for business continuity. \\n\\uf0b7 \\nSecurity: Many cloud vendors offer a broad set of policies, technologies, and controls that strengthen \\nour data security. \\nDisadvantages of Cloud Computing \\n\\uf0b7 \\nRequires good speed internet with good bandwidth: To access your cloud services, you need to have a \\ngood internet connection always with good bandwidth to upload or download files to/from the cloud \\n\\uf0b7 \\nDowntime: Since the cloud requires high internet speed and good bandwidth, there is always a \\npossibility of service outage, which can result in business downtime. Today, no business can afford \\nrevenue or business loss due to downtime or slow down from an interruption in critical business \\nprocesses. \\n\\uf0b7 \\nLimited control of infrastructure: Since you are not the owner of the infrastructure of the cloud, hence \\nyou don’t have any control or have limited access to the cloud infra. \\n\\uf0b7 \\nRestricted or limited flexibility: The cloud provides a huge list of services, but consuming them comes \\nwith a lot of restrictions and limited flexibility for your applications or developments. Also, platform \\ndependency or ‘vendor lock-in’ can sometimes make it difficult for you to migrate from one provider to \\nanother. \\n\\uf0b7 \\nOngoing costs: Although you save your cost of spending on whole infrastructure and its management, \\non the cloud, you need to keep paying for services as long as you use them. But in traditional methods, \\nyou only need to invest once. \\n\\uf0b7 \\nSecurity: Security of data is a big concern for everyone. Since the public cloud utilizes the internet, your \\ndata may become vulnerable. In the case of a public cloud, it depends on the cloud provider to take care \\nof your data. So, before opting for cloud services, it is required that you find a provider who follows \\nmaximum compliance policies for data security. \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n3 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nVendor Lock-in: Although the cloud service providers assure you that they will allow you to switch or \\nmigrate to any other service provider whenever you want, it is a very difficult process. You will find it \\ncomplex to migrate all the cloud services from one service provider to another. During migration, you \\nmight end up facing compatibility, interoperability and support issues. To avoid these issues, many \\ncustomers choose not to change the vendor. \\n\\uf0b7 \\nTechnical issues: Even if you are a tech whiz, the technical issues can occur, and everything can’t be \\nresolved in-house. To avoid interruptions, you will need to contact your service provider for support. \\nHowever, not every vendor provides 24/7 support to their clients. \\nDifference between Conventional Computing and Cloud Computing \\nConventional Computing \\nCloud Computing \\nIn conventional computing environment more time \\nis needed for installation, set up, and configuration. \\nOnce the cloud computing environment is set up \\ninitially, you can gain access faster than conventional \\ncomputing \\nCost must be paid in advance \\nPay-as-you-go \\nCost is fixed \\nCost is variable \\nEconomic to scale for all organization \\nEconomic to scale for large organization only \\nFor Scaling manual effort is needed \\nScaling can be elastic and automatic \\nEnvironment is mix of physical and virtualized \\nUsually environment is virtualized \\nHistory of Cloud Computing \\n\\uf0b7 \\nBefore emerging the cloud computing, there was Client/Server computing which is basically a centralized \\nstorage in which all the software applications, all the data and all the controls are resided on the server \\nside. \\n\\uf0b7 \\nIf a single user wants to access specific data or run a program, he/she need to connect to the server and \\nthen gain appropriate access, and then he/she can do his/her business. \\n\\uf0b7 \\nThen after, distributed computing came into picture, where all the computers are networked together \\nand share their resources when needed. \\n\\uf0b7 \\nOn the basis of above computing, there was emerged of cloud computing concepts that later \\nimplemented. \\n\\uf0b7 \\nAt around in 1961, John MacCharty suggested in a speech at MIT that computing can be sold like a \\nutility, just like a water or electricity.  \\n\\uf0b7 \\nIt was a brilliant idea, but like all brilliant ideas, it was ahead of its time, as for the next few decades, \\ndespite interest in the model, the technology simply was not ready for it. \\n\\uf0b7 \\nBut of course time has passed and the technology caught that idea and after few years we mentioned \\nthat: \\no In 1999, Salesforce.com started delivering of applications to users using a simple website. The \\napplications were delivered to enterprises over the Internet, and this way the dream of computing \\nsold as utility were true. \\no In 2002, Amazon started Amazon Web Services, providing services like storage, computation and \\neven human intelligence. However, only starting with the launch of the Elastic Compute Cloud in \\n2006 a truly commercial service open to everybody existed. \\no In 2009, Google Apps also started to provide cloud computing enterprise applications. \\no Of course, all the big players are present in the cloud computing evolution, some were earlier and \\nsome were later. In 2009, Microsoft launched Windows Azure, and companies like Oracle and HP \\nhave all joined the game. This proves that today, cloud computing has become mainstream. \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n4 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nCloud Orchestration \\n\\uf0b7 \\nCloud Orchestration is a way to manage, co-ordinate, and provision all the components of a cloud \\nplatform automatically from a common interface.  \\n\\uf0b7 \\nIt orchestrates the physical as well as virtual resources of the cloud platform.  \\n\\uf0b7 \\nCloud orchestration is a must because cloud services scale up arbitrarily and dynamically, include \\nfulfillment assurance and billing, and require workflows in various business and technical domains. \\n\\uf0b7 \\nOrchestration tools combine automated tasks by interconnecting the processes running across the \\nheterogeneous platforms in multiple locations.  \\n\\uf0b7 \\nOrchestration tools create declarative templates to convert the interconnected processes into a single \\nworkflow.  \\n\\uf0b7 \\nThe processes are so orchestrated that the new environment creation workflow is achieved with a single \\nAPI call.  \\n\\uf0b7 \\nCreation of these declarative templates, though complex and time consuming, is simplified by the \\norchestration tools. \\n\\uf0b7 \\nCloud orchestration includes two types of models:  \\no Single Cloud model \\no Multi-cloud model \\n\\uf0b7 \\nIn Single cloud model, all the applications designed for a system run on the same IaaS platform (same \\ncloud service provider).  \\n\\uf0b7 \\nApplications, interconnected to create a single workflow, running on various cloud platforms for the \\nsame organization define the concept of multi-cloud model.  \\n\\uf0b7 \\nIaaS requirement for some applications, though designed for same system, might vary. This results in \\navailing services of multiple cloud service providers.  \\n\\uf0b7 \\nFor example, application with patient’s sensitive medical data might reside in some IaaS, whereas the \\napplication for online OPD appointment booking might reside in another IaaS, but they are \\ninterconnected to form one system. This is called multi-cloud orchestration.  \\n\\uf0b7 \\nMulti-cloud models provide high redundancy as compared to single IaaS deployments.  \\n\\uf0b7 \\nThis reduces the risk of down time. \\nElasticity in Cloud \\n\\uf0b7 \\nElasticity covers the ability to scale up but also the ability to scale down.  \\n\\uf0b7 \\nThe idea is that you can quickly provision new infrastructure to handle a high load of traffic.  \\n\\uf0b7 \\nBut what happens after that rush? If you leave all of these new instances running, your bill will skyrocket \\nas you will be paying for unused resources.  \\n\\uf0b7 \\nIn the worst case scenario, these resources can even cancel out revenue from the sudden rush.  \\n\\uf0b7 \\nAn elastic system prevents this from happening. After a scaled up period, your infrastructure can scale \\nback down, meaning you will only be paying for your usual resource usage and some extra for the high \\ntraffic period. \\n\\uf0b7 \\nThe key is that this all happens automatically.  \\n\\uf0b7 \\nWhen resource needs meet a certain threshold (usually measured by traffic), the system “knows” that it \\nneeds to de-provision a certain amount of infrastructure, and does so.  \\n\\uf0b7 \\nWith a couple hours of training, anyone can use the AWS web console to manually add or subtract \\ninstances.  \\n\\uf0b7 \\nBut it takes a true Solutions Architect to set up monitoring, account for provisioning time, and configure \\na system for maximum elasticity. \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n5 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nCloud Service Options / Cloud Service Models / Cloud Computing Stack \\n \\nFig. : Cloud Services \\n\\uf0b7 \\nCloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared \\npool of configurable computing resources (e.g., networks, servers, storage, applications, and services) \\nthat can be rapidly provisioned and released with minimal management effort or service provider \\ninteraction. \\n\\uf0b7 \\nAlthough cloud computing has evolved over the time it has been majorly divided into three broad \\nservice categories:  \\n1. Infrastructure as a Service(IAAS),  \\n2. Platform as a Service (PAAS) and  \\n3. Software as a Service (SAAS)  \\n1. Infrastructure as a Service (IAAS) \\n\\uf0b7 \\nInfrastructure as a Service (IAAS) is a form of cloud computing that provides virtualized computing \\nresources over the internet.  \\n\\uf0b7 \\nIn an IAAS model, a third party provider hosts hardware, software, servers, storage and other \\ninfrastructure components on the behalf of its users.  \\n\\uf0b7 \\nIAAS providers also host users’ applications and handle tasks including system maintenance backup and \\nresiliency planning. \\n\\uf0b7 \\nIAAS platforms offer highly scalable resources that can be adjusted on-demand which makes it a well-\\nsuited for workloads that are temporary, experimental or change unexpectedly.  \\n\\uf0b7 \\nOther characteristics of IAAS environments include the automation of administrative tasks, dynamic \\nscaling, desktop virtualization and policy based services.  \\n\\uf0b7 \\nTechnically, the IaaS market has a relatively low barrier of entry, but it may require substantial financial \\ninvestment in order to build and support the cloud infrastructure.  \\n\\uf0b7 \\nMature open-source cloud management frameworks like OpenStack are available to everyone, and \\nprovide strong a software foundation for companies that want to build their private cloud or become a \\npublic cloud provider. \\nIAAS- Network: \\n\\uf0b7 \\nThere are two major network services offered by public cloud service providers:  \\n1. load balancing and  \\n2. DNS (domain name systems).  \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n6 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nLoad balancing provides a single point of access to multiple servers that run behind it. A load balancer is \\na network device that distributes network traffic among servers using specific load balancing algorithms.  \\n\\uf0b7 \\nDNS is a hierarchical naming system for computers, or any other naming devices that use IP addressing \\nfor network identification – a DNS system associates domain names with IP addresses. \\n2. Platform as a Service (PAAS) \\n\\uf0b7 \\nPlatform as a Service (PAAS) is a cloud computing model that delivers applications over the internet.  \\n\\uf0b7 \\nIn a PAAS model, a cloud provider delivers hardware and software tools, usually those needed for \\napplication development, to its users as a service.  \\n\\uf0b7 \\nA PAAS provider hosts the hardware and software on its own infrastructure. As a result, PAAS frees users \\nfrom having to install in-house hardware and software to develop or run a new application. \\n\\uf0b7 \\nPAAS doesn’t replace a business\\' entire infrastructure but instead a business relies on PAAS providers for \\nkey services, such as Java development or application hosting.  \\n\\uf0b7 \\nA PAAS provider, however, supports all the underlying computing and software, users only need to login \\nand start using the platform-usually through a Web browser interface.  \\n\\uf0b7 \\nPAAS providers then charge for that access on a per-use basis or on monthly basis. \\n\\uf0b7 \\nSome of the main characteristics of PAAS are : \\n1) Scalability and auto-provisioning of the underlying infrastructure. \\n2) Security and redundancy. \\n3) Build and deployment tools for rapid application management and deployment. \\n4) Integration with other infrastructure components such as web services, databases, and LDAP. \\n5) Multi-tenancy, platform service that can be used by many concurrent users. \\n6) Logging, reporting, and code instrumentation. \\n7) Management interfaces and/or API. \\n3. Software as a Service (SAAS) \\n\\uf0b7 \\nSoftware as a Service (SAAS) is a software distribution model in which applications are hosted by a \\nvendor or service provider and made available to customers over a network, typically the Internet.  \\n\\uf0b7 \\nSAAS has become increasingly prevalent delivery model as underlying technologies that support Web \\nservices and service- oriented architecture (SOA) mature and new development approaches, such as \\nAjax, become popular.  \\n\\uf0b7 \\nSAAS is closely related to the ASP (Application service provider) and on demand computing software \\ndelivery models.  \\n\\uf0b7 \\nIDC identifies two slightly different delivery models for SAAS which are \\n1) the hosted application model and  \\n2) the software development model.   \\n\\uf0b7 \\nSome of the core benefits of using SAAS model are: \\n1) Easier administration. \\n2) Automatic updates and patch management. \\n3) Compatibility: all users will have the same version of software. \\n4) Easier collaboration, for the same reason. \\n5) Global accessibility. \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n7 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nIssues of SaaS \\nPermanent Internet connection \\n\\uf0b7 \\nEmployees using SaaS software services must be permanently connected to the Internet.  \\n\\uf0b7 \\nWorking offline is no longer an option in this situation.  \\n\\uf0b7 \\nWe all know an Internet connection is not a problem anymore nowadays for those working in offices or \\nhome.   \\n\\uf0b7 \\nCompanies needing assurance that their employees always have a connection to their SaaS provider \\nshould consider redundant high speed Internet connections.   \\n\\uf0b7 \\nAre you using mobile devices or travelling constantly? The best solution might be Software plus Service. \\nData security \\n\\uf0b7 \\nWhen it comes to migrating traditional local software applications to a cloud based platform, data \\nsecurity may be a problem.  \\n\\uf0b7 \\nWhen a computer and application is compromised the SaaS multi-tenant application supporting many \\ncustomers could be exposed to the hackers. \\n\\uf0b7 \\nAny provider will promise that it will do the best in order for the data to be secure in any circumstances.  \\n\\uf0b7 \\nBut just to make sure, you should ask about their infrastructure and application security. \\nData control \\n\\uf0b7 \\nMany businesses have no idea how their SaaS provider will secure their data or what backup procedures \\nwill be applied when needed.  \\n\\uf0b7 \\nTo avoid undesirable effects, before choosing a SaaS vendor, managers should research for providers \\nwith good reputations and that the vendor has backup solutions which are precisely described in the \\nService Level Agreement contract. \\nData location \\n\\uf0b7 \\nThis means being permanently aware where exactly in the world your data is located.  \\n\\uf0b7 \\nAlthough the Federal Information Security Management Act in the USA requires customers to keep \\nsensitive data within the country, in virtualized systems, data can move dynamically from one country to \\nanother.  \\n\\uf0b7 \\nAsk about the laws for your customers data in respect to where they are located. \\nCloud Deployment Models \\nFollowing are the four types of Cloud Deployment Models identified by NIST. \\n1. Private Cloud \\n2. Community Cloud \\n3. Public Cloud \\n4. Hybrid Cloud \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n8 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n1. Private Cloud \\n \\nFig.: Private Cloud \\n\\uf0b7 \\nThe cloud infrastructure is operated solely for an organization. \\n\\uf0b7 \\nContrary to popular belief, private cloud may exist off premises and can be managed by a third party. \\nThus, two private cloud scenarios exist, as follows: \\nOn-site Private Cloud \\n\\uf0b7 \\nApplies to private clouds implemented at a customer’s premises. \\nOutsourced Private Cloud \\n\\uf0b7 \\nApplies to private clouds where the server side is outsourced to a hosting company. \\nExamples of Private Cloud: \\n\\uf0b7 \\nEucalyptus, Ubuntu Enterprise Cloud - UEC (powered by Eucalyptus), Amazon VPC (Virtual Private \\nCloud), VMware Cloud Infrastructure Suite, Microsoft ECI data center etc. \\n2. Community Cloud \\nFig. Community Cloud \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n9 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nThe cloud infrastructure is shared by several organizations and supports a specific community that has \\nshared concerns (e.g., mission, security requirements, policy, and compliance considerations).   \\n\\uf0b7 \\nGovernment departments, universities, central banks etc. often find this type of cloud useful.  \\n\\uf0b7 \\nCommunity cloud also has two possible scenarios: \\nOn-site Community Cloud Scenario \\n\\uf0b7 \\nApplies to community clouds implemented on the premises of the customers composing a community \\ncloud. \\nOutsourced Community Cloud \\n\\uf0b7 \\nApplies to community clouds where the server side is outsourced to a hosting company. \\nExamples of Community Cloud: \\n\\uf0b7 \\nGoogle Apps for Government, Microsoft Government Community Cloud, etc. \\n3. Public Cloud \\n \\nFig. : Public Cloud \\n\\uf0b7 \\nThe most ubiquitous, and almost a synonym for, cloud computing.  \\n\\uf0b7 \\nThe cloud infrastructure is made available to the general public or a large industry group and is owned \\nby an organization selling cloud services. \\nExamples of Public Cloud: \\n\\uf0b7 \\nGoogle App Engine, Microsoft Windows Azure, IBM Smart Cloud, Amazon EC2, etc. \\n4. Hybrid Cloud \\n \\nFig. : Hybrid Cloud \\n\\uf0b7 \\nThe cloud infrastructure is a composition of two or more clouds (private, community, or public) that \\nremain unique entities but are bound together by standardized or proprietary technology that enables \\ndata and application portability (e.g., cloud bursting for load-balancing between clouds). \\nExamples of Hybrid Cloud: \\n\\uf0b7 \\nWindows Azure (capable of Hybrid Cloud), VMware vCloud (Hybrid Cloud Services), etc. \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n10 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nEucalyptus \\n\\uf0b7 \\nEucalyptus is an open source software platform for implementing Infrastructure as a Service (IaaS) in a \\nprivate or hybrid cloud computing environment. \\n\\uf0b7 \\nThe Eucalyptus cloud platform pools together existing virtualized infrastructure to create cloud \\nresources for infrastructure as a service, network as a service and storage as a service.  \\n\\uf0b7 \\nThe name Eucalyptus is an acronym for Elastic Utility Computing Architecture for Linking Your Programs \\nto Useful Systems. \\n\\uf0b7 \\nEucalyptus was founded out of a research project in the Computer Science Department at the University \\nof California, Santa Barbara, and became a for-profit business called Eucalyptus Systems in 2009.  \\n\\uf0b7 \\nEucalyptus Systems announced a formal agreement with Amazon Web Services (AWS) in March 2012, \\nallowing administrators to move instances between a Eucalyptus private cloud and the Amazon Elastic \\nCompute Cloud (EC2) to create a hybrid cloud.  \\n\\uf0b7 \\nThe partnership also allows Eucalyptus to work with Amazon’s product teams to develop unique AWS-\\ncompatible features. \\nEucalyptus features \\n\\uf0b7 \\nSupports both Linux and Windows virtual machines (VMs). \\n\\uf0b7 \\nApplication program interface- (API) compatible with Amazon EC2 platform. \\n\\uf0b7 \\nCompatible with Amazon Web Services (AWS) and Simple Storage Service (S3). \\n\\uf0b7 \\nWorks with multiple hypervisors including VMware, Xen and KVM. \\n\\uf0b7 \\nCan be installed and deployed from source code or DEB and RPM packages. \\n\\uf0b7 \\nInternal processes communications are secured through SOAP and WS-Security. \\n\\uf0b7 \\nMultiple clusters can be virtualized as a single cloud. \\n\\uf0b7 \\nAdministrative features such as user and group management and reports. \\nBusiness Concerns in the Cloud \\nSecurity \\n\\uf0b7 \\nDue to the nature of cloud computing services and how they involve storing data without knowing its \\nprecise physical location, data security remains a concern for both prospective adopters of the \\ntechnology and existing users. \\n\\uf0b7 \\nHowever, the security concerns associated with storing things in the cloud are more nuanced than \\nmerely not being able to see where data is stored. A number of data breaches involving cloud systems \\nmade the headlines in 2017, including the story of financial giant Deloitte having its cloud data \\ncompromised. \\n\\uf0b7 \\nThese combined with the natural carefulness of trusting third parties with data makes information \\nsecurity a persistent challenge in cloud computing. However, with each breach comes enhanced security \\nin cloud systems designed to ensure similar breaches never happen again. Improvements include the use \\nof multi-factor authentication, implemented to ensure users are who they claim to be. \\n\\uf0b7 \\nTruth be told, security for most cloud providers is watertight, and breaches in the cloud are rare—when \\nthey do occur, though, they get all the headlines. To minimize risk, double-check that your cloud \\nprovider uses secure user identity management and access controls. It’s also important to check which \\ndata security laws your cloud provider must follow. On the whole, cloud data security is as safe, if not \\nsafer, than on premise data security. \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n11 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nOutages \\n\\uf0b7 \\nPerformance is a consistent challenge in cloud computing, particularly for businesses that rely on cloud \\nproviders to help them run mission-critical applications. When a business moves to the cloud it becomes \\ndependent on the cloud provider, meaning that any outages suffered by the cloud provider also affect \\nthe business. \\n\\uf0b7 \\nThe risk of outages in the cloud is not negligible—even the major players in cloud computing are \\nsusceptible. In February 2017, an AWS Amazon S3 outage caused disruptions for many websites and \\napplications, and even sent them offline. \\n\\uf0b7 \\nThere is a need, therefore, for some kind of site recovery solution for data held in cloud-based services. \\nDisaster recovery as a service (DRaaS)—the replication and hosting of servers by a third party to provide \\nfailover in the event of a man-made or natural catastrophe—is a way companies can maintain business \\ncontinuity even when disaster strikes. \\nExpertise \\n\\uf0b7 \\nThe success of any movement towards cloud adoption comes down to the expertise at your disposal. \\nThe complexity of cloud technology and the sheer range of tools makes it difficult to keep up with the \\noptions available for all your use cases. \\n\\uf0b7 \\nOrganizations need to strike a balance between having the right expertise and the cost of hiring \\ndedicated cloud specialists. The optimum solution to this challenge is to work with a trusted cloud \\nManaged Service Provider (MSP). Cloud MSPs have the manpower, tools and experience to manage \\nmultiple and complex customer environments simultaneously. The MSP takes complete responsibility for \\ncloud processes and implementing them as the customer desires. This way, organizations can stay \\nfocused on their business goals. \\nCost Management \\n\\uf0b7 \\nAll the main cloud providers have quite detailed pricing plans for their services that explicitly define costs \\nof processing and storage data in the cloud. The problem is that cost management is often an issue \\nwhen using cloud services because of the sheer range of options available. \\n\\uf0b7 \\nBusinesses often waste money on unused workloads or unnecessarily expensive storage, and 26 percent \\nof respondents in this cloud survey cited cost management as a major challenge in the cloud. The \\nsolution is for organizations to monitor their cloud usage in detail and constantly optimize their choice of \\nservices, instances, and storage. You can monitor and optimize cloud implementation by using a cloud \\ncost management tool such as CloudHealth or consulting a cloud cost expert. \\n\\uf0b7 \\nThere are also some practical cost calculators available which clarify cloud costs, including Amazon’s \\nAWS Simple Monthly Calculator, and NetApp’s calculators for both AWS and Azure cloud storage. \\nGovernance \\n\\uf0b7 \\nCloud governance, meaning the set of policies and methods used to ensure data security and privacy in \\nthe cloud, is a huge challenge. Confusion often arises about who takes responsibility for data stored in \\nthe cloud, who should be allowed use cloud resources without first consulting IT personnel, and how \\nemployees handle sensitive data. \\n\\uf0b7 \\nThe only solution is for the IT department at your organization to adapt its existing governance and \\ncontrol processes to incorporate the cloud and ensure everyone is on the same page. This way, proper \\ngovernance, compliance, and risk management can be enforced. \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n12 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nCloud Optimization Strategy \\n\\uf0b7 \\nFinding the right strategy for cloud adoption is another important challenge. Many businesses moved to \\nthe cloud using a segmented approach in which isolated use cases, projects, and applications were \\nmigrated to cloud providers. The problem then for many companies is a lack of any holistic organization-\\nwide cloud strategy. \\n\\uf0b7 \\nFinding the right strategy for cloud adoption comes back to the issue of cloud governance. With \\neveryone on the same page thanks to robust cloud governance and clear policies, organizations can \\ncreate a unified and optimized strategy for how they use the cloud. \\nSteps to Launch an Application with AWS Elastic Beanstalk \\nStep 1: Create a New Application \\n\\uf0b7 \\nNow that you’re in the AWS Elastic Beanstalk dashboard, click on Create New Application to create and \\nconfigure your application. \\n \\nStep 2: Configure your Application \\n\\uf0b7 \\nFill out the Application name with “Your-sample-app” and Description field with “Sample App”. Click \\nNext to continue.  \\n \\nStep 3: Configure your Environment \\n\\uf0b7 \\nFor this tutorial, we will be creating a web server environment for our sample PHP application. Click on \\nCreate web server. \\n \\n\\uf0b7 \\nClick on Select a platform next to Predefined configuration, then select “Your Plateform”. Next, click on \\nthe drop-down menu next to Environment type, then select Single instance. \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n13 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n \\n\\uf0b7 \\nUnder Source, select the Upload your own option, then click Choose File to select the “Your-sample-app-\\nv1.zip” file we downloaded earlier. \\n \\n\\uf0b7 \\nFill in the values for Environment name with “YourSampleApp-env”. For Environment URL, fill in a \\nglobally unique value since this will be your public-facing URL; we will use “YourSampleApp-env” in this \\ntutorial, so please choose something different from this one. Lastly, fill Description with “Your Sample \\nApp”. For the Environment URL, make sure to click Check availability to make sure that the URL is not \\ntaken. Click Next to continue. \\n \\n\\uf0b7 \\nCheck the box next to Create this environment inside a VPC. Click Next to continue. \\n \\n\\uf0b7 \\nOn the Configuration Details step, you can set configuration options for the instances in your stack. Click \\nNext. \\n\\uf0b7 \\nOn the Environment Tags step, you can tag all the resources in your stack. Click Next. \\n\\uf0b7 \\nOn the VPC Configuration step, select the first AZ listed by checking the box under the EC2 column. Your \\nlist of AZs may look different than the one shown as Regions can have different number of AZs. Click \\nNext. \\n\\n \\nUnit-1 – Introduction to Cloud Technologies \\n \\n \\n14 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n \\n\\uf0b7 \\nAt the Permissions step, leave everything to their default values, then click Next to continue. Then \\nreview your environment configuration on the next screen and then click Launch to deploy your \\napplication. \\nStep 4: Accessing your Elastic Beanstalk Application \\n\\uf0b7 \\nGo back to the main Elastic Beanstalk dashboard page by clicking on Elastic Beanstalk. When your \\napplication successfully launched, your application’s environment, “YourSampleApp-env”, will show up \\nas a green box. Click on “YourSampleApp-env”, which is the green box. \\n \\n\\uf0b7 \\nAt the top of the page, you should see a URL field, with a value that contains the Environment URL you \\nspecified in step 3. Click on this URL field, and you should see a Congratulations page. \\n \\n\\uf0b7 \\nCongratulations! You have successfully launched a sample PHP application using AWS Elastic Beanstalk. \\n \\n\\n \\nUnit-2 – Virtualization and Cloud Platforms \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nVirtualization \\n\\uf0b7 \\nVirtualization is changing the mindset from physical to logical. \\n \\nFig. : Virtualization \\n\\uf0b7 \\nWhat virtualization means is creating more logical IT resources, called virtual systems, within one \\nphysical system. That’s called system virtualization.  \\n\\uf0b7 \\nIt most commonly uses the hypervisor for managing the resources for every virtual system. The \\nhypervisor is a software that can virtualize the hardware resources. \\nBenefits of Virtualization \\n\\uf0b7 \\nMore flexible and efficient allocation of resources. \\n\\uf0b7 \\nEnhance development productivity. \\n\\uf0b7 \\nIt lowers the cost of IT infrastructure. \\n\\uf0b7 \\nRemote access and rapid scalability. \\n\\uf0b7 \\nHigh availability and disaster recovery. \\n\\uf0b7 \\nPay per use of the IT infrastructure on demand. \\n\\uf0b7 \\nEnables running multiple operating system. \\nTypes of Virtualization \\n1. Application Virtualization: \\n\\uf0b7 \\nApplication virtualization helps a user to have a remote access of an application from a server.  \\n\\uf0b7 \\nThe server stores all personal information and other characteristics of the application but can still run on \\na local workstation through internet.  \\n\\uf0b7 \\nExample of this would be a user who needs to run two different versions of the same software.  \\n\\uf0b7 \\nTechnologies that use application virtualization are hosted applications and packaged applications. \\n2. Network Virtualization: \\n\\uf0b7 \\nThe ability to run multiple virtual networks with each has a separate control and data plan.  \\n\\uf0b7 \\nIt co-exists together on top of one physical network.  \\n\\uf0b7 \\nIt can be managed by individual parties that potentially confidential to each other.  \\n\\n \\nUnit-2 – Virtualization and Cloud Platforms \\n \\n \\n2 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nNetwork virtualization provides a facility to create and provision virtual networks—logical switches, \\nrouters, firewalls, load balancer, Virtual Private Network (VPN), and workload security within days or \\neven in weeks. \\n3. Desktop Virtualization: \\n\\uf0b7 \\nDesktop virtualization allows the users’ OS to be remotely stored on a server in the data center. \\n\\uf0b7 \\nIt allows the user to access their desktop virtually, from any location by different machine.  \\n\\uf0b7 \\nUsers who wants specific operating systems other than Windows Server will need to have a virtual \\ndesktop. \\n\\uf0b7 \\nMain benefits of desktop virtualization are user mobility, portability, and easy management of software \\ninstallation, updates and patches. \\n4. Storage Virtualization: \\n\\uf0b7 \\nStorage virtualization is an array of servers that are managed by a virtual storage system.  \\n\\uf0b7 \\nThe servers aren’t aware of exactly where their data is stored, and instead function more like worker \\nbees in a hive.  \\n\\uf0b7 \\nIt makes managing storage from multiple sources to be managed and utilized as a single repository.  \\n\\uf0b7 \\nStorage virtualization software maintains smooth operations, consistent performance and a continuous \\nsuite of advanced functions despite changes, break down and differences in the underlying equipment. \\nFull Virtualization \\n\\uf0b7 \\nVirtual machine simulates hardware to allow an unmodified guest OS to be run in isolation.  \\n\\uf0b7 \\nThere is two type of Full virtualizations in the enterprise market. \\n1. Software assisted full virtualization \\n2. Hardware-assisted full virtualization \\n\\uf0b7 \\nOn both full virtualization types, guest operating system’s source information will not be modified. \\n1. Software Assisted – Full Virtualization (BT – Binary Translation) \\n\\uf0b7 \\nIt completely relies on binary translation to trap and virtualize the execution of sensitive, non-\\nvirtualizable instructions sets.  \\n\\uf0b7 \\nIt emulates the hardware using the software instruction sets.  \\n\\uf0b7 \\nDue to binary translation, it often criticized for performance issue.  \\n\\uf0b7 \\nHere is the list of software which will fall under software assisted (BT). \\no VMware workstation (32Bit guests) \\no Virtual PC \\no VirtualBox (32-bit guests) \\no VMware Server \\n2. Hardware-Assisted – Full Virtualization (VT) \\n\\uf0b7 \\nHardware-assisted full virtualization eliminates the binary translation and it directly interrupts with \\nhardware using the virtualization technology which has been integrated on X86 processors since 2005 \\n(Intel VT-x and AMD-V).   \\n\\uf0b7 \\nGuest OS’s instructions might allow a virtual context execute privileged instructions directly on the \\nprocessor, even though it is virtualized. \\n\\uf0b7 \\nHere is the list of enterprise software which supports hardware-assisted – Full virtualization which falls \\nunder hypervisor type 1  (Bare metal ) \\n\\n \\nUnit-2 – Virtualization and Cloud Platforms \\n \\n \\n3 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\no VMware ESXi /ESX \\no KVM \\no Hyper-V \\no Xen \\n\\uf0b7 \\nThe following virtualization type of virtualization falls under hypervisor type 2 (Hosted). \\no VMware Workstation  (64-bit guests only ) \\no Virtual Box (64-bit guests only ) \\no VMware Server (Retired ) \\nParavirtualization \\n\\uf0b7 \\nParavirtualization works differently from the full virtualization.  \\n\\uf0b7 \\nIt doesn’t need to simulate the hardware for the virtual machines.  \\n\\uf0b7 \\nThe hypervisor is installed on a physical server (host) and a guest OS is installed into the environment.  \\n\\uf0b7 \\nVirtual guests aware that it has been virtualized, unlike the full virtualization (where the guest doesn’t \\nknow that it has been virtualized) to take advantage of the functions.  \\n\\uf0b7 \\nIn this virtualization method, guest source codes will be modified with sensitive information to \\ncommunicate with the host.  \\n\\uf0b7 \\nGuest Operating systems require extensions to make API calls to the hypervisor.  \\n\\uf0b7 \\nIn full virtualization, guests will issue a hardware calls but in paravirtualization, guests will directly \\ncommunicate with the host (hypervisor) using the drivers.  \\n\\uf0b7 \\nHere is the lisf of products which supports paravirtualization. \\no Xen \\no IBM LPAR \\no Oracle VM for SPARC  (LDOM) \\no Oracle VM for X86  (OVM) \\nHybrid Virtualization (Hardware Virtualized with PV Drivers) \\n\\uf0b7 \\nIn Hardware assisted full virtualization, Guest operating systems are unmodified and it involves many \\nVM traps and thus high CPU overheads which limit the scalability.   \\n\\uf0b7 \\nParavirtualization is a complex method where guest kernel needs to be modified to inject the API.  \\n\\uf0b7 \\nBy considering these issues, engineers have come with hybrid paravirtualization.  \\n\\uf0b7 \\nIt’s a combination of both Full & Paravirtualization. The virtual machine uses paravirtualization for \\nspecific hardware drivers (where there is a bottleneck with full virtualization, especially with I/O & \\nmemory intense workloads), and the host uses full virtualization for other features.  \\n\\uf0b7 \\nThe following products support hybrid virtualization. \\no Oracle VM for x86 \\no Xen \\no VMware ESXi \\nOS level Virtualization \\n\\uf0b7 \\nOperating system-level virtualization is widely used. \\n\\uf0b7 \\nIt also known as “containerization”.  \\n\\uf0b7 \\nHost Operating system kernel allows multiple user spaces also known as instance. \\n\\uf0b7 \\nIn OS-level virtualization, unlike other virtualization technologies, there will be very little or no overhead \\nsince its uses the host operating system kernel for execution.  \\n\\n \\nUnit-2 – Virtualization and Cloud Platforms \\n \\n \\n4 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nOracle Solaris zone is one of the famous containers in the enterprise market.   \\n\\uf0b7 \\nHere is the list of other containers. \\no Linux LCX \\no Docker \\no AIX WPAR \\nVirtual computing \\n\\uf0b7 \\nVirtual computing refers to the use of a remote computer from a local computer where the actual \\ncomputer user is located.  \\n\\uf0b7 \\nFor example, a user at a home computer could log in to a remote office computer (via the Internet or a \\nnetwork) to perform job tasks.  \\n\\uf0b7 \\nOnce logged in via special software, the remote computer can be used as though it were at the user\\'s \\nlocation, allowing the user to perform tasks via the keyboard, mouse, or other tools. \\nVirtual Machine \\n\\uf0b7 \\nA virtual machine (VM) is an operating system (OS) or application environment that is installed on \\nsoftware, which reproduces dedicated hardware. The end user has the same experience on a virtual \\nmachine as they would have on dedicated hardware. \\nVirtual Machine Conversions in VMM (Virtual Machine Migration) \\n\\uf0b7 \\nWhen you use cloud computing, you are accessing pooled resources using a technique called \\nvirtualization. \\n\\uf0b7 \\nVirtualization assigns a logical name for a physical resource and then provides a pointer to that physical \\nresource when a request is made.  \\n\\uf0b7 \\nVirtualization provides a means to manage resources efficiently because the mapping of virtual \\nresources to physical resources can be both dynamic and facile. \\n\\uf0b7 \\nVirtualization is dynamic in that the mapping can be assigned based on rapidly changing conditions, and \\nit is facile because changes to a mapping assignment can be nearly instantaneous. \\n\\uf0b7 \\nThese are among the different types of virtualization that are characteristic of cloud computing: \\no Access: A client can request access to a cloud service from any location. \\no Application: A cloud has multiple application instances and directs requests to an instance based on \\nconditions. \\no CPU: Computers can be partitioned into a set of virtual machines with each machine being assigned a \\nworkload. Alternatively, systems can be virtualized through load-balancing technologies. \\no Storage: Data is stored across storage devices and often replicated for redundancy. To enable these \\ncharacteristics, resources must be highly configurable and flexible.  \\n\\uf0b7 \\nYou can define the features in software and hardware that enable this flexibility as conforming to one or \\nmore of the following mobility patterns: \\no P2V: Physical to Virtual \\no V2V: Virtual to Virtual \\no V2P: Virtual to Physical \\no P2P: Physical to Physical \\no D2C: Datacenter to Cloud \\no C2C: Cloud to Cloud \\no C2D: Cloud to Datacenter \\no D2D: Datacenter to Datacenter  \\n\\n \\nUnit-2 – Virtualization and Cloud Platforms \\n \\n \\n5 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nVirtual Machine Types \\n1. General Purpose \\n\\uf0b7 \\nThis family includes the M1 and M3 VM types.  \\n\\uf0b7 \\nThese types provide a balance of CPU, memory, and network resources, which makes them a good \\nchoice for many applications.  \\n\\uf0b7 \\nThe VM types in this family range in size from one virtual CPU with two GB of RAM to eight virtual CPUs \\nwith 30 GB of RAM.  \\n\\uf0b7 \\nThe balance of resources makes them ideal for running small and mid-size databases, more memory-\\nhungry data processing tasks, caching fleets, and backend servers. \\n\\uf0b7 \\nM1 types offer smaller instance sizes with moderate CPU performance.  \\n\\uf0b7 \\nM3 types offer larger number of virtual CPUs that provide higher performance.  \\n\\uf0b7 \\nIt is recommended to use M3 instances if you need general-purpose instances with demanding CPU \\nrequirements. \\n2. Compute Optimized \\n\\uf0b7 \\nThis family includes the C1 and CC2 instance types, and is geared towards applications that benefit from \\nhigh compute power.  \\n\\uf0b7 \\nCompute-optimized VM types have a higher ratio of virtual CPUs to memory than other families but \\nshare the NCs (Node Controllers) with non-optimized ones.  \\n\\uf0b7 \\nIt is recommended to use these type if you are running any CPU-bound scale-out applications.  \\n\\uf0b7 \\nCC2 instances provide high core count (32 virtual CPUs) and support for cluster networking.  \\n\\uf0b7 \\nC1 instances are available in smaller sizes and are ideal for scaled-out applications at massive scale. \\n3. Memory Optimized \\n\\uf0b7 \\nThis family includes the CR1 and M2 VM types and is designed for memory-intensive applications.  \\n\\uf0b7 \\nIt is recommended to use these VM types for performance-sensitive database, where your application is \\nmemory-bound.  \\n\\uf0b7 \\nCR1 VM types provide more memory and faster CPU than do M2 types.  \\n\\uf0b7 \\nCR1 instances also support cluster networking for bandwidth intensive applications.  \\n\\uf0b7 \\nM2 types are available in smaller sizes, and are an excellent option for many memory-bound \\napplications. \\n4. Micro \\n\\uf0b7 \\nThis Micro family contains the T1 VM type.  \\n\\uf0b7 \\nThe T1 micro provides a small amount of consistent CPU resources and allows you to increase CPU \\ncapacity in short bursts when additional cycles are available.  \\n\\uf0b7 \\nIt is recommended to use this type of VM for lower throughput applications like a proxy server or \\nadministrative applications, or for low-traffic websites that occasionally require additional compute \\ncycles. It is not recommended for applications that require sustained CPU performance. \\nLoad Balancing \\n\\uf0b7 \\nIn computing, load balancing improves the distribution of workloads across multiple computing \\nresources, such as computers, a computer cluster, network links, central processing units, or disk drives. \\n\\n \\nUnit-2 – Virtualization and Cloud Platforms \\n \\n \\n6 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nNeed of load balancing in cloud computing \\n(i) High Performing applications \\no Cloud load balancing techniques, unlike their traditional on premise counterparts, are less expensive \\nand simple to implement. Enterprises can make their client applications work faster and deliver \\nbetter performances, that too at potentially lower costs. \\n(ii) Increased scalability \\no Cloud balancing takes help of cloud’s scalability and agility to maintain website traffic. By using \\nefficient load balancers, you can easily match up the increased user traffic and distribute it among \\nvarious servers or network devices. It is especially important for ecommerce websites, who deals \\nwith thousands of website visitors every second. During sale or other promotional offers they need \\nsuch effective load balancers to distribute workloads. \\n(iii) \\nAbility to handle sudden traffic spikes \\no A normally running University site can completely go down during any result declaration. This is \\nbecause too many requests can arrive at the same time. If they are using cloud load balancers, they \\ndo not need to worry about such traffic surges. No matter how large the request is, it can be wisely \\ndistributed among different servers for generating maximum results in less response time. \\n(iv) \\nBusiness continuity with complete flexibility \\no The basic objective of using a load balancer is to save or protect a website from sudden outages. \\nWhen the workload is distributed among various servers or network units, even if one node fails the \\nburden can be shifted to another active node. \\no Thus, with increased redundancy, scalability and other features load balancing easily handles \\nwebsite or application traffic. \\nNetwork resources that can be load balanced \\n\\uf0b7 \\nServers \\n\\uf0b7 \\nRouting mechanism \\nHypervisors \\n\\uf0b7 \\nIt is the part of the private cloud that manages the virtual machines, i.e. it is the part (program) that \\nenables multiple operating systems to share the same hardware.  \\n\\uf0b7 \\nEach operating system could use all the hardware (processor, memory, etc.) if no other operating system \\nis on. That is the maximum hardware available to one operating system in the cloud. \\n\\uf0b7 \\nNevertheless, the hypervisor is what controls and allocates what portion of hardware resources each \\noperating system should get, in order every one of them to get what they need and not to disrupt each \\nother. \\n\\n \\nUnit-2 – Virtualization and Cloud Platforms \\n \\n \\n7 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nThere are two types of hypervisors \\n \\nFig. : Types of Hypervisors \\n\\uf0b7 \\nType 1 hypervisor: hypervisors run directly on the system hardware – A “bare metal” embedded \\nhypervisor. Examples are:  \\n1) VMware ESX and ESXi \\n2) Microsoft Hyper-V \\n3) Citrix XenServer \\n4) Oracle VM \\n\\uf0b7 \\nType 2 hypervisor: hypervisors run on a host operating system that provides virtualization services, such \\nas I/O device support and memory management. Examples are: \\n1) VMware Workstation/Fusion/Player \\n2) Microsoft Virtual PC \\n3) Oracle VM VirtualBox \\n4) Red Hat Enterprise Virtualization \\nMachine Imaging \\n\\uf0b7 \\nMachine imaging is a process that is used to achieve the goal of system portability, provision, and deploy \\nsystems in the cloud through capturing the state of systems using a system image. \\n\\uf0b7 \\nA system image makes a copy or a clone of the entire computer system inside a single file.  \\n\\uf0b7 \\nThe image is made by using a program called system imaging program and can be used later to restore a \\nsystem image.  \\n\\uf0b7 \\nFor example Amazon Machine Image (AMI) is a system image that is used in the cloud computing.  \\n\\uf0b7 \\nThe Amazon Web Services uses AMI to store copies of a virtual machine.  \\n\\uf0b7 \\nAn AMI is a file system image that contains an operating system, all device drivers, and any applications \\nand state information that the working virtual machine would have.  \\n\\uf0b7 \\nThe AMI files are encrypted and compressed for security purpose and stored in Amazon S3 (Simple \\nStorage System) buckets as a set of 10MB chunks.  \\n\\uf0b7 \\nMachine imaging is mostly run on virtualization platform due to this it is also called as Virtual Appliances \\nand running virtual machines are called instances. \\n\\uf0b7 \\nBecause many users share clouds, the cloud helps you track information about images, such as \\nownership, history, and so on.  \\n\\uf0b7 \\nThe IBM SmartCloud Enterprise knows what organization you belong to when you log in.  \\n\\uf0b7 \\nYou can choose whether to keep images private, exclusively for your own use, or to share with other \\nusers in your organization.  \\n\\uf0b7 \\nIf you are an independent software vendor, you can also add your images to the public catalog. \\n\\n \\nUnit-2 – Virtualization and Cloud Platforms \\n \\n \\n8 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nCloud Marketplace Overview \\n\\uf0b7 \\nA cloud marketplace is an online storefront operated by a cloud service provider. \\n\\uf0b7 \\nA cloud marketplace provides customers with access to software applications and services that are built \\non, integrate with or complement the cloud provider\\'s offerings.  \\n\\uf0b7 \\nA marketplace typically provides customers with native cloud applications and approved apps created by \\nthird-party developers.  \\n\\uf0b7 \\nApplications from third-party developers not only help the cloud provider fill niche gaps in its portfolio \\nand meet the needs of more customers, but they also provide the customer with peace of mind by \\nknowing that all purchases from the vendor\\'s marketplace will integrate with each other smoothly. \\nExamples of cloud marketplaces \\n\\uf0b7 \\nAWS Marketplace - helps customers find, buy and use software and services that run in the Amazon \\nElastic Compute Cloud (EC2). \\n\\uf0b7 \\nOracle Marketplace - offers a comprehensive list of apps for sales, service, marketing, talent \\nmanagement and human capital management. \\n\\uf0b7 \\nMicrosoft Windows Azure Marketplace - an online market for buying and selling Software as a Service \\n(SaaS) applications and research datasets. \\n\\uf0b7 \\nSalesforce.com\\'s AppExchange - provides business apps for sales representatives and customer \\nrelationship management (CRM). \\nComparison of Cloud Providers \\n \\nAmazon Web Service \\nAzure \\nRackspace \\nIntroduction \\nAmazon Web Services \\n(AWS) is a collection of \\nremote computing services \\n(also called web services) \\nthat together make up a \\ncloud computing platform, \\noffered over the Internet by \\nAmazon.com. \\nAzure is a cloud computing \\nplatform and \\ninfrastructure, created by \\nMicrosoft, for building, \\ndeploying and managing \\napplications and services \\nthrough a global network \\nof Microsoft-managed \\ndatacenters. \\nRackspace is a managed \\ncloud computing provider \\noffering high percentage \\navailability of applications \\nbased on RAID10. \\nDistinguishing \\nFeatures \\nRich set of services and \\nintegrated monitoring tools; \\ncompetitive pricing model. \\nEasy-to-use administration \\ntool, especially for \\nWindows admins. \\nEasy to use control panel, \\nespecially for non-system \\nadministrators. \\nVirtualization \\nXen hypervisor \\nMicrosoft Hyper-V \\nOpensource (Xen, Kvm ) \\nand VMware \\nBase OS \\nLinux (+QEMU) and \\nWindows \\nWindows and Linux \\nUbuntu \\nPricing model \\nPay-as-you-go, then \\nsubscription \\nPay-as-you-go \\nPay-as-you-go \\nMajor products \\nElastic block store, IP  \\naddresses, virtual private \\ncloud, cloud watch, Cloud \\nFront, clusters etc. \\nServer Failover Clustering, \\nNetwork Load Balancing, \\nSNMP Services, Storage \\nManager for SANs, \\nWindows Internet Name \\nService, Disaster Recovery \\nto Azure, Azure Caching \\nand Azure Redis Cache. \\nManaged cloud, block \\nstorage, monitoring \\n\\n \\nUnit-2 – Virtualization and Cloud Platforms \\n \\n \\n9 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n \\nAmazon Web Service \\nAzure \\nRackspace \\nCDN Features \\nOrigin-Pull, Purge, Gzip \\ncompression, Persistent \\nconnections, Caching \\nheaders, Custom CNAMEs, \\nControl Panel & stats, \\nAccess Logs. \\nRobust security, Lower \\nlatencies, Massively \\nscalable, Capacity on \\ndemand. \\nRackspace provide CDN \\nservices through a \\npartnership with Akamai’s \\nservice. \\nAccess interface \\nWeb-based, API, console \\nWeb interface \\nWeb-based control panel \\nPreventive \\nmeasures \\nModerate \\nBasic \\nBasic \\nReactive \\nmeasures \\nModerate \\nBasic \\nBasic \\nReliability \\nGood \\nAverage \\nGood \\nScalability \\nGood \\nGood \\nGood \\nSupport \\nGood and chargeable \\nGood \\nExcellent \\nAvailability (%) \\n99.95 \\n99.95 \\n99.99 \\nServer \\nPerformance \\n(Over a period) \\nGood \\nExcellent and consistent \\nAverage \\nTools/ \\nframework \\nAmazon machine image \\n(AMI), Java,  PHP,  Python, \\nRuby \\nPHP, ASP.NET, Node.js, \\nPython \\n    - \\nDatabase RDS \\nMySQL, MsSQL, Oracle \\nMicrosoft SQL Database \\nMySQL \\n \\n\\n \\nUnit-3 – Introduction to AWS \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nAWS History \\n\\uf0b7 \\nThe AWS platform was launched in July 2002.  \\n\\uf0b7 \\nIn its early stages, the platform consisted of only a few disparate tools and services.  \\n\\uf0b7 \\nThen in late 2003, the AWS concept was publicly reformulated when Chris Pinkham and Benjamin Black \\npresented a paper describing a vision for Amazon\\'s retail computing infrastructure that was completely \\nstandardized, completely automated, and would rely extensively on web services for services such as \\nstorage and would draw on internal work already underway.  \\n\\uf0b7 \\nNear the end of their paper, they mentioned the possibility of selling access to virtual servers as a \\nservice, proposing the company could generate revenue from the new infrastructure investment. \\n\\uf0b7 \\nIn November 2004, the first AWS service launched for public usage: Simple Queue Service (SQS). \\n\\uf0b7 \\nThereafter Pinkham and lead developer Christopher Brown developed the Amazon EC2 service, with a \\nteam in Cape Town, South Africa. \\n\\uf0b7 \\nAmazon Web Services was officially re-launched on March 14, 2006, combining the three initial service \\nofferings of Amazon S3 cloud storage, SQS, and EC2.  \\n\\uf0b7 \\nThe AWS platform finally provided an integrated suite of core online services, as Chris Pinkham and \\nBenjamin Black had proposed back in 2003, as a service offered to other developers, web sites, client-\\nside applications, and companies. \\n\\uf0b7 \\nAndy Jassy, AWS founder and vice president in 2006, said at the time that Amazon S3 (one of the first \\nand most scalable elements of AWS) helps free developers from worrying about where they are going to \\nstore data, whether it will be safe and secure, if it will be available when they need it, the costs \\nassociated with server maintenance, or whether they have enough storage available.  \\n\\uf0b7 \\nAmazon S3 enables developers to focus on innovating with data, rather than figuring out how to store it. \\n\\uf0b7 \\nIn 2016 Jassy was promoted to CEO of the division. \\n\\uf0b7 \\nReflecting the success of AWS, his annual compensation in 2017 hit nearly $36 million. \\n\\uf0b7 \\nIn 2014, AWS launched its partner network entitled APN (AWS Partner Network) which is focused on \\nhelping AWS-based companies grow and scale the success of their business with close collaboration and \\nbest practices. \\n\\uf0b7 \\nTo support industry-wide training and skills standardization, AWS began offering a certification program \\nfor computer engineers, on April 30, 2013, to highlight expertise in cloud computing. \\n\\uf0b7 \\nIn January 2015, Amazon Web Services acquired Annapurna Labs, an Israel-based microelectronics \\ncompany reputedly for US$350–370M. \\n\\uf0b7 \\nJames Hamilton, an AWS engineer, wrote a retrospective article in 2016 to highlight the ten-year history \\nof the online service from 2006 to 2016. As an early fan and outspoken proponent of the technology, he \\nhad joined the AWS engineering team in 2008. \\n\\uf0b7 \\nIn January 2018, Amazon launched an auto scaling service on AWS. \\n\\uf0b7 \\nIn November 2018, AWS announced customized ARM cores for use in its servers. \\n\\uf0b7 \\nAlso in November 2018, AWS is developing ground stations to communicate with customer\\'s satellites. \\nAWS Infrastructure \\n\\uf0b7 \\nAmazon Web Services (AWS) is a global public cloud provider, and as such, it has to have a global \\nnetwork of infrastructure to run and manage its many growing cloud services that support customers \\naround the world.  \\n\\uf0b7 \\nNow we’ll take a look at the components that make up the AWS Global Infrastructure. \\n1) Availability Zones (AZs) \\n\\n \\nUnit-3 – Introduction to AWS \\n \\n \\n2 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n2) Regions \\n3) Edge Locations \\n4) Regional Edge Caches \\n\\uf0b7 \\nIf you are deploying services on AWS, you’ll want to have a clear understanding of each of these \\ncomponents, how they are linked, and how you can use them within your solution to YOUR maximum \\nbenefit. Let’s take a closer look. \\n1) Availability Zones (AZ) \\n\\uf0b7 \\nAZs are essentially the physical data centers of AWS. This is where the actual compute, storage, network, \\nand database resources are hosted that we as consumers provision within our Virtual Private Clouds \\n(VPCs).  \\n\\uf0b7 \\nA common misconception is that a single availability zone is equal to a single data center. This is not the \\ncase. In fact, it’s likely that multiple data centers located close together form a single availability zone. \\n\\uf0b7 \\nEach AZ will always have at least one other AZ that is geographically located within the same area, \\nusually a city, linked by highly resilient and very low latency private fiber optic connections. However, \\neach AZ will be isolated from the others using separate power and network connectivity that minimizes \\nimpact to other AZs should a single AZ fail. \\n\\uf0b7 \\nThese low latency links between AZs are used by many AWS services to replicate data for high \\navailability and resilience purposes.  \\n\\uf0b7 \\nMultiple AZs within a region allows you to create highly available and resilient applications and services.  \\n\\uf0b7 \\nBy architecting your solutions to utilize resources across more than one AZ ensures that minimal or no \\nimpact will occur to your infrastructure should an AZ experience a failure, which does happen. \\n\\uf0b7 \\nAnyone can deploy resources in the cloud, but architecting them in a way that ensures your \\ninfrastructure remains stable, available, and resilient when faced with a disaster is a different matter.  \\n\\uf0b7 \\nMaking use of at least two AZs in a region helps you maintain high availability of your infrastructure and \\nit’s always a recommended best practice. \\n \\nFig. : Availability Zone and Region \\n2) Regions \\n\\uf0b7 \\nRegion is a collection of availability zones that are geographically located close to one other.  \\n\\uf0b7 \\nThis is generally indicated by AZs within the same city. AWS has deployed them across the globe to allow \\nits worldwide customer base to take advantage of low latency connections.  \\n\\uf0b7 \\nEach Region will act independently of the others, and each will contain at least two Availability Zones.  \\n\\uf0b7 \\nExample: if an organization based in London was serving customers throughout Europe, there would be \\nno logical sense to deploy services in the Sydney Region simply due to the latency response times for its \\n\\n \\nUnit-3 – Introduction to AWS \\n \\n \\n3 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\ncustomers. Instead, the company would select the region most appropriate for them and their customer \\nbase, which may be the London, Frankfurt, or Ireland Region. \\n\\uf0b7 \\nHaving global regions also allows for compliance with regulations, laws, and governance relating to data \\nstorage (at rest and in transit).  \\n\\uf0b7 \\nExample: you may be required to keep all data within a specific location, such as Europe. Having multiple \\nregions within this location allows an organization to meet this requirement. \\n\\uf0b7 \\nSimilarly to how utilizing multiple AZs within a region creates a level of high availability, the same can be \\napplied to utilizing multiple regions.  \\n\\uf0b7 \\nYou may want to use multiple regions if you are a global organization serving customers in different \\ncountries that have specific laws and governance about the use of data.  \\n\\uf0b7 \\nIn this case, you could even connect different VPCs together in different regions.  \\n\\uf0b7 \\nThe number of regions is increasing year after year as AWS works to keep up with the demand for cloud \\ncomputing services.  \\n\\uf0b7 \\nIn July 2017, there are currently 16 Regions and 43 Availability Zones, with 4 Regions and 11 AZs \\nplanned. \\n3) Edge Locations \\n\\uf0b7 \\nEdge Locations are AWS sites deployed in major cities and highly populated areas across the globe. They \\nfar outnumber the number of availability zones available. \\n\\uf0b7 \\nWhile Edge Locations are not used to deploy your main infrastructures such as EC2 instances, EBS \\nstorage, VPCs, or RDS resources like AZs, they are used by AWS services such as AWS CloudFront and \\nAWS Lambda@Edge (currently in Preview) to cache data and reduce latency for end user access by using \\nthe Edge Locations as a global Content Delivery Network (CDN). \\n\\uf0b7 \\nAs a result, Edge Locations are primarily used by end users who are accessing and using your services. \\n\\uf0b7 \\nFor example, you may have your website hosted on EC2 instances and S3 (your origin) within the Ohio \\nregion with a configured CloudFront distribution associated. When a user accesses your website from \\nEurope, they would be re-directed to their closest Edge Location (in Europe) where cached data could be \\nread on your website, significantly reducing latency. \\n \\nFig. : Edge Location and Regional Edge Cache \\n4) Regional Edge Cache \\n\\uf0b7 \\nIn November 2016, AWS announced a new type of Edge Location, called a Regional Edge Cache.  \\n\\uf0b7 \\nThese sit between your CloudFront Origin servers and the Edge Locations.   \\n\\n \\nUnit-3 – Introduction to AWS \\n \\n \\n4 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nA Regional Edge Cache has a larger cache-width than each of the individual Edge Locations, and because \\ndata expires from the cache at the Edge Locations, the data is retained at the Regional Edge Caches. \\n\\uf0b7 \\nTherefore, when data is requested at the Edge Location that is no longer available, the Edge Location can \\nretrieve the cached data from the Regional Edge Cache instead of the Origin servers, which would have a \\nhigher latency. \\nPods, Aggregation, Silos \\n\\uf0b7 \\nWorkloads support a certain no. of user when the workload reaches the limit of largest virtual machine \\ninstance possible, a copy or clone of the instance is required. A group of users within a particular \\ninstance is called a pod. \\n\\uf0b7 \\nSizing limitation of pod need to be considered when building large cloud-based application. Pods are \\naggregated into pools within IaaS region or site called an availability zone. \\n\\uf0b7 \\nWhen the computing infrastructure isolates user clouds from one another, so that interoperating is \\nimpossible this creates an information silo, or simply silo. \\nAWS Services \\n\\uf0b7 \\nThis AWS services list covers the huge catalog of services offered by Amazon Web Services (AWS). These \\nservices range from the core compute products like EC2 to newer releases like AWS Deepracer for \\nmachine learning. \\n\\uf0b7 \\nThere are currently 190 unique services provided by AWS which divided into 24 categories which are \\nlisted below: \\no Analytics \\no Application Integration \\no AR & VR \\no AWS Cost Management \\no Blockchain \\no Business Applications \\no Compute \\no Customer Engagement \\no Database \\no Developer Tools \\no End User Computing \\no Game Tech \\no Internet of Things \\no Machine Learning \\no Management & Governance \\no Media Services \\no Migration & Transfer \\no Mobile \\no Networking & Content Delivery \\no Robotics \\no Satellite \\no Security, Identity, & Compliance \\no Storage \\no Quantum Technologies \\n\\n \\nUnit-3 – Introduction to AWS \\n \\n \\n5 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nAWS Ecosystem \\n\\uf0b7 \\nIn general a cloud ecosystem is a complex system of interdependent components that all work together \\nto enable cloud services. In cloud computing, the ecosystem consists of hardware and software as well \\nas cloud customers, cloud engineers, consultants, integrators and partners. \\n\\uf0b7 \\nAmazon Web Services (AWS) is the market leader in IaaS (Infrastructure-as-a-Service) and PaaS \\n(Platform-as-a-Service) for cloud ecosystems, which can be combined to create a scalable cloud \\napplication without worrying about delays related to infrastructure provisioning (compute, storage, and \\nnetwork) and management. \\n\\uf0b7 \\nWith AWS you can select the specific solutions you need, and only pay for exactly what you use, \\nresulting in lower capital expenditure and faster time to value without sacrificing application \\nperformance or user experience. \\n\\uf0b7 \\nNew and existing companies can build their digital infrastructure partially or entirely in the cloud with \\nAWS, making the on premise data center a thing of the past.  \\n\\uf0b7 \\nThe AWS cloud ensures infrastructure reliability, compliance with security standards, and the ability to \\ninstantly grow or shrink your infrastructure to meet your needs and maximize your budget, all without \\nupfront investment in equipment. \\n\\n \\nUnit-4 – Programming, Management console \\nand Storage on AWS \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nBasic Understanding APIs \\n\\uf0b7 \\nAmazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing \\nREST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other \\nweb services, as well as data stored in the AWS Cloud. As an API Gateway API developer, you can create \\nAPIs for use in your own client applications. Or you can make your APIs available to third-party app \\ndevelopers. For more information, see Who uses API Gateway? \\n\\uf0b7 \\nAPI Gateway creates RESTful APIs that: \\no Are HTTP-based. \\no Enable stateless client-server communication. \\no Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE. \\nAWS Programming Interfaces \\nAPI Gateway \\n\\uf0b7 \\nAPI Gateway is an AWS service that supports the following: \\no Creating, deploying, and managing a RESTful application programming interface (API) to expose \\nbackend HTTP endpoints, AWS Lambda functions, or other AWS services. \\no Creating, deploying, and managing a WebSocket API to expose AWS Lambda functions or other AWS \\nservices. \\no Invoking exposed API methods through the frontend HTTP and WebSocket endpoints. \\nAPI Gateway REST API \\n\\uf0b7 \\nA collection of HTTP resources and methods that are integrated with backend HTTP endpoints, Lambda \\nfunctions, or other AWS services. You can deploy this collection in one or more stages. Typically, API \\nresources are organized in a resource tree according to the application logic. Each API resource can expose \\none or more API methods that have unique HTTP verbs supported by API Gateway. \\nAPI Gateway HTTP API \\n\\uf0b7 \\nA collection of routes and methods that are integrated with backend HTTP endpoints or Lambda functions. \\nYou can deploy this collection in one or more stages. Each route can expose one or more API methods that \\nhave unique HTTP verbs supported by API Gateway. \\nAPI Gateway WebSocket API \\n\\uf0b7 \\nA collection of WebSocket routes and route keys that are integrated with backend HTTP endpoints, \\nLambda functions, or other AWS services. You can deploy this collection in one or more stages. API \\nmethods are invoked through frontend WebSocket connections that you can associate with a registered \\ncustom domain name. \\nAPI Deployment \\n\\uf0b7 \\nA point-in-time snapshot of your API Gateway API. To be available for clients to use, the deployment must \\nbe associated with one or more API stages. \\nAPI Developer \\n\\uf0b7 \\nYour AWS account that owns an API Gateway deployment (for example, a service provider that also \\nsupports programmatic access). \\n\\n \\nUnit-4 – Programming, Management console \\nand Storage on AWS \\n \\n \\n2 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nAPI Endpoint \\n\\uf0b7 \\nA hostname for an API in API Gateway that is deployed to a specific Region. The hostname is of the form \\n{api-id}.execute-api.{region}.amazonaws.com. The following types of API endpoints are supported: \\no Edge-optimized API endpoint \\n\\uf0a7 \\nThe default hostname of an API Gateway API that is deployed to the specified Region while using a \\nCloudFront distribution to facilitate client access typically from across AWS Regions. API requests \\nare routed to the nearest CloudFront Point of Presence (POP), which typically improves connection \\ntime for geographically diverse clients. \\no Private API endpoint \\n\\uf0a7 \\nAn API endpoint that is exposed through interface VPC endpoints and allows a client to securely \\naccess private API resources inside a VPC. Private APIs are isolated from the public internet, and \\nthey can only be accessed using VPC endpoints for API Gateway that have been granted access. \\no Regional API endpoint \\n\\uf0a7 \\nThe host name of an API that is deployed to the specified Region and intended to serve clients, \\nsuch as EC2 instances, in the same AWS Region. API requests are targeted directly to the Region-\\nspecific API Gateway API without going through any CloudFront distribution. For in-Region \\nrequests, a Regional endpoint bypasses the unnecessary round trip to a CloudFront distribution. \\nAPI Key \\n\\uf0b7 \\nAn alphanumeric string that API Gateway uses to identify an app developer who uses your REST or \\nWebSocket API. API Gateway can generate API keys on your behalf, or you can import them from a CSV \\nfile. You can use API keys together with Lambda authorizers or usage plans to control access to your APIs. \\nWebSocket Connection \\n\\uf0b7 \\nAPI Gateway maintains a persistent connection between clients and API Gateway itself. There is no \\npersistent connection between API Gateway and backend integrations such as Lambda functions. Backend \\nservices are invoked as needed, based on the content of messages received from clients. \\nWeb Services \\n\\uf0b7 \\nYou can choose from a couple of different schools of thought for how web services should be delivered.  \\n\\uf0b7 \\nThe older approach, SOAP (short for Simple Object Access Protocol), had widespread industry support, \\ncomplete with a comprehensive set of standards.  \\n\\uf0b7 \\nThose standards were too comprehensive, unfortunately. The people designing SOAP set it up to be \\nextremely flexible —it can communicate across the web, e-mail, and private networks.  \\n\\uf0b7 \\nTo ensure security and manageability, a number of supporting standards that integrate with SOAP were \\nalso defined. \\n\\uf0b7 \\nSOAP is based on a document encoding standard known as Extensible Markup Language (XML, for short), \\nand the SOAP service is defined in such a way that users can then leverage XML no matter what the \\nunderlying communication network is.  \\n\\uf0b7 \\nFor this system to work, though, the data transferred by SOAP (commonly referred to as the payload) also \\nneeds to be in XML format. \\n\\uf0b7 \\nNotice a pattern here? The push to be comprehensive and flexible (or, to be all things to all people) plus \\nthe XML payload requirement meant that SOAP ended up being quite complex, making it a lot of work to \\nuse properly.  \\n\\uf0b7 \\nAs you might guess, many IT people found SOAP daunting and, consequently, resisted using it. \\n\\n \\nUnit-4 – Programming, Management console \\nand Storage on AWS \\n \\n \\n3 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nAbout a decade ago, a doctoral student defined another web services approach as part of his thesis: REST, \\nor Representational State Transfer. \\n\\uf0b7 \\nREST, which is far less comprehensive than SOAP, aspires to solve fewer problems.  \\n\\uf0b7 \\nIt doesn’t address some aspects of SOAP that seemed important but that, in retrospect, made it more \\ncomplex to use — security, for example.  \\n\\uf0b7 \\nThe most important aspect of REST is that it’s designed to integrate with standard web protocols so that \\nREST services can be called with standard web verbs and URLs.  \\n\\uf0b7 \\nFor example, a valid REST call looks like this: \\nhttp://search.examplecompany.com/CompanyDirectory/EmployeeInfo?empname=BernardGolden \\n\\uf0b7 \\nThat’s all it takes to make a query to the REST service of examplecompany to see my personnel \\ninformation.  \\n\\uf0b7 \\nThe HTTP verb that accompanies this request is GET, asking for information to be returned.  \\n\\uf0b7 \\nTo delete information, you use the verb DELETE.  \\n\\uf0b7 \\nTo insert my information, you use the verb POST. \\n\\uf0b7 \\nTo update my information, you use the verb PUT. \\n\\uf0b7 \\nFor the POST and PUT actions, additional information would accompany the empname and be separated \\nby an ampersand (&) to indicate another argument to be used by the service. \\n\\uf0b7 \\nREST imposes no particular formatting requirements on the service payloads. In this respect, it differs from \\nSOAP, which requires XML.  \\n\\uf0b7 \\nFor simple interactions, a string of bytes is all you need for the payload. For more complex interactions \\n(say, in addition to returning my employee information, I want to place a request for the employee \\ninformation of all employees whose names start with G), the encoding convention JSON is used. \\n\\uf0b7 \\nAs you might expect, REST’s simpler use model, its alignment with standard web protocols and verbs, and \\nits less restrictive payload formatting made it catch on with developers like a house on fire. \\n\\uf0b7 \\nAWS originally launched with SOAP support for interactions with its API, but it has steadily deprecated its \\nSOAP interface in favor of REST.  \\nAWS URL Naming \\n\\uf0b7 \\nYou can access your bucket using the Amazon S3 console. Using the console UI, you can perform almost \\nall bucket operations without having to write any code. \\n\\uf0b7 \\nIf you access a bucket programmatically, note that Amazon S3 supports RESTful architecture in which your \\nbuckets and objects are resources, each with a resource URI that uniquely identifies the resource. \\n\\uf0b7 \\nAmazon S3 supports both virtual-hosted–style and path-style URLs to access a bucket. \\n\\uf0b7 \\nIn a virtual-hosted–style URL, the bucket name is part of the domain name in the URL. For example: \\no http://bucket.s3.amazonaws.com \\no http://bucket.s3-aws-region.amazonaws.com \\n\\uf0b7 \\nIn a virtual-hosted–style URL, you can use either of these endpoints.  \\n\\uf0b7 \\nIf you make a request to the http://bucket.s3.amazonaws.com endpoint, the DNS has sufficient \\ninformation to route your request directly to the Region where your bucket resides. \\n\\uf0b7 \\nIn a path-style URL, the bucket name is not part of the domain (unless you use a Region-specific endpoint). \\nFor example: \\no US East (N. Virginia) Region endpoint, http://s3.amazonaws.com/bucket \\no Region-specific endpoint, http://s3-aws-region.amazonaws.com/bucket \\n\\uf0b7 \\nIn a path-style URL, the endpoint you use must match the Region in which the bucket resides.  \\n\\n \\nUnit-4 – Programming, Management console \\nand Storage on AWS \\n \\n \\n4 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nFor example, if your bucket is in the South America (São Paulo) Region, you must use the http://s3-sa-\\neast-1.amazonaws.com/bucket endpoint. If your bucket is in the US East (N. Virginia) Region, you must \\nuse the http://s3.amazonaws.com/bucket endpoint. \\nMatching Interfaces and Services \\n\\uf0b7 \\nIn the simplest case, the service interfaces are identical apart from name and category (inbound or \\noutbound), that is, if the outbound service interface and all interface objects, are referenced by this service \\ninterface, are copies of the corresponding objects of an inbound service interfaces.  \\n\\uf0b7 \\nIf, however, the consumer only wants to call one operation of the inbound service interface, for example, \\nit is not necessary to create the other inbound service interface operations in the outbound service \\ninterface as well. \\n\\uf0b7 \\nSimply put, the operations and corresponding outbound service interface data structures can be a subset \\nof the operations and corresponding data structures of the inbound interface referenced.  \\n\\uf0b7 \\nThe service interface editor provides a check for the service interface pairs to determine the compatibility \\nof the inbound and outbound service interface.  \\n\\uf0b7 \\nThis check is performed in multiple steps to determine compatibility (starting service interfaces, across \\noperations and down to data types).  \\n\\uf0b7 \\nThe following section describes the steps to be able to estimate for a service interface assignment whether \\ntwo service interfaces match. \\nMatching Service Interfaces \\n\\uf0b7 \\nTwo service interfaces match each other if the following conditions are fulfilled: \\no One service interface is of the outbound category and the other service interface if of the inbound \\ncategory. Neither of the service interfaces can be abstract.  \\no Both of the service interfaces have the same interface pattern. \\no There is a matching operation in the inbound service interface for each of the operations in the \\noutbound service interface. \\nMatching Operations \\n\\uf0b7 \\nAn inbound service interface operation matches an outbound service interface operation (and the other \\nway around) if the following conditions are met: \\no Both operations must have the name mode (asynchronous or synchronous). \\no Both operations must have the same Operation Pattern. \\no The message type for the request, which must be referenced by each operation, must have the same \\nname and same XML Namespace. The names of the operations may differ. The same applies for the \\nresponse with synchronous communication. \\no If the inbound service interface operation references a fault message type, the outbound service \\ninterface operation must also reference a fault message type with the same name and XML \\nNamespace. \\no The data types of the message types, which the outbound service interface for the request message \\nreferences (and, if necessary, for the response and fault message) must be compatible with the \\ncorresponding inbound service interface data types. \\nMatching Data Types \\n\\uf0b7 \\nThe check whether the corresponding data types are compatible with each other is sufficient until the \\ncomparison of the Facets of an XSD type.  \\n\\n \\nUnit-4 – Programming, Management console \\nand Storage on AWS \\n \\n \\n5 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nThe data types are compared using the same method as other objects: The structures are compatible if \\nthey contain the same fields (elements and attributes) and if these fields have compatible types, \\nfrequencies, details, and default values.  \\n\\uf0b7 \\nThere are however a few restraints, for example the target structure can contain attributes or elements \\nthat do not appear in the outbound structure, but if these are not required and where the frequency is \\noptional or prohibited (attributes) or minOccurs=0 (elements). \\no The data structures compared must both be correct. For example, not all correct facets are skipped or \\nconsidered in the compatibility check. \\no Some XSD schema language elements that can appear in a reference to an external message in the data \\nstructure are not supported. Therefore, the elements redefine and any, for example, as well as the \\nattributes blockDefault, finalDefault, and substitutionGroup. \\no The comparison of structures is, for example, restricted to the following: \\n\\uf0a7 \\nThe details white Space and pattern are not checked \\n\\uf0a7 \\nIf the facet pattern is used for the outbound structure field, all the other details are not checked. \\n\\uf0a7 \\nIf the order of sub elements if different between the outbound and target field, a warning is \\ndisplayed. \\nElastic Block Store \\n\\uf0b7 \\nAmazon Elastic Block Store is an AWS block storage system that is best used for storing persistent data.  \\n\\uf0b7 \\nOften incorrectly referred to as Elastic Block Storage, Amazon EBS provides highly available block level \\nstorage volumes for use with Amazon Elastic Compute Cloud (EC2) instances. \\n\\uf0b7 \\nAn EC2 instance is a virtual server in Amazon\\'s Elastic Compute Cloud (EC2) for running applications on \\nthe Amazon Web Services (AWS) infrastructure. \\n\\uf0b7 \\nTo begin, create an EBS volume (General Purpose, Provisioned IOPS or Magnetic), pick a size for it (up to \\na terabyte of data) and attach that to any one of your EC2 instances.  \\n\\uf0b7 \\nAn EBS volume can only be attached to one instance at a time but if you need to have multiple copies of \\nthe volume, you can take a snapshot and create another volume from that snapshot and attach it to \\nanother drive.  \\n\\uf0b7 \\nA snapshot file is equivalent to a backup of whatever the EBS volume looks like at the time. For every \\nsnapshot you create, you can make an identical EC2 instance. This will allow you to publish identical \\ncontent on multiple servers. \\n\\uf0b7 \\nAmazon EBS is ideal if you’re doing any substantial work with EC2, you want to keep data persistently on \\na file system, and you want to keep that data around even after you shut down your EC2 instance. \\n\\uf0b7 \\nEC2 instances have local storage that you can use as long as you’re running the instance, but as soon as \\nyou shut down the instance you lose the data that was on there.  \\n\\uf0b7 \\nIf you want to save anything, you need to save it on Amazon EBS. Because EC2 is like having a local drive \\non the machine, you can access and read the EBS volumes anytime once you attach the file to an EC2 \\ninstance. \\nAmazon Simple Storage Service (S3) \\n\\uf0b7 \\nAmazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, \\nat any time, from anywhere on the web. \\n\\uf0b7 \\nAmazon S3 is intentionally built with a minimal feature set that focuses on simplicity and robustness.  \\n\\uf0b7 \\nFollowing are some of advantages of the Amazon S3 service: \\n\\n \\nUnit-4 – Programming, Management console \\nand Storage on AWS \\n \\n \\n6 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\no Create Buckets – Create and name a bucket that stores data. Buckets are the fundamental container \\nin Amazon S3 for data storage. \\no Store data in Buckets – Store an infinite amount of data in a bucket. Upload as many objects as you \\nlike into an Amazon S3 bucket. Each object can contain up to 5 TB of data. Each object is stored and \\nretrieved using a unique developer-assigned key. \\no Download data – Download your data any time you like or allow others to do the same. \\no Permissions – Grant or deny access to others who want to upload or download data into your Amazon \\nS3 bucket. \\no Standard interfaces – Use standards-based REST and SOAP interfaces designed to work with any \\nInternet-development toolkit. \\nAmazon S3 Application Programming Interfaces (API) \\n\\uf0b7 \\nThe Amazon S3 architecture is designed to be programming language-neutral, using their supported \\ninterfaces to store and retrieve objects. \\n\\uf0b7 \\nAmazon S3 provides a REST and a SOAP interface.  \\n\\uf0b7 \\nThey are similar, but there are some differences. For example, in the REST interface, metadata is returned \\nin HTTP headers. Because we only support HTTP requests of up to 4 KB (not including the body), the \\namount of metadata you can supply is restricted. \\nThe REST Interface \\n\\uf0b7 \\nThe REST API is an HTTP interface to Amazon S3.  \\n\\uf0b7 \\nUsing REST, you use standard HTTP requests to create, fetch, and delete buckets and objects. \\n\\uf0b7 \\nYou can use any toolkit that supports HTTP to use the REST API.  \\n\\uf0b7 \\nYou can even use a browser to fetch objects, as long as they are anonymously readable. \\n\\uf0b7 \\nThe REST API uses the standard HTTP headers and status codes, so that standard browsers and toolkits \\nwork as expected.  \\n\\uf0b7 \\nIn some areas, they have added functionality to HTTP (for example, we added headers to support access \\ncontrol).  \\nThe SOAP Interface \\n\\uf0b7 \\nSOAP support over HTTP is deprecated, but it is still available over HTTPS.  \\n\\uf0b7 \\nNew Amazon S3 features will not be supported for SOAP.  \\n\\uf0b7 \\nThe SOAP API provides a SOAP 1.1 interface using document literal encoding.  \\n\\uf0b7 \\nThe most common way to use SOAP is to download the WSDL, and use a SOAP toolkit such as Apache Axis \\nor Microsoft .NET to create bindings, and then write code that uses the bindings to call Amazon S3. \\nOperations we can execute through API \\n\\uf0b7 \\nLogin into Amazon S3. \\n\\uf0b7 \\nUploading. \\n\\uf0b7 \\nRetrieving. \\n\\uf0b7 \\nDeleting etc. \\nAmazon Glacier (Now Amazon S3 Glacier) - Content Delivery Platforms \\n\\uf0b7 \\nAmazon Glacier is an extremely low-cost storage service that provides secure, durable, and flexible storage \\nfor data backup and archival.  \\n\\uf0b7 \\nWith Amazon Glacier, customers can reliably store their data for as little as $0.004 per gigabyte per month.  \\n\\n \\nUnit-4 – Programming, Management console \\nand Storage on AWS \\n \\n \\n7 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nAmazon Glacier enables customers to offload the administrative burdens of operating and scaling storage \\nto AWS, so that they don’t have to worry about capacity planning, hardware provisioning, data replication, \\nhardware failure detection and repair, or time-consuming hardware migrations. \\n\\uf0b7 \\nAmazon Glacier enables any business or organization to easily and cost effectively retain data for months, \\nyears, or decades.  \\n\\uf0b7 \\nWith Amazon Glacier, customers can now cost effectively retain more of their data for future analysis or \\nreference, and they can focus on their business rather than operating and maintaining their storage \\ninfrastructure.  \\n\\uf0b7 \\nCustomers seeking compliance storage can deploy compliance controls using Vault Lock to meet \\nregulatory and compliance archiving requirements. \\nBenefits of Glacier Storage Service. \\n1. Retrievals as Quick as 1-5 Minutes \\n\\uf0b7 \\nAmazon Glacier provides three retrieval options to fit your use case. Expedited retrievals typically return \\ndata in 1-5 minutes, and are great for Active Archive use cases. Standard retrievals typically complete \\nbetween 3-5 hours’ work, and work well for less time-sensitive needs like backup data, media editing, or \\nlong-term analytics. Bulk retrievals are the lowest-cost retrieval option, returning large amounts of data \\nwithin 5-12 hours. \\n2. Unmatched Durability & Scalability \\n\\uf0b7 \\nAmazon Glacier runs on the world’s largest global cloud infrastructure, and was designed for \\n99.999999999% of durability. Data is automatically distributed across a minimum of three physical \\nAvailability Zones that are geographically separated within an AWS Region, and Amazon Glacier can also \\nautomatically replicate data to any other AWS Region. \\n3. Most Comprehensive Security & Compliance Capabilities \\n\\uf0b7 \\nAmazon Glacier offers sophisticated integration with AWS CloudTrail to log, monitor and retain storage \\nAPI call activities for auditing, and supports three different forms of encryption. Amazon Glacier also \\nsupports security standards and compliance certifications including SEC Rule 17a-4, PCI-DSS, \\nHIPAA/HITECH, FedRAMP, EU Data Protection Directive, and FISMA, and Amazon Glacier Vault Lock \\nenables WORM storage capabilities, helping satisfy compliance requirements for virtually every regulatory \\nagency around the globe. \\n4. Low Cost \\n\\uf0b7 \\nAmazon Glacier is designed to be the lowest cost AWS object storage class, allowing you to archive large \\namounts of data at a very low cost. This makes it feasible to retain all the data you want for use cases like \\ndata lakes, analytics, IoT, machine learning, compliance, and media asset archiving. You pay only for what \\nyou need, with no minimum commitments or up-front fees. \\n5. Most Supported Platform with the Largest Ecosystem \\n\\uf0b7 \\nIn addition to integration with most AWS services, the Amazon object storage ecosystem includes tens of \\nthousands of consulting, systems integrator and independent software vendor partners, with more joining \\nevery month. And the AWS Marketplace offers 35 categories and more than 3,500 software listings from \\nover 1,100 ISVs that are pre-configured to deploy on the AWS Cloud. AWS Partner Network partners have \\nadapted their services and software to work with Amazon S3 and Amazon Glacier for solutions like Backup \\n& Recovery, Archiving, and Disaster Recovery. No other cloud provider has more partners with solutions \\nthat are pre-integrated to work with their service. \\n\\n \\nUnit-4 – Programming, Management console \\nand Storage on AWS \\n \\n \\n8 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n6. Query in Place \\n\\uf0b7 \\nAmazon Glacier is the only cloud archive storage service that allows you to query data in place and retrieve \\nonly the subset of data you need from within an archive. Amazon Glacier Select helps you reduce the total \\ncost of ownership by extending your data lake into cost-effective archive storage. \\n\\n \\nUnit-5 – AWS identity services, security and \\ncompliance \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nIdentity Management and Access Management (IAM) \\n\\uf0b7 \\nIdentity and access management (IAM) is a framework for business processes that facilitates the \\nmanagement of electronic or digital identities.  \\n\\uf0b7 \\nThe framework includes the organizational policies for managing digital identity as well as the technologies \\nneeded to support identity management. \\n\\uf0b7 \\nWith IAM technologies, IT managers can control user access to critical information within their \\norganizations.  \\n\\uf0b7 \\nIdentity and access management products offer role-based access control, which lets system \\nadministrators regulate access to systems or networks based on the roles of individual users within the \\nenterprise. \\n\\uf0b7 \\nIn this context, access is the ability of an individual user to perform a specific task, such as view, create or \\nmodify a file.  \\n\\uf0b7 \\nRoles are defined according to job competency, authority and responsibility within the enterprise. \\n\\uf0b7 \\nSystems used for identity and access management include single sign-on systems, multifactor \\nauthentication and access management.  \\n\\uf0b7 \\nThese technologies also provide the ability to securely store identity and profile data as well as data \\ngovernance functions to ensure that only data that is necessary and relevant is shared. \\n\\uf0b7 \\nThese products can be deployed on premises, provided by a third party vendor via a cloud-based \\nsubscription model or deployed in a hybrid cloud. \\nHow Does IAM Work? \\n\\uf0b7 \\nThe IAM workflow includes the following six elements: \\n1. A principal is an entity that can perform actions on an AWS resource. A user, a role or an application \\ncan be a principal. \\n2. Authentication is the process of confirming the identity of the principal trying to access an AWS \\nproduct. The principal must provide its credentials or required keys for authentication. \\n3. Request: A principal sends a request to AWS specifying the action and which resource should perform \\nit. \\n4. Authorization: By default, all resources are denied. IAM authorizes a request only if all parts of the \\nrequest are allowed by a matching policy. After authenticating and authorizing the request, AWS \\napproves the action. \\n5. Actions are used to view, create, edit or delete a resource. \\n6. Resources: A set of actions can be performed on a resource related to your AWS account. \\nIdentities (Users, Groups, and Roles) \\n\\uf0b7 \\nIAM identities, which you create to provide authentication for people and processes in your AWS account.  \\n\\uf0b7 \\nIAM groups, which are collections of IAM users that you can manage as a unit.  \\n\\uf0b7 \\nIdentities represent the user, and can be authenticated and then authorized to perform actions in AWS.  \\n\\uf0b7 \\nEach of these can be associated with one or more policies to determine what actions a user, role, or \\nmember of a group can do with which AWS resources and under what conditions. \\nThe AWS Account Root User \\n\\uf0b7 \\nWhen you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity \\nthat has complete access to all AWS services and resources in the account.  \\n\\n \\nUnit-5 – AWS identity services, security and \\ncompliance \\n \\n \\n2 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nThis identity is called the AWS account root user and is accessed by signing in with the email address and \\npassword that you used to create the account. \\nIAM Users \\n\\uf0b7 \\nAn IAM user is an entity that you create in AWS.  \\n\\uf0b7 \\nThe IAM user represents the person or service who uses the IAM user to interact with AWS.  \\n\\uf0b7 \\nA primary use for IAM users is to give people the ability to sign in to the AWS Management Console for \\ninteractive tasks and to make programmatic requests to AWS services using the API or CLI.  \\n\\uf0b7 \\nA user in AWS consists of a name, a password to sign into the AWS Management Console, and up to two \\naccess keys that can be used with the API or CLI.  \\n\\uf0b7 \\nWhen you create an IAM user, you grant it permissions by making it a member of a group that has \\nappropriate permission policies attached (recommended), or by directly attaching policies to the user.  \\n\\uf0b7 \\nYou can also clone the permissions of an existing IAM user, which automatically makes the new user a \\nmember of the same groups and attaches all the same policies. \\nIAM Groups \\n\\uf0b7 \\nAn IAM group is a collection of IAM users.  \\n\\uf0b7 \\nYou can use groups to specify permissions for a collection of users, which can make those permissions \\neasier to manage for those users.  \\n\\uf0b7 \\nFor example, you could have a group called Admins and give that group the types of permissions that \\nadministrators typically need.  \\n\\uf0b7 \\nAny user in that group automatically has the permissions that are assigned to the group. If a new user joins \\nyour organization and should have administrator privileges, you can assign the appropriate permissions \\nby adding the user to that group.  \\n\\uf0b7 \\nSimilarly, if a person changes jobs in your organization, instead of editing that user\\'s permissions, you can \\nremove him or her from the old groups and add him or her to the appropriate new groups.  \\n\\uf0b7 \\nNote that a group is not truly an identity because it cannot be identified as a Principal in a resource-based \\nor trust policy. It is only a way to attach policies to multiple users at one time. \\nIAM Roles \\n\\uf0b7 \\nAn IAM role is very similar to a user, in that it is an identity with permission policies that determine what \\nthe identity can and cannot do in AWS.  \\n\\uf0b7 \\nHowever, a role does not have any credentials (password or access keys) associated with it.  \\n\\uf0b7 \\nInstead of being uniquely associated with one person, a role is intended to be assumable by anyone who \\nneeds it.  \\n\\uf0b7 \\nAn IAM user can assume a role to temporarily take on different permissions for a specific task.  \\n\\uf0b7 \\nA role can be assigned to a federated user who signs in by using an external identity provider instead of \\nIAM.  \\n\\uf0b7 \\nAWS uses details passed by the identity provider to determine which role is mapped to the federated user. \\nTemporary Credentials \\n\\uf0b7 \\nTemporary credentials are primarily used with IAM roles, but there are also other uses.  \\n\\uf0b7 \\nYou can request temporary credentials that have a more restricted set of permissions than your standard \\nIAM user.  \\n\\uf0b7 \\nThis prevents you from accidentally performing tasks that are not permitted by the more restricted \\ncredentials.  \\n\\n \\nUnit-5 – AWS identity services, security and \\ncompliance \\n \\n \\n3 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nA benefit of temporary credentials is that they expire automatically after a set period of time.  \\n\\uf0b7 \\nYou have control over the duration that the credentials are valid. \\nSecurity Policies \\n\\uf0b7 \\nYou manage access in AWS by creating policies and attaching them to IAM identities (users, groups of \\nusers, or roles) or AWS resources.  \\n\\uf0b7 \\nA policy is an object in AWS that, when associated with an identity or resource, defines their permissions.  \\n\\uf0b7 \\nAWS evaluates these policies when an IAM principal (user or role) makes a request.  \\n\\uf0b7 \\nPermissions in the policies determine whether the request is allowed or denied.  \\n\\uf0b7 \\nMost policies are stored in AWS as JSON documents.  \\n\\uf0b7 \\nAWS supports six types of policies. \\n\\uf0b7 \\nIAM policies define permissions for an action regardless of the method that you use to perform the \\noperation.  \\n\\uf0b7 \\nFor example, if a policy allows the GetUser action, then a user with that policy can get user information \\nfrom the AWS Management Console, the AWS CLI, or the AWS API.  \\n\\uf0b7 \\nWhen you create an IAM user, you can choose to allow console or programmatic access. If console access \\nis allowed, the IAM user can sign in to the console using a user name and password. Or if programmatic \\naccess is allowed, the user can use access keys to work with the CLI or API. \\nPolicy Types \\n\\uf0b7 \\nIdentity-based policies – Attach managed and inline policies to IAM identities (users, groups to which \\nusers belong, or roles). Identity-based policies grant permissions to an identity. \\n\\uf0b7 \\nResource-based policies – Attach inline policies to resources. The most common examples of resource-\\nbased policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant \\npermissions to the principal that is specified in the policy. Principals can be in the same account as the \\nresource or in other accounts. \\n\\uf0b7 \\nPermissions boundaries – Use a managed policy as the permissions boundary for an IAM entity (user or \\nrole). That policy defines the maximum permissions that the identity-based policies can grant to an entity, \\nbut does not grant permissions. Permissions boundaries do not define the maximum permissions that a \\nresource-based policy can grant to an entity. \\n\\uf0b7 \\nOrganizations SCPs – Use an AWS Organizations service control policy (SCP) to define the maximum \\npermissions for account members of an organization or organizational unit (OU). SCPs limit permissions \\nthat identity-based policies or resource-based policies grant to entities (users or roles) within the account, \\nbut do not grant permissions. \\n\\uf0b7 \\nAccess control lists (ACLs) – Use ACLs to control which principals in other accounts can access the resource \\nto which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy \\ntype that does not use the JSON policy document structure. ACLs are cross-account permissions policies \\nthat grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same \\naccount. \\n\\uf0b7 \\nSession policies – Pass advanced session policies when you use the AWS CLI or AWS API to assume a role \\nor a federated user. Session policies limit the permissions that the role or user\\'s identity-based policies \\ngrant to the session. Session policies limit permissions for a created session, but do not grant permissions. \\nFor more information, see Session Policies. \\n\\n \\nUnit-5 – AWS identity services, security and \\ncompliance \\n \\n \\n4 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nIAM Abilities/Features \\n\\uf0b7 \\nShared access to the AWS account. The main feature of IAM is that it allows you to create separate \\nusernames and passwords for individual users or resources and delegate access. \\n\\uf0b7 \\nGranular permissions. Restrictions can be applied to requests. For example, you can allow the user to \\ndownload information, but deny the user the ability to update information through the policies. \\n\\uf0b7 \\nMultifactor authentication (MFA). IAM supports MFA, in which users provide their username and \\npassword plus a one-time password from their phone—a randomly generated number used as an \\nadditional authentication factor. \\n\\uf0b7 \\nIdentity Federation. If the user is already authenticated, such as through a Facebook or Google account, \\nIAM can be made to trust that authentication method and then allow access based on it. This can also be \\nused to allow users to maintain just one password for both on-premises and cloud environment work. \\n\\uf0b7 \\nFree to use. There is no additional charge for IAM security. There is no additional charge for creating \\nadditional users, groups or policies. \\n\\uf0b7 \\nPCI DSS compliance. The Payment Card Industry Data Security Standard is an information security \\nstandard for organizations that handle branded credit cards from the major card schemes. IAM complies \\nwith this standard. \\n\\uf0b7 \\nPassword policy. The IAM password policy allows you to reset a password or rotate passwords remotely. \\nYou can also set rules, such as how a user should pick a password or how many attempts a user may \\nmake to provide a password before being denied access. \\nIAM Limitations \\n\\uf0b7 \\nNames of all IAM identities and IAM resources can be alphanumeric. They can include common characters \\nsuch as plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). \\n\\uf0b7 \\nNames of IAM identities (users, roles, and groups) must be unique within the AWS account. So you can\\'t \\nhave two groups named DEVELOPERS and developers in your AWS account. \\n\\uf0b7 \\nAWS account ID aliases must be unique across AWS products in your account. It cannot be a 12 digit \\nnumber. \\n\\uf0b7 \\nYou cannot create more than 100 groups in an AWS account. \\n\\uf0b7 \\nYou cannot create more than 5000 users in an AWS account. AWS recommends the use of temporary \\nsecurity credentials for adding a large number of users in an AWS account. \\n\\uf0b7 \\nYou cannot create more than 500 roles in an AWS account. \\n\\uf0b7 \\nAn IAM user cannot be a member of more than 10 groups. \\n\\uf0b7 \\nAn IAM user cannot be assigned more than 2 access keys. \\n\\uf0b7 \\nAn AWS account cannot have more than 1000 customer managed policies. \\n\\uf0b7 \\nYou cannot attach more than 10 managed policies to each IAM entity (user, groups, or roles). \\n\\uf0b7 \\nYou cannot store more than 20 server certificates in an AWS account. \\n\\uf0b7 \\nYou cannot have more than 100 SAML providers in an AWS account. \\n\\uf0b7 \\nA policy name should not exceed 128 characters. \\n\\uf0b7 \\nAn alias for an AWS account ID should be between 3 and 63 characters. \\n\\uf0b7 \\nA username and role name should not exceed 64 characters. \\n\\uf0b7 \\nA group name should not exceed 128 characters. \\nAWS Physical and Environmental Security \\n\\uf0b7 \\nAWS data centers are state of the art, utilizing innovative architectural and engineering approaches.  \\n\\n \\nUnit-5 – AWS identity services, security and \\ncompliance \\n \\n \\n5 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nAmazon has many years of experience in designing, constructing, and operating large-scale data centers.  \\n\\uf0b7 \\nThis experience has been applied to the AWS platform and infrastructure.  \\n\\uf0b7 \\nAWS data centers are housed in facilities that are not branded as AWS facilities.  \\n\\uf0b7 \\nPhysical access is strictly controlled both at the perimeter and at building ingress points by professional \\nsecurity staff utilizing video surveillance, intrusion detection systems, and other electronic means. \\n\\uf0b7 \\nAuthorized staff must pass two-factor authentication a minimum of two times to access data center floors. \\nAll visitors are required to present identification and are signed in and continually escorted by authorized \\nstaff. \\n\\uf0b7 \\nAWS only provides data center access and information to employees and contractors who have a \\nlegitimate business need for such privileges.  \\n\\uf0b7 \\nWhen an employee no longer has a business need for these privileges, his or her access is immediately \\nrevoked, even if they continue to be an employee of Amazon or Amazon Web Services.  \\n\\uf0b7 \\nAll physical access to data centers by AWS employees is logged and audited routinely. \\nFire Detection and Suppression \\n\\uf0b7 \\nAutomatic fire detection and suppression equipment has been installed to reduce risk. \\n\\uf0b7 \\nThe fire detection system utilizes smoke detection sensors in all data center environments, mechanical \\nand electrical infrastructure spaces, chiller rooms and generator equipment rooms.  \\n\\uf0b7 \\nThese areas are protected by either wet-pipe, double interlocked pre-action, or gaseous sprinkler systems. \\nPower \\n\\uf0b7 \\nThe data center electrical power systems are designed to be fully redundant and maintainable without \\nimpact to operations, 24 hours a day, and seven days a week. \\n\\uf0b7 \\nUninterruptible Power Supply (UPS) units provide back-up power in the event of an electrical failure for \\ncritical and essential loads in the facility.  \\n\\uf0b7 \\nData centers use generators to provide back-up power for the entire facility. \\nClimate and Temperature \\n\\uf0b7 \\nClimate control is required to maintain a constant operating temperature for servers and other hardware, \\nwhich prevents overheating and reduces the possibility of service outages.  \\n\\uf0b7 \\nData centers are conditioned to maintain atmospheric conditions at optimal levels.  \\n\\uf0b7 \\nPersonnel and systems monitor and control temperature and humidity at appropriate levels. \\nManagement \\n\\uf0b7 \\nAWS monitors electrical, mechanical, and life support systems and equipment so that any issues are \\nimmediately identified.  \\n\\uf0b7 \\nPreventative maintenance is performed to maintain the continued operability of equipment. \\nStorage Device Decommissioning \\n\\uf0b7 \\nWhen a storage device has reached the end of its useful life, AWS procedures include a decommissioning \\nprocess that is designed to prevent customer data from being exposed to unauthorized individuals.  \\n\\uf0b7 \\nAWS uses the techniques detailed in NIST 800-88 (“Guidelines for Media Sanitization”) as part of the \\ndecommissioning process. \\n\\n \\nUnit-5 – AWS identity services, security and \\ncompliance \\n \\n \\n6 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nAWS Compliance Initiatives \\n\\uf0b7 \\nAWS Compliance enables customers to understand the robust controls in place at AWS to maintain \\nsecurity and data protection in the cloud.  \\n\\uf0b7 \\nAs systems are built on top of AWS cloud infrastructure, compliance responsibilities are shared.  \\n\\uf0b7 \\nBy tying together governance-focused, audit friendly service features with applicable compliance or audit \\nstandards, AWS Compliance enablers build on traditional programs; helping customers to establish and \\noperate in an AWS security control environment.  \\n\\uf0b7 \\nThe IT infrastructure that AWS provides to its customers is designed and managed in alignment with \\nsecurity best practices and a variety of IT security standards, including: \\no SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70) \\no SOC 2 \\no SOC 3 \\no FISMA, DIACAP, and FedRAMP \\no DOD CSM Levels 1-5 \\no PCI DSS Level 1 \\no ISO 9001 / ISO 27001 / ISO 27017 / ISO 27018 \\no ITAR \\no FIPS 140-2 \\no MTCS Level 3 \\no HITRUST \\n\\uf0b7 \\nIn addition, the flexibility and control that the AWS platform provides allows customers to deploy solutions \\nthat meet several industry-specific standards, including: \\no Criminal Justice Information Services (CJIS) \\no Cloud Security Alliance (CSA) \\no Family Educational Rights and Privacy Act (FERPA) \\no Health Insurance Portability and Accountability Act (HIPAA) \\no Motion Picture Association of America (MPAA) \\n\\uf0b7 \\nAWS provides a wide range of information regarding its IT control environment to customers through \\nwhite papers, reports, certifications, accreditations, and other third party attestations. \\nUnderstanding Public/Private Keys \\n\\uf0b7 \\nAmazon AWS uses keys to encrypt and decrypt login information. \\n\\uf0b7 \\nAt the basic level, a sender uses a public key to encrypt data, which its receiver then decrypts using another \\nprivate key. These two keys, public and private, are known as a key pair. \\n\\uf0b7 \\nYou need a key pair to be able to connect to your instances. The way this works on Linux and Windows \\ninstances is different. \\n\\uf0b7 \\nFirst, when you launch a new instance, you assign a key pair to it. Then, when you log in to it, you use the \\nprivate key. \\n\\uf0b7 \\nThe difference between Linux and Windows instances is that Linux instances do not have a password \\nalready set and you must use the key pair to log in to Linux instances.  \\n\\uf0b7 \\nOn the other hand, on Windows instances, you need the key pair to decrypt the administrator password. \\nUsing the decrypted password, you can use RDP and then connect to your Windows instance. \\n\\uf0b7 \\nAmazon EC2 stores only the public key, and you can either generate it inside Amazon EC2 or you can \\nimport it.  \\n\\n \\nUnit-5 – AWS identity services, security and \\ncompliance \\n \\n \\n7 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nSince the private key is not stored by Amazon, it’s advisable to store it in a secure place as anyone who \\nhas this private key can log in on your behalf. \\nAWS API Security \\n\\uf0b7 \\nAPI Gateway supports multiple mechanisms of access control, including metering or tracking API uses by \\nclients using API keys.  \\n\\uf0b7 \\nThe standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to \\nan entire API set or individual methods.  \\n\\uf0b7 \\nCustom authorizers and Amazon Cognito user pools provide customizable authorization and \\nauthentication solutions. \\nA. Control Access to an API with IAM Permissions \\n\\uf0b7 \\nYou control access to Amazon API Gateway with IAM permissions by controlling access to the following \\ntwo API Gateway component processes: \\no To create, deploy, and manage an API in API Gateway, you must grant the API developer permissions \\nto perform the required actions supported by the API management component of API Gateway. \\no To call a deployed API or to refresh the API caching, you must grant the API caller permissions to \\nperform required IAM actions supported by the API execution component of API Gateway. \\nB. Use API Gateway Custom Authorizers \\n\\uf0b7 \\nAn Amazon API Gateway custom authorizer is a Lambda function that you provide to control access to \\nyour API methods.  \\n\\uf0b7 \\nA custom authorizer uses bearer token authentication strategies, such as OAuth or SAML. It can also use \\ninformation described by headers, paths, query strings, stage variables, or context variables request \\nparameters. \\n\\uf0b7 \\nWhen a client calls your API, API Gateway verifies whether a custom authorizer is configured for the API \\nmethod. If so, API Gateway calls the Lambda function.  \\n\\uf0b7 \\nIn this call, API Gateway supplies the authorization token that is extracted from a specified request header \\nfor the token-based authorizer, or passes in the incoming request parameters as the input (for example, \\nthe event parameter) to the request parameters-based authorizer function. \\n\\uf0b7 \\nYou can implement various authorization strategies, such as JSON Web Token (JWT) verification and \\nOAuth provider callout.  \\n\\uf0b7 \\nYou can also implement a custom scheme based on incoming request parameter values, to return IAM \\npolicies that authorize the request. If the returned policy is invalid or the permissions are denied, the API \\ncall does not succeed.  \\nC. Use Amazon Cognito User Pools \\n\\uf0b7 \\nIn addition to using IAM roles and policies or custom authorizers, you can use an Amazon Cognito user \\npool to control who can access your API in Amazon API Gateway. \\n\\uf0b7 \\nTo use an Amazon Cognito user pool with your API, you must first create an authorizer of the \\nCOGNITO_USER_POOLS type and then configure an API method to use that authorizer.  \\n\\uf0b7 \\nAfter the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access \\ntoken for the user, and then call the API method with one of the tokens, which are typically set to the \\nrequest\\'s Authorization header.  \\n\\uf0b7 \\nThe API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the \\nclient isn\\'t authorized to make the call because the client did not have credentials that could be authorized. \\n\\n \\nUnit-5 – AWS identity services, security and \\ncompliance \\n \\n \\n8 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nD. Use Client-Side SSL Certificates for Authentication by the Backend \\n\\uf0b7 \\nYou can use API Gateway to generate an SSL certificate and use its public key in the backend to verify that \\nHTTP requests to your backend system are from API Gateway.  \\n\\uf0b7 \\nThis allows your HTTP backend to control and accept only requests originating from Amazon API Gateway, \\neven if the backend is publicly accessible. \\n\\uf0b7 \\nThe SSL certificates that are generated by API Gateway are self-signed and only the public key of a \\ncertificate is visible in the API Gateway console or through the APIs. \\nE. Create and Use API Gateway Usage Plans \\n\\uf0b7 \\nAfter you create, test, and deploy your APIs, you can use API Gateway usage plans to extend them as \\nproduct offerings for your customers.  \\n\\uf0b7 \\nYou can provide usage plans to allow specified customers to access selected APIs at agreed-upon request \\nrates and quotas that can meet their business requirements and budget constraints. \\nAWS Security, Identity, & Compliance services \\nCategory \\nUse cases \\nAWS service \\nIdentity & access \\nmanagement \\nSecurely manage access to services and \\nresources \\nAWS Identity & Access Management \\nCloud single-sign-on (SSO) service \\nAWS Single Sign-On \\nIdentity management for your apps \\nAmazon Cognito \\nManaged Microsoft Active Directory \\nAWS Directory Service \\nSimple, secure service to share AWS resources \\nAWS Resource Access Manager \\nDetective \\ncontrols \\nUnified security and compliance center \\nAWS Security Hub \\nManaged threat detection service \\nAmazon GuardDuty \\nAnalyze application security \\nAmazon Inspector \\nInvestigate potential security issues \\nAmazon Detective \\nInfrastructure \\nprotection \\nDDoS protection \\nAWS Shield \\nFilter malicious web traffic \\nAWS Web Application Firewall (WAF) \\nCentral management of firewall rules \\nAWS Firewall Manager \\nData protection \\nDiscover and protect your sensitive data at \\nscale \\nAmazon Macie \\nKey storage and management \\nAWS Key Management Service (KMS) \\nHardware based key storage for regulatory \\ncompliance \\nAWS CloudHSM \\nProvision, manage, and deploy public and \\nprivate SSL/TLS certificates \\nAWS Certificate Manager \\nRotate, manage, and retrieve secrets \\nAWS Secrets Manager \\nCompliance \\nNo cost, self-service portal for on-demand \\naccess to AWS’ compliance reports \\nAWS Artifact \\nDark Web \\n\\uf0b7 \\nThe dark web is a general term for the seedier corners of the web, where people can interact online \\nwithout worrying about the watchful eye of the authorities.  \\n\\uf0b7 \\nUsually, these sites are guarded by encryption mechanisms such as Tor that allow users to visit them \\nanonymously.  \\n\\uf0b7 \\nBut there are also sites that don\\'t rely on Tor, such as password-protected forums where hackers trade \\nsecrets and stolen credit card numbers, that can also be considered part of the dark web. \\n\\n \\nUnit-5 – AWS identity services, security and \\ncompliance \\n \\n \\n9 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nPeople use the dark web for a variety of purposes: buying and selling drugs, discussing hacking techniques \\nand selling hacking services and so forth. \\n\\uf0b7 \\nIt\\'s important to remember that the technologies used to facilitate \"dark web\" activities aren\\'t inherently \\ngood or bad.  \\n\\uf0b7 \\nThe same technologies used by drug dealers to hide their identity can also be used by authorized informers \\nto securely pass information to government agencies. \\n\\n \\nUnit-6 – AWS computing and marketplace \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nElastic Cloud Compute (EC2) \\n\\uf0b7 \\nAmazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute \\ncapacity in the cloud.  \\n\\uf0b7 \\nIt is designed to make web-scale cloud computing easier for developers.  \\n\\uf0b7 \\nAmazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal \\nfriction.  \\n\\uf0b7 \\nIt provides you with complete control of your computing resources and lets you run on Amazon’s proven \\ncomputing environment. \\nAdvantages of EC2 \\n\\uf0b7 \\nIn less than 10 minutes you can rent a slice of Amazon’s vast cloud network and put those computing \\nresources to work on anything from data science to bitcoin mining. \\n\\uf0b7 \\nEC2 offers a number of benefits and advantages over alternatives. Most notably: \\nAffordability \\n\\uf0b7 \\nEC2 allows you to take advantage of Amazon’s enormous scale.  \\n\\uf0b7 \\nYou can pay a very low rate for the resources you use. The smallest EC2 instance can be rented for as little \\nas $.0058 per hour which works out to about $4.18 per month. Of course, instances with more resources \\nare more expensive but this gives you a sense of how affordable EC2 instances are. \\n\\uf0b7 \\nWith EC2 instances, you’re only paying for what you use in terms of compute hours and bandwidth so \\nthere’s little wasted expense. \\nEase of use \\n\\uf0b7 \\nAmazon’s goal with EC2 was to make accessing compute resources low friction and, by and large, they’ve \\nsucceeded.  \\n\\uf0b7 \\nLaunching an instance is simply a matter of logging into the AWS Console, selecting your operating system, \\ninstance type, and storage options.  \\n\\uf0b7 \\nAt most, it’s a 10 minute process and there aren’t any major technical barriers preventing anyone from \\nspinning up an instance, though it may take some technical knowledge to leverage those resources after \\nlaunch. \\nScalability \\n\\uf0b7 \\nYou can easily add EC2 instances as needed, creating your own private cloud of computer resources that \\nperfectly matches your needs.  \\n\\uf0b7 \\nHere at Pagely a common configuration is an EC2 instance to run a WordPress app, an instance to run RDS \\n(a database service), and an EBS so that data can easily be moved and shared between instances as they’re \\nadded. \\n\\uf0b7 \\nAWS offers built-in, rules-based auto scaling so that you can automatically turn instances on or off based \\non demand.  \\n\\uf0b7 \\nThis helps you ensure that you’re never wasting resources but you also have enough resources available \\nto do the job. \\nIntegration \\n\\uf0b7 \\nPerhaps the biggest advantage of EC2, and something no competing solution can claim, is its native \\nintegration with the vast ecosystem of AWS services.  \\n\\n \\nUnit-6 – AWS computing and marketplace \\n \\n \\n2 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nCurrently there are over 170 services. No other cloud network can claim the breadth, depth, and flexibility \\nAWS can. \\nEC2 Image Builder \\n\\uf0b7 \\nEC2 Image Builder simplifies the creation, maintenance, validation, sharing, and deployment of Linux or \\nWindows Server images for use with Amazon EC2 and on-premises. \\n\\uf0b7 \\nKeeping server images up-to-date can be time consuming, resource intensive, and error-prone.  \\n\\uf0b7 \\nCurrently, customers either manually update and snapshot VMs or have teams that build automation \\nscripts to maintain images. \\n\\uf0b7 \\nImage Builder significantly reduces the effort of keeping images up-to-date and secure by providing a \\nsimple graphical interface, built-in automation, and AWS-provided security settings.  \\n\\uf0b7 \\nWith Image Builder, there are no manual steps for updating an image nor do you have to build your own \\nautomation pipeline. \\n\\uf0b7 \\nImage Builder is offered at no cost, other than the cost of the underlying AWS resources used to create, \\nstore, and share the images. \\nAuto Scaling \\n\\uf0b7 \\nAWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, \\npredictable performance at the lowest possible cost.  \\n\\uf0b7 \\nUsing AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple \\nservices in minutes.  \\n\\uf0b7 \\nThe service provides a simple, powerful user interface that lets you build scaling plans for resources \\nincluding Amazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon DynamoDB tables and \\nindexes, and Amazon Aurora Replicas.  \\n\\uf0b7 \\nAWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, \\ncosts, or balance between them.  \\n\\uf0b7 \\nIf you’re already using Amazon EC2 Auto Scaling to dynamically scale your Amazon EC2 instances, you can \\nnow combine it with AWS Auto Scaling to scale additional resources for other AWS services.  \\n\\uf0b7 \\nWith AWS Auto Scaling, your applications always have the right resources at the right time. \\n\\uf0b7 \\nIt’s easy to get started with AWS Auto Scaling using the AWS Management Console, Command Line \\nInterface (CLI), or SDK.  \\n\\uf0b7 \\nAWS Auto Scaling is available at no additional charge. You pay only for the AWS resources needed to run \\nyour applications and Amazon CloudWatch monitoring fees. \\nElastic Load Balancing \\n\\uf0b7 \\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such \\nas Amazon EC2 instances, containers, and IP addresses.  \\n\\uf0b7 \\nIt can handle the varying load of your application traffic in a single Availability Zone or across multiple \\nAvailability Zones.  \\n\\uf0b7 \\nElastic Load Balancing offers three types of load balancers that all feature the high availability, automatic \\nscaling, and robust security necessary to make your applications fault tolerant. \\no Application Load Balancers,  \\no Network Load Balancers, and  \\no Classic Load Balancers.  \\n\\n \\nUnit-6 – AWS computing and marketplace \\n \\n \\n3 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nApplication Load Balancer \\n\\uf0b7 \\nApplication Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides \\nadvanced request routing targeted at the delivery of modern application architectures, including micro \\nservices and containers.  \\n\\uf0b7 \\nOperating at the individual request level (Layer 7), Application Load Balancer routes traffic to targets \\nwithin Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request. \\nNetwork Load Balancer \\n\\uf0b7 \\nNetwork Load Balancer is best suited for load balancing of TCP traffic where extreme performance is \\nrequired.  \\n\\uf0b7 \\nOperating at the connection level (Layer 4), Network Load Balancer routes traffic to targets within Amazon \\nVirtual Private Cloud (Amazon VPC) and is capable of handling millions of requests per second while \\nmaintaining ultra-low latencies.  \\n\\uf0b7 \\nNetwork Load Balancer is also optimized to handle sudden and volatile traffic patterns. \\nClassic Load Balancer \\n\\uf0b7 \\nClassic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at \\nboth the request level and connection level.  \\n\\uf0b7 \\nClassic Load Balancer is intended for applications that were built within the EC2-Classic network. \\nBenefits of Elastic Load Balancing for reducing workload \\nHighly Available \\n\\uf0b7 \\nElastic Load Balancing automatically distributes incoming traffic across multiple targets – Amazon EC2 \\ninstances, containers, and IP addresses – in multiple Availability Zones and ensures only healthy targets \\nreceive traffic. Elastic Load Balancing can also load balance across a Region, routing traffic to healthy \\ntargets in different Availability Zones. \\nSecure \\n\\uf0b7 \\nElastic Load Balancing works with Amazon Virtual Private Cloud (VPC) to provide robust security features, \\nincluding integrated certificate management and SSL decryption. Together, they give you the flexibility to \\ncentrally manage SSL settings and offload CPU intensive workloads from your applications. \\nElastic \\n\\uf0b7 \\nElastic Load Balancing is capable of handling rapid changes in network traffic patterns. Additionally, deep \\nintegration with Auto Scaling ensures sufficient application capacity to meet varying levels of application \\nload without requiring manual intervention. \\nFlexible \\n\\uf0b7 \\nElastic Load Balancing also allows you to use IP addresses to route requests to application targets. This \\noffers you flexibility in how you virtualize your application targets, allowing you to host more applications \\non the same instance. This also enables these applications to have individual security groups and use the \\nsame network port to further simplify inter-application communication in microservices based \\narchitecture. \\nRobust Monitoring and Auditing \\n\\uf0b7 \\nElastic Load Balancing allows you to monitor your applications and their performance in real time with \\nAmazon CloudWatch metrics, logging, and request tracing. This improves visibility into the behavior of \\n\\n \\nUnit-6 – AWS computing and marketplace \\n \\n \\n4 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nyour applications, uncovering issues and identifying performance bottlenecks in your application stack at \\nthe granularity of an individual request. \\nHybrid Load Balancing \\n\\uf0b7 \\nElastic Load Balancing offers ability to load balance across AWS and on-premises resources using the same \\nload balancer. This makes it easy for you to migrate, burst, or failover on-premises applications to the \\ncloud. \\nAMIs \\n\\uf0b7 \\nAn Amazon Machine Image (AMI) provides the information required to launch an instance.  \\n\\uf0b7 \\nYou must specify an AMI when you launch an instance.  \\n\\uf0b7 \\nYou can launch multiple instances from a single AMI when you need multiple instances with the same \\nconfiguration.  \\n\\uf0b7 \\nYou can use different AMIs to launch instances when you need instances with different configurations. \\n\\uf0b7 \\nAn AMI includes the following: \\no One or more EBS snapshots, or, for instance-store-backed AMIs, a template for the root volume of the \\ninstance (for example, an operating system, an application server, and applications). \\no Launch permissions that control which AWS accounts can use the AMI to launch instances. \\no A block device mapping that specifies the volumes to attach to the instance when it\\'s launched. \\nUsing an AMI \\n \\nFig. : The AMI lifecycle (create, register, launch, copy, and deregister) \\n\\uf0b7 \\nThe following diagram summarizes the AMI lifecycle.  \\n\\uf0b7 \\nAfter you create and register an AMI, you can use it to launch new instances. (You can also launch instances \\nfrom an AMI if the AMI owner grants you launch permissions.)  \\n\\uf0b7 \\nYou can copy an AMI within the same Region or to different Regions.  \\n\\uf0b7 \\nWhen you no longer require an AMI, you can deregister it. \\n\\uf0b7 \\nYou can search for an AMI that meets the criteria for your instance.  \\n\\uf0b7 \\nYou can search for AMIs provided by AWS or AMIs provided by the community.  \\n\\uf0b7 \\nAfter you launch an instance from an AMI, you can connect to it.  \\n\\uf0b7 \\nWhen you are connected to an instance, you can use it just like you use any other server.  \\n\\uf0b7 \\nFor information about launching, connecting, and using your instance, see Amazon EC2 instances. \\n\\n \\nUnit-6 – AWS computing and marketplace \\n \\n \\n5 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nMulti Tenancy \\n\\uf0b7 \\nIn cloud computing, multi tenancy means that multiple customers of a cloud vendor are using the same \\ncomputing resources.  \\n\\uf0b7 \\nDespite the fact that they share resources, cloud customers aren\\'t aware of each other, and their data is \\nkept totally separate.  \\n\\uf0b7 \\nMulti tenancy is a crucial component of cloud computing; without it, cloud services would be far less \\npractical.  \\n\\uf0b7 \\nMultitenant architecture is a feature in many types of public cloud computing, including IaaS, PaaS, SaaS, \\ncontainers, and server less computing. \\n\\uf0b7 \\nTo understand multi tenancy, think of how banking works.  \\n\\uf0b7 \\nMultiple people can store their money in one bank, and their assets are completely separate even though \\nthey\\'re stored in the same place.  \\n\\uf0b7 \\nCustomers of the bank don\\'t interact with each other, don\\'t have access to other customers\\' money, and \\naren\\'t even aware of each other.  \\n\\uf0b7 \\nSimilarly, in public cloud computing, customers of the cloud vendor use the same infrastructure – the same \\nservers, typically – while still keeping their data and their business logic separate and secure. \\n\\uf0b7 \\nThe classic definition of multi tenancy was a single software instance that served multiple users, or \\ntenants.  \\n\\uf0b7 \\nHowever, in modern cloud computing, the term has taken on a broader meaning, referring to shared cloud \\ninfrastructure instead of just a shared software instance. \\nCataloging the Marketplace \\n\\uf0b7 \\nAWS Marketplace is a curated digital catalog customers can use to find, buy, deploy, and manage third-\\nparty software, data, and services that customers need to build solutions and run their businesses.  \\n\\uf0b7 \\nAWS Marketplace includes thousands of software listings from popular categories such as security, \\nnetworking, storage, machine learning, business intelligence, database, and DevOps.  \\n\\uf0b7 \\nAWS Marketplace also simplifies software licensing and procurement with flexible pricing options and \\nmultiple deployment methods.  \\n\\uf0b7 \\nIn addition, AWS Marketplace includes data products available from AWS Data Exchange. \\n\\uf0b7 \\nCustomers can quickly launch preconfigured software with just a few clicks, and choose software solutions \\nin Amazon Machine Images (AMIs), software as a service (SaaS), and other formats.  \\n\\uf0b7 \\nYou can browse and subscribe to data products.  \\n\\uf0b7 \\nFlexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL, and get billed \\nfrom one source.  \\n\\uf0b7 \\nAWS handles billing and payments, and charges appear on customers’ AWS bill. \\n\\uf0b7 \\nYou can use AWS Marketplace as a buyer (subscriber), seller (provider), or both.  \\n\\uf0b7 \\nAnyone with an AWS account can use AWS Marketplace as a buyer, and can register to become a seller.  \\n\\uf0b7 \\nA seller can be an independent software vendor (ISV), value-added reseller, or individual who has \\nsomething to offer that works with AWS products and services. \\n\\uf0b7 \\nEvery software product on AWS Marketplace has been through a curation process.  \\n\\uf0b7 \\nOn the product page, there can be one or more offerings for the product.  \\n\\uf0b7 \\nWhen the seller submits a product in AWS Marketplace, they define the price of the product and the terms \\nand conditions of use.  \\n\\n \\nUnit-6 – AWS computing and marketplace \\n \\n \\n6 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nWhen a consumer subscribes to a product offering, they agree to the pricing and terms and conditions set \\nfor the offer. \\n\\uf0b7 \\nThe product can be free to use or it can have an associated charge.  \\n\\uf0b7 \\nThe charge becomes part of your AWS bill, and after you pay, AWS Marketplace pays the seller.  \\n\\uf0b7 \\nProducts can take many forms. For example, a product can be offered as an Amazon Machine Image (AMI) \\nthat is instantiated using your AWS account.  \\n\\uf0b7 \\nThe product can also be configured to use AWS CloudFormation templates for delivery to the consumer.  \\n\\uf0b7 \\nThe product can also be software as a service (SaaS) offerings from an ISV, web ACL, set of rules, or \\nconditions for AWS WAF. \\n\\uf0b7 \\nSoftware products can be purchased at the listed price using the ISV’s standard end user license agreement \\n(EULA) or offered with customer pricing and EULA.  \\n\\uf0b7 \\nProducts can also be purchased under a contract with specified time or usage boundaries.  \\n\\uf0b7 \\nAfter the product subscriptions are in place, the consumer can copy the product to their AWS Service \\nCatalog to manage how the product is accessed and used in the consumer’s organization. \\nSelling On the Marketplace. \\n\\uf0b7 \\nAs a seller, go to the AWS Marketplace Management Portal to register.  \\n\\uf0b7 \\nIf you\\'re providing a data product or you\\'re charging for use of your software product, you must also \\nprovide tax and banking information as part of your registration.  \\n\\uf0b7 \\nWhen you register, you create a profile for your company or for yourself that is discoverable on AWS \\nMarketplace.  \\n\\uf0b7 \\nYou also use the AWS Marketplace Management Portal to create and manage product pages for your \\nproducts. \\n\\uf0b7 \\nEligible partners can programmatically list AWS Marketplace products outside of AWS Marketplace.  \\n\\uf0b7 \\nFor information about becoming an eligible partner, contact your AWS Marketplace business development \\npartner. \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nVirtual private clouds \\n\\uf0b7 \\nA virtual private cloud (VPC) is an on-demand configurable pool of shared computing resources allocated \\nwithin a public cloud environment, providing a certain level of isolation between the different \\norganizations. \\n\\uf0b7 \\nYou already know that there are three major types of clouds: Public, Private and Hybrid. Now, there’s a \\nnewer player in the game: Virtual Private Clouds.  \\n\\uf0b7 \\nWhat makes these different from public and private clouds, and what is the benefit? Is it just a fancy name \\nfor public cloud, or is it a private one? \\n\\uf0b7 \\nVPC are related to the public cloud, but they are not the same. Instead of sharing resources and space in \\na public infrastructure, you get a changeable allotment of resources to configure.  \\n\\uf0b7 \\nThere is a certain level of isolation between you and other users, via a private IP subnet and virtual \\ncommunication construct (such as a VLAN) on a per user basis.  \\n\\uf0b7 \\nThis ensures a secure method of remotely accessing your cloud resources. This isolation within a public \\ncloud lends the name “virtual private” because you are essentially operating a private cloud within a public \\ncloud. \\n\\uf0b7 \\nThat also doesn’t mean Virtual Private Clouds and private clouds are the same.  \\n\\uf0b7 \\nPrivate clouds are entirely dedicated to your organization, and that includes the hardware.  \\n\\uf0b7 \\nVirtual Private clouds do not have the same hardware dedication; it just creates a more secure \\nenvironment on public infrastructure.  \\n\\uf0b7 \\nThink of it as operating like a VPN: You use them to send messages over the public internet in a secure \\nway as if you had your own personal network, but it’s not the same as actually having your own. \\n\\uf0b7 \\nWhat’s the benefit to this? Wouldn’t it just be easier to have a private cloud? Not necessarily. Private \\nclouds are expensive to operate, and because the hardware as well as the resources required to run it \\nbelong to you alone, there is no one to share that cost with.  \\n\\uf0b7 \\nVirtual Private Clouds give you the best of both worlds: A private cloud for security and compliance \\npurposes, reduced infrastructure costs that come with public clouds. The allotment of resources is yours \\nto use, so there is no worry about running out or having to share with others. You simply are sharing the \\ninfrastructure. \\n\\uf0b7 \\nVirtual Private Clouds are commonly used with Infrastructure as a Service (IaaS) providers.  \\n\\uf0b7 \\nBecause the shared resources (CPU, RAM, etc.) are not always the responsibility of the hardware provider, \\nit is possible to have different infrastructure and VPC providers.  \\n\\uf0b7 \\nHowever, having the same VPC and infrastructure provider can help cut down on the confusion and \\ncommunication process between you and your vendor. \\nAmazon Route 53 Announces Private DNS within Amazon VPC \\n\\uf0b7 \\nYou can now use Amazon Route 53, AWS\\'s highly available and scalable DNS service, to easily manage \\nyour internal domain names with the same simplicity, security, and cost effectiveness that Route 53 \\nalready provides for external DNS names.  \\n\\uf0b7 \\nYou can use the Route 53 Private DNS feature to manage authoritative DNS within your Virtual Private \\nClouds (VPCs), so you can use custom domain names for your internal AWS resources without exposing \\nDNS data to the public Internet. \\n\\uf0b7 \\nYou can use Route 53 Private DNS to manage internal DNS hostnames for resources like application \\nservers, database servers, and web servers.  \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n2 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nRoute 53 will only respond to queries for these names when the queries originate from within the VPC(s) \\nthat you authorize.  \\n\\uf0b7 \\nUsing custom internal DNS names (rather than IP addresses or AWS-provided names such as ec2-10-1-2-\\n3.us-west-2.compute.amazonaws.com) has a variety of benefits, for example, being able to flip from one \\ndatabase to another just by changing the mapping of a domain name such as internal.example.com to \\npoint to a new IP address.  \\n\\uf0b7 \\nRoute 53 also supports split-view DNS, so you can configure public and private hosted zones to return \\ndifferent external and internal IP addresses for the same domain names. \\nRelational Database Service \\n\\uf0b7 \\nAmazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational \\ndatabase in the cloud.  \\n\\uf0b7 \\nIt provides cost-efficient and resizable capacity while automating time-consuming administration tasks \\nsuch as hardware provisioning, database setup, patching and backups.  \\n\\uf0b7 \\nIt frees you to focus on your applications so you can give them the fast performance, high availability, \\nsecurity and compatibility they need. \\n\\uf0b7 \\nAmazon RDS is available on several database instance types optimized for memory, performance or I/O \\nand provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, \\nMySQL, MariaDB, Oracle, and Microsoft SQL Server.  \\n\\uf0b7 \\nYou can use the AWS Database Migration Service to easily migrate or replicate your existing databases to \\nAmazon RDS. \\nAdvantages/Benefits of Relational Database Service \\n(i) \\nEasy to Administer \\n\\uf0b7 \\nAmazon RDS makes it easy to go from project conception to deployment. Use the AWS Management \\nConsole, the AWS RDS Command-Line Interface, or simple API calls to access the capabilities of a \\nproduction-ready relational database in minutes. No need for infrastructure provisioning, and no need for \\ninstalling and maintaining database software. \\n(ii) Highly Scalable \\n\\uf0b7 \\nWe can scale our database\\'s compute and storage resources with only a few mouse clicks or an API call, \\noften with no downtime. Many Amazon RDS engine types allow you to launch one or more Read Replicas \\nto offload read traffic from your primary database instance. \\n(iii) Available and Durable \\n\\uf0b7 \\nAmazon RDS runs on the same highly reliable infrastructure used by other Amazon Web Services. When \\nyou provision a Multi-AZ DB Instance, Amazon RDS synchronously replicates the data to a standby instance \\nin a different Availability Zone (AZ). Amazon RDS has many other features that enhance reliability for \\ncritical production databases, including automated backups, database snapshots, and automatic host \\nreplacement. \\n(iv) Fast \\n\\uf0b7 \\nAmazon RDS supports the most demanding database applications. You can choose between two SSD-\\nbacked storage options: one optimized for high-performance OLTP applications, and the other for cost-\\neffective general-purpose use. In addition, Amazon Aurora provides performance on par with commercial \\ndatabases at 1/10th the cost. \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n3 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n(v) \\nSecure \\n\\uf0b7 \\nAmazon RDS makes it easy to control network access to your database. Amazon RDS also lets you run your \\ndatabase instances in Amazon Virtual Private Cloud (Amazon VPC), which enables you to isolate your \\ndatabase instances and to connect to your existing IT infrastructure through an industry-standard \\nencrypted IPsec VPN. Many Amazon RDS engine types offer encryption at rest and encryption in transit. \\n(vi) Inexpensive \\n\\uf0b7 \\nYou pay very low rates and only for the resources you actually consume. In addition, you benefit from the \\noption of On-Demand pricing with no up-front or long-term commitments, or even lower hourly rates via \\nReserved Instance pricing. \\nDynamoDB \\n\\uf0b7 \\nAmazon DynamoDB -- also known as Dynamo Database or DDB -- is a fully managed NoSQL database \\nservice provided by Amazon Web Services. DynamoDB is known for low latencies and scalability. \\n\\uf0b7 \\nAccording to AWS, DynamoDB makes it simple and cost-effective to store and retrieve any amount of data, \\nas well as serve any level of request traffic.  \\n\\uf0b7 \\nAll data items are stored on solid-state drives, which provide high I/O performance and can more \\nefficiently handle high-scale requests.  \\n\\uf0b7 \\nAn AWS user interacts with the service by using the AWS Management Console or a DynamoDB API. \\n\\uf0b7 \\nDynamoDB uses a NoSQL database model, which is nonrelational, allowing documents, graphs and \\ncolumnar among its data models.  \\n\\uf0b7 \\nA user stores data in DynamoDB tables, then interacts with it via GET and PUT queries, which are read and \\nwrite operations, respectively.  \\n\\uf0b7 \\nDynamoDB supports basic CRUD operations and conditional operations. Each DynamoDB query is \\nexecuted by a primary key identified by the user, which uniquely identifies each item. \\nScalability, Availability and Durability \\n\\uf0b7 \\nDynamoDB enforces replication across three availability zones for high availability, durability and read \\nconsistency.  \\n\\uf0b7 \\nA user can also opt for cross-region replication, which creates a backup copy of a DynamoDB table in one \\nor more global geographic locations. \\n\\uf0b7 \\nThe DynamoDB scan API provides two consistency options when reading DynamoDB data:  \\no Eventually consistent reads \\no Strongly consistent reads \\n\\uf0b7 \\nThe former, which is the AWS default setting, maximizes throughput at the potential expense of not having \\na read reflect the latest write or update. The latter reflects all writes and updates. \\n\\uf0b7 \\nThere are no DynamoDB limits on data storage per user, nor a maximum throughput per table. \\nSecurity \\n\\uf0b7 \\nAmazon DynamoDB offers Fine-Grained Access Control (FGAC) for an administrator to protect data in a \\ntable.  \\n\\uf0b7 \\nThe admin or table owner can specify who can access which items or attributes in a table and what actions \\nthat person can perform.  \\n\\uf0b7 \\nFGAC is based on the AWS Identity and Access Management service, which manages credentials and \\npermissions.  \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n4 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nAs with other AWS products, the cloud provider recommends a policy of least privilege when granting \\naccess to items and attributes. \\n\\uf0b7 \\nAn admin can view usage metrics for DynamoDB with Amazon CloudWatch. \\nAdditional DynamoDB Features \\n\\uf0b7 \\nThe DynamoDB Triggers feature integrates with AWS Lambda to allow a developer to code actions based \\non updates to items in a DynamoDB table, such as sending a notification or connecting a table to another \\ndata source.  \\n\\uf0b7 \\nThe developer associates a Lambda function, which stores the logic code, with the stream on a DynamoDB \\ntable.  \\n\\uf0b7 \\nAWS Lambda then reads updates to a table from a stream and executes the function. \\n\\uf0b7 \\nThe DynamoDB Streams feature provides a 24-hour chronological sequence of updates to items in a table.  \\n\\uf0b7 \\nAn admin can access the stream via an API call to take action based on updates, such as synchronizing \\ninformation with another data store. An admin enables DynamoDB Streams on a per-table basis. \\nAdvantages of DynamoDB \\nPerformance at scale \\n\\uf0b7 \\nDynamoDB supports some of the world’s largest scale applications by providing consistent, single-digit \\nmillisecond response times at any scale.  \\n\\uf0b7 \\nYou can build applications with virtually unlimited throughput and storage.  \\n\\uf0b7 \\nDynamoDB global tables replicate your data across multiple AWS Regions to give you fast, local access to \\ndata for your globally distributed applications.  \\n\\uf0b7 \\nFor use cases that require even faster access with microsecond latency, DynamoDB Accelerator (DAX) \\nprovides a fully managed in-memory cache. \\nNo servers to manage \\n\\uf0b7 \\nDynamoDB is server less with no servers to provision, patch, or manage and no software to install, \\nmaintain, or operate.  \\n\\uf0b7 \\nDynamoDB automatically scales tables up and down to adjust for capacity and maintain performance.  \\n\\uf0b7 \\nAvailability and fault tolerance are built in, eliminating the need to architect your applications for these \\ncapabilities.  \\n\\uf0b7 \\nDynamoDB provides both provisioned and on-demand capacity modes so that you can optimize costs by \\nspecifying capacity per workload, or paying for only the resources you consume. \\nEnterprise ready \\n\\uf0b7 \\nDynamoDB supports ACID transactions to enable you to build business-critical applications at scale.  \\n\\uf0b7 \\nDynamoDB encrypts all data by default and provides fine-grained identity and access control on all your \\ntables.  \\n\\uf0b7 \\nYou can create full backups of hundreds of terabytes of data instantly with no performance impact to your \\ntables, and recover to any point in time in the preceding 35 days with no downtime.  \\n\\uf0b7 \\nDynamoDB is also backed by a service level agreement for guaranteed availability. \\nElastiCache \\n\\uf0b7 \\nElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory data \\nstore or cache environment in the cloud.  \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n5 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nIt provides a high-performance, scalable, and cost-effective caching solution, while removing the \\ncomplexity associated with deploying and managing a distributed cache environment. \\n\\uf0b7 \\nWith ElastiCache, you can quickly deploy your cache environment, without having to provision hardware \\nor install software.  \\n\\uf0b7 \\nYou can choose from Memcached or Redis protocol-compliant cache engine software, and let ElastiCache \\nperform software upgrades and patch management for you.  \\n\\uf0b7 \\nFor enhanced security, ElastiCache can be run in the Amazon Virtual Private Cloud (Amazon VPC) \\nenvironment, giving you complete control over network access to your clusters.  \\n\\uf0b7 \\nWith just a few clicks in the AWS Management Console, you can add or remove resources such as nodes, \\nclusters, or read replicas to your ElastiCache environment to meet your business needs and application \\nrequirements. \\n\\uf0b7 \\nExisting applications that use Memcached or Redis can use ElastiCache with almost no modification.  \\n\\uf0b7 \\nYour applications simply need to know the host names and port numbers of the ElastiCache nodes that \\nyou have deployed.  \\n\\uf0b7 \\nThe ElastiCache Auto Discovery feature for Memcached lets your applications identify all of the nodes in \\na cache cluster and connect to them, rather than having to maintain a list of available host names and port \\nnumbers.  \\n\\uf0b7 \\nIn this way, your applications are effectively insulated from changes to node membership in a cluster. \\n\\uf0b7 \\nElastiCache has multiple features to enhance reliability for critical production deployments: \\no Automatic detection and recovery from cache node failures. \\no Multi-AZ with Automatic Failover of a failed primary cluster to a read replica in Redis clusters that \\nsupport replication (called replication groups in the ElastiCache API and AWS CLI. \\no Flexible Availability Zone placement of nodes and clusters. \\no Integration with other AWS services such as Amazon EC2, Amazon CloudWatch, AWS CloudTrail, and \\nAmazon SNS to provide a secure, high-performance, managed in-memory caching solution. \\nElastiCache Nodes \\n\\uf0b7 \\nA node is the smallest building block of an ElastiCache deployment.  \\n\\uf0b7 \\nA node can exist in isolation from or in some relationship to other nodes. \\n\\uf0b7 \\nA node is a fixed-size chunk of secure, network-attached RAM. \\n\\uf0b7 \\n Each node runs an instance of the engine and version that was chosen when you created your cluster.  \\n\\uf0b7 \\nIf necessary, you can scale the nodes in a cluster up or down to a different instance type.  \\n\\uf0b7 \\nEvery node within a cluster is the same instance type and runs the same cache engine.  \\n\\uf0b7 \\nEach cache node has its own Domain Name Service (DNS) name and port.  \\n\\uf0b7 \\nMultiple types of cache nodes are supported, each with varying amounts of associated memory.  \\n\\uf0b7 \\nYou can purchase nodes on a pay-as-you-go basis, where you only pay for your use of a node.  \\n\\uf0b7 \\nOr you can purchase reserved nodes at a much-reduced hourly rate.  \\n\\uf0b7 \\nIf your usage rate is high, purchasing reserved nodes can save you money. \\nElastiCache for Redis Shards \\n\\uf0b7 \\nA Redis shard (called a node group in the API and CLI) is a grouping of one to six related nodes.  \\n\\uf0b7 \\nA Redis (cluster mode disabled) cluster always has one shard.  \\n\\uf0b7 \\nA Redis (cluster mode enabled) cluster can have 1–90 shards. \\n\\uf0b7 \\nA multiple node shard implements replication by having one read/write primary node and 1–5 replica \\nnodes. \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n6 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nElastiCache for Redis Clusters \\n\\uf0b7 \\nA Redis cluster is a logical grouping of one or more ElastiCache for Redis Shards.  \\n\\uf0b7 \\nData is partitioned across the shards in a Redis (cluster mode enabled) cluster. \\n\\uf0b7 \\nMany ElastiCache operations are targeted at clusters: \\no Creating a cluster \\no Modifying a cluster \\no Taking snapshots of a cluster (all versions of Redis) \\no Deleting a cluster \\no Viewing the elements in a cluster \\no Adding or removing cost allocation tags to and from a cluster \\nRedshift \\n\\uf0b7 \\nPerhaps one of the most exciting outcomes of the public cloud was addressing the shortcomings of \\ntraditional enterprise data warehouse (EDW) storage and processing. The fast provisioning, commodity \\ncosts, infinite scale, and pay-as-you-grow pricing of public cloud are a natural fit for EDW needs, providing \\neven the smallest of users the ability to now get valuable answers to business intelligence (BI) questions.  \\n\\uf0b7 \\nAmazon Redshift is one such system built to address EDW needs, and it boasts low costs, an easy SQL-\\nbased access model, easy integration to other Amazon Web Services (AWS) solutions, and most \\nimportantly, high query performance. \\n\\uf0b7 \\nAmazon Redshift gets its name from the astronomical phenomenon noticed by Hubble, which explained \\nthe expansion of the universe. By adopting the Amazon Redshift moniker, AWS wanted to relay to \\ncustomers that the service was built to handle the perpetual expansion of their data. \\n \\nFig. : Amazon Redshift Architecture \\n\\uf0b7 \\nAn Amazon Redshift cluster consists of one leader node (which clients submit queries to) and one or more \\nfollower (or “compute”) nodes, which actually perform the queries on locally stored data.  \\n\\uf0b7 \\nBy allowing for unlimited expansion of follower nodes, Amazon Redshift ensures that customers can \\ncontinue to grow their cluster as their data needs grow.  \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n7 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nCustomers can start with a “cluster” as small as a single node (acting as both leader and follower), and for \\nthe smallest supported instance type (a DW2), that could be as low cost as $0.25/hour or about \\n$180/month. By using “Reservations” (paying an up-front fee in exchange for a lower hourly running cost) \\nfor the underlying instances, Amazon Redshift can cost as little as $1,000/TB/year — upwards of one-fifth \\nto one-tenth of the cost of a traditional EDW. \\n\\uf0b7 \\nBecause Amazon Redshift provides native Open Database Connectivity (ODBC) and Database Connectivity \\n(JDBC) connectivity (in addition to PostgresSQL driver support), most third-party BI tools (like Tableu, \\nQlikview, and MicroStrategy) work right out of the box. Amazon Redshift also uses the ubiquitous \\nStructured Query Language (SQL) language for queries, ensuring that your current resources can quickly \\nand easily become productive with the technology. \\n\\uf0b7 \\nAmazon Redshift was custom designed from the ParAccel engine — an analytic database which used \\ncolumnar storage and parallel processing to achieve very fast I/O.  \\n\\uf0b7 \\nColumns of data in Amazon Redshift are stored physically adjacent on disk, meaning that queries and scans \\non those columns (common in online analytical processing [OLAP] queries) run very fast.  \\n\\uf0b7 \\nAdditionally, Amazon Redshift uses 10GB Ethernet interconnects, and specialized EC2 instances (with \\nbetween three and 24 spindles per node) to achieve high throughput and low latency.  \\n\\uf0b7 \\nFor even faster queries, Amazon Redshift allows customers to use column-level compression to both \\ngreatly reduce the amount of data that needs stored, and reduce the amount of disk I/O. \\n\\uf0b7 \\nAmazon Redshift, like many of AWS’s most popular services, is also fully managed, meaning that low-level, \\ntime-consuming administrative tasks like OS patching, backups, replacing failed hardware, and software \\nupgrades are handled automatically and transparently.  \\n\\uf0b7 \\nWith Amazon Redshift, users simply provision a cluster, load it with their data, and begin executing \\nqueries. All data is continuously, incrementally, automatically backed up in the highly durable S3, and \\nenabling disaster recovery across regions can be accomplished with just a few clicks.  \\n\\uf0b7 \\nSpinning a cluster up can be as simple as a few mouse clicks, and as fast as a few minutes. \\n\\uf0b7 \\nA very exciting aspect of Amazon Redshift, and something that is not possible in traditional EDWs, is the \\nability to easily scale a provisioned cluster up and down.  \\n\\uf0b7 \\nIn Amazon Redshift, this scaling is transparent to the customer—when a resize is requested, data is copied \\nin parallel from the source cluster (which continues to function in read-only mode) to a new cluster, and \\nonce all data is live migrated, DNS is flipped to the new cluster and the old cluster is de-provisioned.  \\n\\uf0b7 \\nThis allows customers to easily scale up and down, and each scaling event nicely re-stripes the data across \\nthe new cluster for a balanced workload.  \\n\\uf0b7 \\nAmazon Redshift offers mature, native, and tunable security. Clusters can be deployed into a Virtual \\nPrivate Cloud (VPC), and encryption of data is supported via hardware accelerated AES-256 (for data at \\nrest) and SSL (for data on the wire).  \\n\\uf0b7 \\nCompliance teams will be pleased to learn that users can manage their own encryption keys via AWS’s \\nHardware Security Module (HSM) service, and that Amazon Redshift provides a full audit trail of all SQL \\nconnection attempts, queries, and modifications of the cluster. \\nAdvantages of Amazon Redshift \\nExceptionally fast \\n\\uf0b7 \\nRedshift is very fast when it comes to loading data and querying it for analytical and reporting purposes.  \\n\\uf0b7 \\nRedshift has Massively Parallel Processing (MPP) Architecture which allows you to load data at blazing fast \\nspeed.  \\n\\uf0b7 \\nIn addition, using this architecture, Redshift distributes and parallelize your queries across multiple nodes. \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n8 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nRedshift gives you an option to use Dense Compute nodes which are SSD based data warehouses. Using \\nthis you can run most complex queries in very less time. \\nHigh Performance \\n\\uf0b7 \\nAs discussed in the previous point, Redshift gains high performance using massive parallelism, efficient \\ndata compression, query optimization, and distribution. \\n\\uf0b7 \\nMPP enables Redshift to parallelize data loading, backup and restore operation. Furthermore, queries that \\nyou execute get distributed across multiple nodes. \\n\\uf0b7 \\nRedshift is a columnar storage database, which is optimized for huge and repetitive type of data. Using \\ncolumnar storage, reduces the I/O operations on disk drastically, improving performance as a result. \\n\\uf0b7 \\nRedshift gives you an option to define column-based encoding for data compression. If not specified by \\nthe user, redshift automatically assigns compression encoding.  \\n\\uf0b7 \\nData compression helps in reducing memory footprint and significantly improves the I/O speed. \\nHorizontally Scalable \\n\\uf0b7 \\nScalability is a very crucial point for any Data warehousing solution and Redshift does pretty well job in \\nthat.  \\n\\uf0b7 \\nRedshift is horizontally scalable. Whenever you need to increase the storage or need it to run faster, just \\nadd more nodes using AWS console or Cluster API and it will upscale immediately. \\n\\uf0b7 \\nDuring this process, your existing cluster will remain available for read operations so your application stays \\nuninterrupted. \\n\\uf0b7 \\nDuring the scaling operation, Redshift moves data parallel between compute nodes of old and new \\nclusters. Therefore enabling the transition to complete smoothly and as quickly as possible. \\nMassive Storage capacity \\n\\uf0b7 \\nAs expected from a Data warehousing solution, Redshift provides massive storage capacity.  \\n\\uf0b7 \\nA basic setup can give you a petabyte range of data storage.  \\n\\uf0b7 \\nIn addition, Redshift gives you an option to choose Dense Storage type of compute nodes which can \\nprovide large storage space using Hard Disk Drives for a very low price.  \\n\\uf0b7 \\nYou can further increase the storage by adding more nodes to your cluster and it can go well beyond \\npetabyte of data range. \\nAttractive and transparent pricing \\n\\uf0b7 \\nPricing is a very strong point in favor of Redshift, it is considerably cheaper than alternatives or an on \\npremise solution. Redshift has 2 pricing models, pay as you go and reserved instance.  \\n\\uf0b7 \\nHence this gives you the flexibility to categorize this expense as an operational expense or capital expense. \\n\\uf0b7 \\nIf your use case requires more data storage, then with 3 years reserved instance Dense Storage plan, \\neffective price per terabyte per year can be as low as $935.  \\n\\uf0b7 \\nComparing this to traditional on premise storage, which roughly costs around $19k-$25k per terabyte, \\nRedshift is significantly cheaper. \\n SQL interface \\n\\uf0b7 \\nRedshift Query Engine is based on ParAccel which has the same interface as PostgreSQL If you are already \\nfamiliar with SQL, you don’t need to learn a lot of new techs to start using query module of Redshift.  \\n\\uf0b7 \\nSince Redshift uses SQL, it works with existing Postgres JDBC/ODBC drivers, readily connecting to most of \\nthe Business Intelligence tools. \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n9 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n AWS ecosystem \\n\\uf0b7 \\nMany businesses are running their infrastructure on AWS already, EC2 for servers, S3 for long-term \\nstorage, RDS for database and this number is constantly increasing.  \\n\\uf0b7 \\nRedshift works very well if the rest of your infra is already on AWS and you get the benefit of data locality \\nand cost of data transport is comparatively low.  \\n\\uf0b7 \\nFor a lot of businesses, S3 has become the de-facto destination for cloud storage.  \\n\\uf0b7 \\nSince Redshift is virtually co-located with S3 and it can access formatted data on S3 with single COPY \\ncommand.  \\n\\uf0b7 \\nWhen loading or dumping data on S3, Redshift uses Massive Parallel Processing which can move data at a \\nvery fast speed. \\nSecurity \\n\\uf0b7 \\nAmazon Redshift comes packed with various security features.  \\n\\uf0b7 \\nThere are options like VPC for network isolation, various ways to handle access control, data encryption \\netc.  \\n\\uf0b7 \\nData encryption option is available at multiple places in Redshift.  \\n\\uf0b7 \\nTo encrypt data stored in your cluster you can enable cluster encryption at the time of launching the \\ncluster.  \\n\\uf0b7 \\nAlso, to encrypt data in transit, you can enable SSL encryption.  \\n\\uf0b7 \\nWhen loading data from S3, redshift allows you to use either server-side encryption or client-side \\nencryption.  \\n\\uf0b7 \\nFinally, at the time of loading data, S3 or Redshift copy command handles the decryption respectively. \\n\\uf0b7 \\nAmazon Redshift clusters can be launched inside your infrastructure Virtual Private Cloud (VPC).  \\n\\uf0b7 \\nHence you can define VPC security groups to restrict inbound or outbound access to your redshift clusters. \\n\\uf0b7 \\nUsing the robust Access Control system of AWS, you can grant privilege to specific users or maintain access \\non specific database level.  \\n\\uf0b7 \\nAdditionally, you can even define users and groups to have access to specific data in tables. \\nAmazon Redshift Limitations \\nDoesn’t enforce uniqueness  \\n\\uf0b7 \\nThere is no way in redshift to enforce uniqueness on inserted data.  \\n\\uf0b7 \\nHence, if you have a distributed system and it writes data on Redshift, you will have to handle the \\nuniqueness yourself either on the application layer or by using some method of data de-duplication. \\nOnly S3, DynamoDB and Amazon EMR support for parallel upload \\n\\uf0b7 \\nIf your data is in Amazon S3 or relational DynamoDB or on Amazon EMR, Redshift can load it using \\nMassively Parallel Processing which is very fast.  \\n\\uf0b7 \\nBut for all other sources, parallel loading is not supported.  \\n\\uf0b7 \\nYou will either have to use JDBC inserts or some scripts to load data into Redshift.  \\n\\uf0b7 \\nAlternatively, you can use an ETL solution like Hevo which can load your data into Redshift parallel from \\n100s of sources. \\nRequires a good understanding of Sort and Distribution keys \\n\\uf0b7 \\nSort keys and Distribution keys decide how data is stored and indexed across all Redshift nodes.  \\n\\uf0b7 \\nTherefore, you need to have a solid understanding of these concepts and you need to properly set them \\non your tables for optimal performance.  \\n\\n \\nUnit-7 – AWS networking and databases \\n \\n \\n10 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nThere can be only one distribution key for a table and that cannot be changed later on, which means you \\nhave to think carefully and anticipate future workloads before deciding Distribution key.  \\nCan’t be used as live app database \\n\\uf0b7 \\nWhile Redshift is very fast when running queries on a huge amount of data or running reporting and \\nanalytics, but it is not fast enough for live web apps.  \\n\\uf0b7 \\nSo you will have to pull data into a caching layer or a vanilla Postgres instance to serve redshift data to \\nweb apps. \\nData on Cloud \\n\\uf0b7 \\nThough it is a good thing for most of the people, in some use cases it could be a point of concern.  \\n\\uf0b7 \\nSo if you are concerned with the privacy of data or your data has extremely sensitive content, you may \\nnot be comfortable putting it on the cloud. \\nHigh performance AWS Networking. \\n\\uf0b7 \\nHigh performance AWS Networking is nothing but use of various network services provided by AWS for \\nbetter performance. \\n\\uf0b7 \\nAWS Networking include following services: \\n1. Private DNS Servers \\no The Private DNS are name servers that reflect your domain name rather than our default ones. \\no Having private nameservers could be useful if you intend to resell hosting services or want to \\nbrand your business.  \\no Also, when using Private DNS, if a domain name is migrated to another server, there is no need to \\nchange any nameservers and the domain names will automatically point to the new location. \\n2. Virtual Private Clouds (Explain Earlier) \\n3. Cloud Models (Explain Earlier) etc. \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nBig Data Analytics \\n\\uf0b7 \\nBig data analytics is the often complex process of examining large and varied data sets, or big data, to \\nuncover information -- such as hidden patterns, unknown correlations, market trends and customer \\npreferences -- that can help organizations make informed business decisions. \\nAWS Analytics Services \\nAmazon Athena \\n\\uf0b7 \\nAmazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using \\nstandard SQL.  \\n\\uf0b7 \\nAthena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you \\nrun. \\n\\uf0b7 \\nAthena is easy to use. Simply point to your data in Amazon S3, deﬁne the schema, and start querying using \\nstandard SQL.  \\n\\uf0b7 \\nMost results are delivered within seconds. With Athena, there’s no need for complex extract, transform, \\nand load (ETL) jobs to prepare your data for analysis.  \\n\\uf0b7 \\nThis makes it easy for anyone with SQL skills to quickly analyze large-scale datasets. \\n\\uf0b7 \\nAthena is out-of-the-box integrated with AWS Glue Data Catalog, allowing you to create a unified \\nmetadata repository across various services, crawl data sources to discover schemas and populate your \\nCatalog with new and modified table and partition definitions, and maintain schema versioning.  \\n\\uf0b7 \\nYou can also use Glue’s fully-managed ETL capabilities to transform data or convert it into columnar \\nformats to optimize cost and improve performance. \\nAmazon EMR \\n\\uf0b7 \\nAmazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-eﬀective to \\nprocess vast amounts of data across dynamically scalable Amazon EC2 instances.  \\n\\uf0b7 \\nYou can also run other popular distributed frameworks such as Apache Spark, HBase, Presto, and Flink in \\nAmazon EMR, and interact with data in other AWS data stores such as Amazon S3 and Amazon DynamoDB.  \\n\\uf0b7 \\nEMR Notebooks, based on the popular Jupyter Notebook, provide a development and collaboration \\nenvironment for ad hoc querying and exploratory analysis. \\n\\uf0b7 \\nAmazon EMR securely and reliably handles a broad set of big data use cases, including log analysis, web \\nindexing, data transformations (ETL), machine learning, ﬁnancial analysis, scientiﬁc simulation, and \\nbioinformatics. \\nAmazon CloudSearch \\n\\uf0b7 \\nAmazon CloudSearch is a managed service in the AWS Cloud that makes it simple and cost-effective to set \\nup, manage, and scale a search solution for your website or application.  \\n\\uf0b7 \\nAmazon CloudSearch supports 34 languages and popular search features such as highlighting, \\nautocomplete, and geospatial search. \\nAmazon Elasticsearch Service \\n\\uf0b7 \\nAmazon Elasticsearch Service makes it easy to deploy, secure, operate, and scale Elasticsearch to search, \\nanalyze, and visualize data in real-time.  \\n\\uf0b7 \\nWith Amazon Elasticsearch Service, you get easy-to-use APIs and real-time analytics capabilities to power \\nuse-cases such as log analytics, full-text search, application monitoring, and clickstream analytics, with \\nenterprise-grade availability, scalability, and security.  \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n2 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nThe service oﬀers integrations with open-source tools like Kibana and Logstash for data ingestion and \\nvisualization.  \\n\\uf0b7 \\nIt also integrates seamlessly with other AWS services such as Amazon Virtual Private Cloud (Amazon VPC), \\nAWS Key Management Service (AWS KMS), Amazon Kinesis Data Firehose, AWS Lambda, AWS Identity \\nand Access Management (IAM), Amazon Cognito, and Amazon CloudWatch, so that you can go from raw \\ndata to actionable insights quickly. \\nAmazon Kinesis \\n\\uf0b7 \\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get \\ntimely insights and react quickly to new information.  \\n\\uf0b7 \\nAmazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with \\nthe flexibility to choose the tools that best suit the requirements of your application.  \\n\\uf0b7 \\nWith Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website \\nclickstreams, and IoT telemetry data for machine learning, analytics, and other applications.  \\n\\uf0b7 \\nAmazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of \\nhaving to wait until all your data is collected before the processing can begin. \\n\\uf0b7 \\nAmazon Kinesis currently oﬀers four services: Kinesis Data Firehose, Kinesis Data Analytics, Kinesis Data \\nStreams, and Kinesis Video Streams. \\nAmazon Kinesis Data Firehose \\no Amazon Kinesis Firehose is the easiest way to reliably load streaming data into data stores and \\nanalytics tools.  \\no It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon \\nElasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence \\ntools and dashboards you’re already using today.  \\no It is a fully managed service that automatically scales to match the throughput of your data and \\nrequires no ongoing administration.  \\no It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount \\nof storage used at the destination and increasing security. \\no You can easily create a Firehose delivery stream from the AWS Management Console, conﬁgure it with \\na few clicks, and start sending data to the stream from hundreds of thousands of data sources to be \\nloaded continuously to AWS—all in just a few minutes.  \\no You can also configure your delivery stream to automatically convert the incoming data to columnar \\nformats like Apache Parquet and Apache ORC, before the data is delivered to Amazon S3, for cost-\\neffective storage and analytics. \\nAmazon Kinesis Data Analytics \\no Amazon Kinesis Data Analytics is the easiest way to analyze streaming data, gain actionable insights, \\nand respond to your business and customer needs in real time.  \\no Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating \\nstreaming applications with other AWS services.  \\no SQL users can easily query streaming data or build entire streaming applications using templates and \\nan interactive SQL editor.  \\no Java developers can quickly build sophisticated streaming applications using open source Java libraries \\nand AWS integrations to transform and analyze data in real-time. \\no Amazon Kinesis Data Analytics takes care of everything required to run your queries continuously and \\nscales automatically to match the volume and throughput rate of your incoming data. \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n3 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nAmazon Kinesis Data Streams \\no Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming \\nservice.  \\no KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources \\nsuch as website clickstreams, database event streams, financial transactions, social media feeds, IT \\nlogs, and location-tracking events.  \\no The data collected is available in milliseconds to enable real-time analytics use cases such as real-time \\ndashboards, real-time anomaly detection, dynamic pricing, and more. \\nAmazon Kinesis Video Streams \\no Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS \\nfor analytics, machine learning (ML), playback, and other processing.  \\no Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to \\ningest streaming video data from millions of devices.  \\no It also durably stores, encrypts, and indexes video data in your streams, and allows you to access your \\ndata through easy-to-use APIs.  \\no Kinesis Video Streams enables you to playback video for live and on-demand viewing, and quickly build \\napplications that take advantage of computer vision and video analytics through integration with \\nAmazon Recognition Video, and libraries for ML frameworks such as Apache MxNet, TensorFlow, and \\nOpenCV. \\nAmazon Redshift \\n\\uf0b7 \\nAmazon Redshift is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all \\nyour data across your data warehouse and data lake.  \\n\\uf0b7 \\nRedshift delivers ten times faster performance than other data warehouses by using machine learning, \\nmassively parallel query execution, and columnar storage on high-performance disk.  \\n\\uf0b7 \\nYou can setup and deploy a new data warehouse in minutes, and run queries across petabytes of data in \\nyour Redshift data warehouse, and exabytes of data in your data lake built on Amazon S3.  \\n\\uf0b7 \\nYou can start small for just $0.25 per hour and scale to $250 per terabyte per year, less than one-tenth \\nthe cost of other solutions. \\nAmazon QuickSight \\n\\uf0b7 \\nAmazon QuickSight is a fast, cloud-powered business intelligence (BI) service that makes it easy for you to \\ndeliver insights to everyone in your organization.  \\n\\uf0b7 \\nQuickSight lets you create and publish interactive dashboards that can be accessed from browsers or \\nmobile devices.  \\n\\uf0b7 \\nYou can embed dashboards into your applications, providing your customers with powerful self-service \\nanalytics.  \\n\\uf0b7 \\nQuickSight easily scales to tens of thousands of users without any software to install, servers to deploy, or \\ninfrastructure to manage. \\nAWS Data Pipeline \\n\\uf0b7 \\nAWS Data Pipeline is a web service that helps you reliably process and move data between diﬀerent AWS \\ncompute and storage services, as well as on-premises data sources, at speciﬁed intervals.  \\n\\uf0b7 \\nWith AWS Data Pipeline, you can regularly access your data where it’s stored, transform and process it at \\nscale, and eﬃciently transfer the results to AWS services such as Amazon S3, Amazon RDS, Amazon \\nDynamoDB, and Amazon EMR. \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n4 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nAWS Data Pipeline helps you easily create complex data processing workloads that are fault tolerant, \\nrepeatable, and highly available.  \\n\\uf0b7 \\nYou don’t have to worry about ensuring resource availability, managing inter-task dependencies, retrying \\ntransient failures or timeouts in individual tasks, or creating a failure notification system.  \\n\\uf0b7 \\nAWS Data Pipeline also allows you to move and process data that was previously locked up in on-premises \\ndata silos. \\nAWS Glue \\n\\uf0b7 \\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to \\nprepare and load their data for analytics.  \\n\\uf0b7 \\nYou can create and run an ETL job with a few clicks in the AWS Management Console.  \\n\\uf0b7 \\nYou simply point AWS Glue to your data stored on AWS, and AWS Glue discovers your data and stores the \\nassociated metadata (e.g. table definition and schema) in the AWS Glue Data Catalog.  \\n\\uf0b7 \\nOnce cataloged, your data is immediately searchable, queryable, and available for ETL. \\nAWS Lake Formation \\n\\uf0b7 \\nAWS Lake Formation is a service that makes it easy to set up a secure data lake in days.  \\n\\uf0b7 \\nA data lake is a centralized, curated, and secured repository that stores all your data, both in its original \\nform and prepared for analysis.  \\n\\uf0b7 \\nA data lake enables you to break down data silos and combine different types of analytics to gain insights \\nand guide better business decisions. \\n\\uf0b7 \\nHowever, setting up and managing data lakes today involves a lot of manual, complicated, and time-\\nconsuming tasks.  \\n\\uf0b7 \\nThis work includes loading data from diverse sources, monitoring those data flows, setting up partitions, \\nturning on encryption and managing keys, defining transformation jobs and monitoring their operation, \\nre-organizing data into a columnar format, configuring access control settings, deduplicating redundant \\ndata, matching linked records, granting access to data sets, and auditing access over time. \\n\\uf0b7 \\nCreating a data lake with Lake Formation is as simple as defining where your data resides and what data \\naccess and security policies you want to apply.  \\n\\uf0b7 \\nLake Formation then collects and catalogs data from databases and object storage, moves the data into \\nyour new Amazon S3 data lake, cleans and classifies data using machine learning algorithms, and secures \\naccess to your sensitive data.  \\n\\uf0b7 \\nYour users can then access a centralized catalog of data which describes available data sets and their \\nappropriate usage.  \\n\\uf0b7 \\nYour users then leverage these data sets with their choice of analytics and machine learning services, like \\nAmazon EMR for Apache Spark, Amazon Redshift, Amazon Athena, Amazon SageMaker, and Amazon \\nQuickSight. \\nAmazon Managed Streaming for Kafka (MSK) \\n\\uf0b7 \\nAmazon Managed Streaming for Kafka (MSK) is a fully managed service that makes it easy for you to build \\nand run applications that use Apache Kafka to process streaming data.  \\n\\uf0b7 \\nApache Kafka is an open-source platform for building real-time streaming data pipelines and applications.  \\n\\uf0b7 \\nWith Amazon MSK, you can use Apache Kafka APIs to populate data lakes, stream changes to and from \\ndatabases, and power machine learning and analytics applications. \\n\\uf0b7 \\nApache Kafka clusters are challenging to setup, scale, and manage in production.  \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n5 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nWhen you run Apache Kafka on your own, you need to provision servers, configure Apache Kafka \\nmanually, replace servers when they fail, orchestrate server patches and upgrades, architect the cluster \\nfor high availability, ensure data is durably stored and secured, setup monitoring and alarms, and carefully \\nplan scaling events to support load changes.  \\n\\uf0b7 \\nAmazon Managed Streaming for Kafka makes it easy for you to build and run production applications on \\nApache Kafka without needing Apache Kafka infrastructure management expertise.  \\n\\uf0b7 \\nThat means you spend less time managing infrastructure and more time building applications. \\n\\uf0b7 \\nWith a few clicks in the Amazon MSK console you can create highly available Apache Kafka clusters with \\nsettings and configuration based on Apache Kafka’s deployment best practices.  \\n\\uf0b7 \\nAmazon MSK automatically provisions and runs your Apache Kafka clusters.  \\n\\uf0b7 \\nAmazon MSK continuously monitors cluster health and automatically replaces unhealthy nodes with no \\ndowntime to your application.  \\n\\uf0b7 \\nIn addition, Amazon MSK secures your Apache Kafka cluster by encrypting data at rest. \\nApplication Services \\nTracking Software Licenses with AWS Service Catalog and AWS Step Functions \\n\\uf0b7 \\nEnterprises have many business requirements for tracking how software product licenses are used in their \\norganization for financial, governance, and compliance reasons.  \\n\\uf0b7 \\nBy tracking license usage, organizations can stay within budget, track expenditures, and avoid unplanned \\ntrue-up bills from their vendors’ true-up processes.  \\n\\uf0b7 \\nThe goal is to track the usage licenses as resources are deployed. \\n\\uf0b7 \\nIn this post, you learn how to use AWS Service Catalog to deploy services and applications while tracking \\nthe licenses being consumed by end users, and how to prevent license overruns on AWS. \\n\\uf0b7 \\nThis solution uses the following AWS services. Most of the resources are set up for you with an AWS \\nCloudFormation stack: \\no AWS Service Catalog \\no AWS Lambda \\no AWS Step Functions \\no AWS CloudFormation \\no Amazon DynamoDB \\no Amazon SES \\nSecure Serverless Development Using AWS Service Catalog \\n\\uf0b7 \\nServerless computing allows you to build and run applications and services without having to manage \\nservers.  \\n\\uf0b7 \\nAWS Service Catalog allows you to create and manage catalogs of services that are approved for use on \\nAWS.  \\n\\uf0b7 \\nCombining Serverless and Service Catalog together is a great way to safely allow developers to create \\nproducts and services in the cloud. \\n\\uf0b7 \\nIn this post, I demonstrate how to combine the controls of Service Catalog with AWS Lambda and Amazon \\nAPI Gateway and allow your developers to build a Serverless application without full AWS access. \\nHow to secure infrequently used EC2 instances with AWS Systems Manager \\n\\uf0b7 \\nMany organizations have predictable spikes in the usage of their applications and services.  \\n\\uf0b7 \\nFor example, retailers see large spikes in usage during Black Friday or Cyber Monday.  \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n6 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nThe beauty of Amazon Elastic Compute Cloud (Amazon EC2) is that it allows customers to quickly scale up \\ntheir compute power to meet these demands.  \\n\\uf0b7 \\nHowever, some customers might require more time-consuming setup for their software running on EC2 \\ninstances.  \\n\\uf0b7 \\nInstead of creating and terminating instances to meet demand, these customers turn off instances and \\nthen turn them on again when they are needed.  \\n\\uf0b7 \\nEventually the patches on those instances become out of date, and they require updates. \\nHow Cloudticity Automates Security Patches for Linux and Windows using Amazon \\nEC2 Systems Manager and AWS Step Functions \\n\\uf0b7 \\nAs a provider of HIPAA-compliant solutions using AWS, Cloudticity always has security as the base of \\neverything we do.  \\n\\uf0b7 \\nHIPAA breaches would be an end-of-life event for most of our customers.  \\n\\uf0b7 \\nHaving been born in the cloud with automation in our DNA, Cloudticity embeds automation into all levels \\nof infrastructure management including security, monitoring, and continuous compliance.  \\n\\uf0b7 \\nAs mandated by the HIPAA Security Rule (45 CFR Part 160 and Subparts A and C of Part 164), patches at \\nthe operating system and application level are required to prevent security vulnerabilities.  \\n\\uf0b7 \\nAs a result, patches are a major component of infrastructure management. \\n\\uf0b7 \\nCloudticity strives to provide consistent and reliable services to all of our customers.  \\n\\uf0b7 \\nAs such, we needed to create a custom patching solution that supports both Linux and Windows.  \\n\\uf0b7 \\nThe minimum requirements for such a solution were to read from a manifest file that contains instance \\nnames and a list of knowledge base articles (KBs) or security packages to apply to each instance.  \\n\\uf0b7 \\nBelow is a simplified, high-level process overview. \\n \\nFig. : High-Level Process Overview \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n7 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nThere were a few guidelines to be considered when designing the solution: \\no Each customer has a defined maintenance window that patches can be completed within. As such, the \\nsolution must be able to perform the updates within the specified maintenance window. \\no The solution must be able to provide patches to one or many instances and finish within the \\nmaintenance window. \\no The solution should use as many AWS services as possible to reduce time-to-market and take \\nadvantage of the built-in scaling that many AWS services provide. \\no Code reusability is essential. \\nCloud Security \\n\\uf0b7 \\nA number of security threats are associated with cloud data services: not only traditional security threats, \\nsuch as network eavesdropping, illegal invasion, and denial of service attacks, but also specific cloud \\ncomputing threats, such as side channel attacks, virtualization vulnerabilities, and abuse of cloud services.  \\n\\uf0b7 \\nThe following security requirements limit the threats if we achieve that requirement than we can say our \\ndata is safe on cloud. \\n\\uf0b7 \\nIdentity management  \\no Every enterprise will have its own identity management system to control access to information and \\ncomputing resources.  \\no Cloud providers either integrate the customer’s identity management system into their own \\ninfrastructure, using federation or SSO technology, or a biometric-based identification system, or \\nprovide an identity management system of their own.  \\no CloudID, for instance, provides privacy-preserving cloud-based and cross-enterprise biometric \\nidentification.  \\no It links the confidential information of the users to their biometrics and stores it in an encrypted \\nfashion.  \\no Making use of a searchable encryption technique, biometric identification is performed in encrypted \\ndomain to make sure that the cloud provider or potential attackers do not gain access to any sensitive \\ndata or even the contents of the individual queries. \\n\\uf0b7 \\nPhysical security  \\no Cloud service providers physically secure the IT hardware (servers, routers, cables etc.) against \\nunauthorized access, interference, theft, fires, floods etc. and ensure that essential supplies (such as \\nelectricity) are sufficiently robust to minimize the possibility of disruption.  \\no This is normally achieved by serving cloud applications from \\'world-class\\' (i.e. professionally specified, \\ndesigned, constructed, managed, monitored and maintained) data centers. \\n\\uf0b7 \\nPersonnel security  \\no Various information security concerns relating to the IT and other professionals associated with cloud \\nservices are typically handled through pre-, para- and post-employment activities such as security \\nscreening potential recruits, security awareness and training programs, proactive. \\n\\uf0b7 \\nPrivacy  \\no Providers ensure that all critical data (credit card numbers, for example) are masked or encrypted and \\nthat only authorized users have access to data in its entirety. Moreover, digital identities and \\ncredentials must be protected as should any data that the provider collects or produces about \\ncustomer activity in the cloud. \\n\\uf0b7 \\nConfidentiality \\no Data confidentiality is the property that data contents are not made available or disclosed to illegal \\nusers.  \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n8 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\no Outsourced data is stored in a cloud and out of the owners\\' direct control. Only authorized users can \\naccess the sensitive data while others, including CSPs, should not gain any information of the data.  \\no Meanwhile, data owners expect to fully utilize cloud data services, e.g., data search, data \\ncomputation, and data sharing, without the leakage of the data contents to CSPs or other adversaries. \\n\\uf0b7 \\nAccess controllability \\no Access controllability means that a data owner can perform the selective restriction of access to her \\nor his data outsourced to cloud.  \\no Legal users can be authorized by the owner to access the data, while others cannot access it without \\npermissions.  \\no Further, it is desirable to enforce fine-grained access control to the outsourced data, i.e., different \\nusers should be granted different access privileges with regard to different data pieces.  \\no The access authorization must be controlled only by the owner in untrusted cloud environments. \\n\\uf0b7 \\nIntegrity \\no Data integrity demands maintaining and assuring the accuracy and completeness of data.  \\no A data owner always expects that her or his data in a cloud can be stored correctly and trustworthily.  \\no It means that the data should not be illegally tampered, improperly modified, deliberately deleted, or \\nmaliciously fabricated.  \\no If any undesirable operations corrupt or delete the data, the owner should be able to detect the \\ncorruption or loss.  \\no Further, when a portion of the outsourced data is corrupted or lost, it can still be retrieved by the data \\nusers. \\nCloudWatch \\n\\uf0b7 \\nAmazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on \\nAWS.  \\n\\uf0b7 \\nYou can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, set alarms, and \\nautomatically react to changes in your AWS resources.  \\n\\uf0b7 \\nAmazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB \\ntables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and \\nservices, and any log files your applications generate.  \\n\\uf0b7 \\nYou can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application \\nperformance, and operational health.  \\n\\uf0b7 \\nYou can use these insights to react and keep your application running smoothly. \\nCloudFormation \\n\\uf0b7 \\nAWS CloudFormation provides a common language for you to describe and provision all the infrastructure \\nresources in your cloud environment.  \\n\\uf0b7 \\nCloudFormation allows you to use a simple text file to model and provision, in an automated and secure \\nmanner, all the resources needed for your applications across all regions and accounts.  \\n\\uf0b7 \\nThis file serves as the single source of truth for your cloud environment.  \\n\\uf0b7 \\nAWS CloudFormation is available at no additional charge, and you pay only for the AWS resources needed \\nto run your applications. \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n9 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nAdvantage of Cloud Formation \\nModel it all \\n\\uf0b7 \\nAWS CloudFormation allows you to model your entire infrastructure in a text file. This template becomes \\nthe single source of truth for your infrastructure. This helps you to standardize infrastructure components \\nused across your organization, enabling configuration compliance and faster troubleshooting. \\nAutomate and deploy \\n\\uf0b7 \\nAWS CloudFormation provisions your resources in a safe, repeatable manner, allowing you to build and \\nrebuild your infrastructure and applications, without having to perform manual actions or write custom \\nscripts. CloudFormation takes care of determining the right operations to perform when managing your \\nstack, and rolls back changes automatically if errors are detected. \\nIt\\'s just code \\n\\uf0b7 \\nCodifying your infrastructure allows you to treat your infrastructure as just code. You can author it with \\nany code editor, check it into a version control system, and review the files with team members before \\ndeploying into production. \\nCloudTrail \\n\\uf0b7 \\nAWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk \\nauditing of your AWS account.  \\n\\uf0b7 \\nActions taken by a user, role, or an AWS service are recorded as events in CloudTrail.  \\n\\uf0b7 \\nEvents include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS \\nSDKs and APIs. \\n\\uf0b7 \\nCloudTrail is enabled on your AWS account when you create it.  \\n\\uf0b7 \\nWhen activity occurs in your AWS account, that activity is recorded in a CloudTrail event.  \\n\\uf0b7 \\nYou can easily view recent events in the CloudTrail console by going to Event history.  \\n\\uf0b7 \\nFor an ongoing record of activity and events in your AWS account, create a trail.  \\n\\uf0b7 \\nVisibility into your AWS account activity is a key aspect of security and operational best practices.  \\n\\uf0b7 \\nYou can use CloudTrail to view, search, download, archive, analyze, and respond to account activity across \\nyour AWS infrastructure.  \\n\\uf0b7 \\nYou can identify who or what took which action, what resources were acted upon, when the event \\noccurred, and other details to help you analyze and respond to activity in your AWS account.  \\n\\uf0b7 \\nOptionally, you can enable AWS CloudTrail Insights on a trail to help you identify and respond to unusual \\nactivity. \\n\\uf0b7 \\nYou can integrate CloudTrail into applications using the API, automate trail creation for your organization, \\ncheck the status of trails you create, and control how users view CloudTrail events. \\nWorking of CloudTrail \\n\\uf0b7 \\nYou can create two types of trails for an AWS account: \\nA trail that applies to all regions \\no When you create a trail that applies to all regions, CloudTrail records events in each region and delivers \\nthe CloudTrail event log files to an S3 bucket that you specify.  \\no If a region is added after you create a trail that applies to all regions that new region is automatically \\nincluded, and events in that region are logged.  \\no This is the default option when you create a trail in the CloudTrail console. \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n10 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nA trail that applies to one region \\no When you create a trail that applies to one region, CloudTrail records the events in that region only.  \\no It then delivers the CloudTrail event log files to an Amazon S3 bucket that you specify.  \\no If you create additional single trails, you can have those trails deliver CloudTrail event log files to the \\nsame Amazon S3 bucket or to separate buckets.  \\no This is the default option when you create a trail using the AWS CLI or the CloudTrail API. \\n\\uf0b7 \\nBeginning on April 12, 2019, trails will be viewable only in the AWS Regions where they log events.  \\n\\uf0b7 \\nIf you create a trail that logs events in all AWS Regions, it will appear in the console in all AWS Regions.  \\n\\uf0b7 \\nIf you create a trail that only logs events in a single AWS Region, you can view and manage it only in that \\nAWS Region. \\n\\uf0b7 \\nIf you have created an organization in AWS Organizations, you can also create a trail that will log all events \\nfor all AWS accounts in that organization.  \\n\\uf0b7 \\nThis is referred to as an organization trail. Organization trails can apply to all AWS Regions or one Region.  \\n\\uf0b7 \\nOrganization trails must be created in the master account, and when specified as applying to an \\norganization, are automatically applied to all member accounts in the organization.  \\n\\uf0b7 \\nMember accounts will be able to see the organization trail, but cannot modify or delete it.  \\n\\uf0b7 \\nBy default, member accounts will not have access to the log files for the organization trail in the Amazon \\nS3 bucket. \\n\\uf0b7 \\nYou can change the configuration of a trail after you create it, including whether it logs events in one \\nregion or all regions.  \\n\\uf0b7 \\nYou can also change whether it logs data or CloudTrail Insights events.  \\n\\uf0b7 \\nChanging whether a trail logs events in one region or in all regions affects which events are logged. \\n\\uf0b7 \\nBy default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE).  \\n\\uf0b7 \\nYou can also choose to encrypt your log files with an AWS Key Management Service (AWS KMS) key.  \\n\\uf0b7 \\nYou can store your log files in your bucket for as long as you want.  \\n\\uf0b7 \\nYou can also define Amazon S3 lifecycle rules to archive or delete log files automatically.  \\n\\uf0b7 \\nIf you want notifications about log file delivery and validation, you can set up Amazon SNS notifications. \\n\\uf0b7 \\nCloudTrail typically delivers log files within 15 minutes of account activity.  \\n\\uf0b7 \\nIn addition, CloudTrail publishes log files multiple times an hour, about every five minutes.  \\n\\uf0b7 \\nThese log files contain API calls from services in the account that support CloudTrail. \\nBenefits of CloudTrail \\nSimplified compliance \\n\\uf0b7 \\nWith AWS CloudTrail, simplify your compliance audits by automatically recording and storing event logs \\nfor actions made within your AWS account.  \\n\\uf0b7 \\nIntegration with Amazon CloudWatch Logs provides a convenient way to search through log data, identify \\nout-of-compliance events, accelerate incident investigations, and expedite responses to auditor requests. \\nSecurity analysis and troubleshooting \\n\\uf0b7 \\nWith AWS CloudTrail, you can discover and troubleshoot security and operational issues by capturing a \\ncomprehensive history of changes that occurred in your AWS account within a specified period of time. \\nVisibility into user and resource activity \\n\\uf0b7 \\nAWS CloudTrail increases visibility into your user and resource activity by recording AWS Management \\nConsole actions and API calls.  \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n11 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nYou can identify which users and accounts called AWS, the source IP address from which the calls were \\nmade, and when the calls occurred. \\nSecurity automation \\n\\uf0b7 \\nAWS CloudTrail allows you track and automatically respond to account activity threatening the security of \\nyour AWS resources.  \\n\\uf0b7 \\nWith Amazon CloudWatch Events integration, you can define workflows that execute when events that \\ncan result in security vulnerabilities are detected.  \\n\\uf0b7 \\nFor example, you can create a workflow to add a specific policy to an Amazon S3 bucket when CloudTrail \\nlogs an API call that makes that bucket public. \\nOpsWorks \\n\\uf0b7 \\nAWS OpsWorks is a configuration management service that provides managed instances of Chef and \\nPuppet.  \\n\\uf0b7 \\nChef and Puppet are automation platforms that allow you to use code to automate the configurations of \\nyour servers.  \\n\\uf0b7 \\nOpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed \\nacross your Amazon EC2 instances or on-premises compute environments.  \\n\\uf0b7 \\nOpsWorks has three offerings, AWS Opsworks for Chef Automate, AWS OpsWorks for Puppet Enterprise, \\nand AWS OpsWorks Stacks. \\nAWS OpsWorks for Chef Automate \\n\\uf0b7 \\nAWS OpsWorks for Chef Automate is a fully managed configuration management service that hosts Chef \\nAutomate, a suite of automation tools from Chef for configuration management, compliance and security, \\nand continuous deployment.  \\n\\uf0b7 \\nOpsWorks also maintains your Chef server by automatically patching, updating, and backing up your \\nserver.  \\n\\uf0b7 \\nOpsWorks eliminates the need to operate your own configuration management systems or worry about \\nmaintaining its infrastructure.  \\n\\uf0b7 \\nOpsWorks gives you access to all of the Chef Automate features, such as configuration and compliance \\nmanagement, which you manage through the Chef Console or command line tools like Knife.  \\n\\uf0b7 \\nIt also works seamlessly with your existing Chef cookbooks. \\n\\uf0b7 \\nChoose AWS OpsWorks for Chef Automate if you are an existing Chef user. \\nAWS OpsWorks for Puppet Enterprise \\n\\uf0b7 \\nAWS OpsWorks for Puppet Enterprise is a fully managed configuration management service that hosts \\nPuppet Enterprise, a set of automation tools from Puppet for infrastructure and application management.  \\n\\uf0b7 \\nOpsWorks also maintains your Puppet master server by automatically patching, updating, and backing up \\nyour server.  \\n\\uf0b7 \\nOpsWorks eliminates the need to operate your own configuration management systems or worry about \\nmaintaining its infrastructure.  \\n\\uf0b7 \\nOpsWorks gives you access to all of the Puppet Enterprise features, which you manage through the Puppet \\nconsole.  \\n\\uf0b7 \\nIt also works seamlessly with your existing Puppet code. \\n\\uf0b7 \\nChoose AWS OpsWorks for Puppet Enterprise if you are an existing Puppet user. \\n\\n \\nUnit-8 – Other AWS Services & Management \\nServices \\n \\n \\n12 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nAWS OpsWorks Stacks \\n\\uf0b7 \\nAWS OpsWorks Stacks is an application and server management service. With OpsWorks Stacks, you can \\nmodel your application as a stack containing different layers, such as load balancing, database, and \\napplication server.  \\n\\uf0b7 \\nWithin each layer, you can provision Amazon EC2 instances, enable automatic scaling, and configure your \\ninstances with Chef recipes using Chef Solo.  \\n\\uf0b7 \\nThis allows you to automate tasks such as installing packages and programming languages or frameworks, \\nconfiguring software, and more. \\n\\uf0b7 \\nChoose AWS OpsWorks Stacks if you need a solution for application modeling and management. \\nOpenID Connect (OIDC) \\n\\uf0b7 \\nIAM OIDC identity providers are entities in IAM that describe an external identity provider (IdP) service \\nthat supports the OpenID Connect (OIDC) standard, such as Google or Salesforce.  \\n\\uf0b7 \\nYou use an IAM OIDC identity provider when you want to establish trust between an OIDC-compatible IdP \\nand your AWS account.  \\n\\uf0b7 \\nThis is useful when creating a mobile app or web application that requires access to AWS resources, but \\nyou don\\'t want to create custom sign-in code or manage your own user identities.  \\n\\uf0b7 \\nYou can create and manage an IAM OIDC identity provider using the AWS Management Console, the AWS \\nCommand Line Interface, the Tools for Windows PowerShell, or the IAM API. \\n\\uf0b7 \\nWhen you create an OpenID Connect (OIDC) identity provider in IAM, you must supply a thumbprint.  \\n\\uf0b7 \\nIAM requires the thumbprint for the root certificate authority (CA) that signed the certificate used by the \\nexternal identity provider (IdP).  \\n\\uf0b7 \\nThe thumbprint is a signature for the CA\\'s certificate that was used to issue the certificate for the OIDC-\\ncompatible IdP.  \\n\\uf0b7 \\nWhen you create an IAM OIDC identity provider, you are trusting identities authenticated by that IdP to \\nhave access to your AWS account.  \\n\\uf0b7 \\nBy supplying the CA\\'s certificate thumbprint, you trust any certificate issued by that CA with the same DNS \\nname as the one registered.  \\n\\uf0b7 \\nThis eliminates the need to update trusts in each account when you renew the IdP\\'s signing certificate. \\n\\uf0b7 \\nYou can create an IAM OIDC identity provider with the AWS Command Line Interface, the Tools for \\nWindows PowerShell, or the IAM API.  \\n\\uf0b7 \\nWhen you use these methods, you must obtain the thumbprint manually and supply it to AWS.  \\n\\uf0b7 \\nWhen you create an OIDC identity provider with the IAM console, the console attempts to fetch the \\nthumbprint for you.  \\n\\uf0b7 \\nWe recommend that you also obtain the thumbprint for your OIDC IdP manually and verify that the \\nconsole fetched the correct thumbprint. \\n\\n \\nUnit-9 – AWS Billing & Dealing with Disaster \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nManaging Costs, Utilization and Tracking \\n\\uf0b7 \\nThe cloud allows you to trade capital expenses (such as data centers and physical servers) for variable \\nexpenses, and only pay for IT as you consume it.  \\n\\uf0b7 \\nAnd, because of the economies of scale, the variable expenses are much lower than what you would pay \\nto do it yourself.  \\n\\uf0b7 \\nWhether you were born in the cloud, or you are just starting your migration journey to the cloud, AWS \\nhas a set of solutions to help you manage and optimize your spend. \\n\\uf0b7 \\nDuring this unprecedented time, many businesses and organizations are facing disruption to their \\noperations, budgets, and revenue.  \\n\\uf0b7 \\nAWS has a set of solutions to help you with cost management and optimization.  \\n\\uf0b7 \\nThis includes services, tools, and resources to organize and track cost and usage data, enhance control \\nthrough consolidated billing and access permission, enable better planning through budgeting and \\nforecasts, and further lower cost with resources and pricing optimizations. \\nAWS Cost Management Solutions \\nOrganize and Report Cost and Usage Based on User-Defined Methods \\n\\uf0b7 \\nYou need complete, near real-time visibility of your cost and usage information to make informed \\ndecisions.  \\n\\uf0b7 \\nAWS equips you with tools to organize your resources based on your needs, visualize and analyze cost and \\nusage data in a single pane of glass, and accurately chargeback to appropriate entities (e.g. department, \\nproject, and product).  \\n\\uf0b7 \\nRather than centrally policing the cost, you can provide real-time cost data that makes sense to your \\nengineering, application, and business teams.  \\n\\uf0b7 \\nThe detailed, allocable cost data allows teams to have the visibility and details to be accountable of their \\nown spend. \\nBilling with Built-in Control \\n\\uf0b7 \\nBusiness and organization leaders need a simple and easy way to access AWS billing information, including \\na spend summary, a breakdown of all service costs incurred by accounts across the organization, along \\nwith discounts and credits.  \\n\\uf0b7 \\nCustomer can choose to consolidate your bills and take advantage of higher volume discounts based on \\naggregated usage across your bills.  \\n\\uf0b7 \\nLeaders also need to set appropriate guardrails in place so you can maintain control over cost, governance, \\nand security.  \\n\\uf0b7 \\nAWS helps organizations balance freedom and control by enabling the governance of granular user \\npermission. \\nImproved Planning with Flexible Forecasting and Budgeting \\n\\uf0b7 \\nBusinesses and organizations need to plan and set expectations around cloud costs for your projects, \\napplications, and more.  \\n\\uf0b7 \\nThe emergence of the cloud allowed teams to acquire and deprecate resources on an ongoing basis, \\nwithout relying on teams to approve, procure and install infrastructure.  \\n\\uf0b7 \\nHowever, this flexibility requires organizations to adapt to the new, dynamic forecasting and budgeting \\nprocess.  \\n\\n \\nUnit-9 – AWS Billing & Dealing with Disaster \\n \\n \\n2 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nAWS provides forecasts based on your cost and usage history and allows you to set budget threshold and \\nalerts, so you can stay informed whenever cost and usage is forecasted to, or exceeds the threshold limit.  \\n\\uf0b7 \\nYou can also set reservation utilization and/or coverage targets for your Reserved Instances and Savings \\nPlans and monitor how they are progressing towards your target. \\nOptimize Costs with Resource and Pricing Recommendations \\n\\uf0b7 \\nWith AWS, customers can take control of your cost and continuously optimize your spend.  \\n\\uf0b7 \\nThere are a variety of AWS pricing models and resources you can choose from to meet requirements for \\nboth performance and cost efficiency, and adjust as needed.  \\n\\uf0b7 \\nWhen evaluating AWS services for your architectural and business needs, you will have the flexibility to \\nchoose from a variety of elements, such as operating systems, instance types, availability zones, and \\npurchase options.  \\n\\uf0b7 \\nAWS offers resources optimization recommendations to simplify the evaluation process so you can \\nefficiently select the cost-optimized resources.  \\n\\uf0b7 \\nWe also provide recommendations around pricing models (up to 72% with Reserved Instances and Savings \\nPlans and up to 90% with Spot Instances) based on your utilization patterns, so you can further drive down \\nyour cost without compromising workload performance. \\nMonitor, Track, and Analyze Your AWS Costs & Usage \\n\\uf0b7 \\nAppropriate management, tracking and measurement are fundamental in achieving the full benefits of \\ncost optimization. \\nAmazon CloudWatch \\n\\uf0b7 \\nAmazon CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, \\nproviding you with a unified view of AWS resources, applications, and services that run on AWS and on-\\npremises servers. \\nAWS Trusted Advisor \\n\\uf0b7 \\nAWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your \\nresources following AWS best practices. \\nAWS Cost Explorer \\n\\uf0b7 \\nAWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS \\ncosts and usage over time. \\nBottom Line Impact \\n\\uf0b7 \\nAs AWS provide large range of service and we can utilize it for our business on pay as you go basis so it \\nwill save our cost and time. \\n\\uf0b7 \\nDue to that company can reduce their cost and increase revenue by focusing on core work and other \\nservice management is done by cloud providers. \\n\\uf0b7 \\nIt will create bottom line impact for organization. \\nGeographic Concerns \\n\\uf0b7 \\nThe AWS Global Cloud Infrastructure is the most secure, extensive, and reliable cloud platform, offering \\nover 175 fully featured services from data centers globally.  \\n\\n \\nUnit-9 – AWS Billing & Dealing with Disaster \\n \\n \\n3 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nWhether you need to deploy your application workloads across the globe in a single click, or you want to \\nbuild and deploy specific applications closer to your end-users with single-digit millisecond latency, AWS \\nprovides you the cloud infrastructure where and when you need it. \\n\\uf0b7 \\nWith millions of active customers and tens of thousands of partners globally, AWS has the largest and \\nmost dynamic ecosystem.  \\n\\uf0b7 \\nCustomers across virtually every industry and of every size, including start-ups, enterprises, and public \\nsector organizations, are running every imaginable use case on AWS. \\nFailure plans / Disaster Recovery (DR) \\n\\uf0b7 \\nOur data is the most precious asset that we have and protecting it is our top priority.  \\n\\uf0b7 \\nCreating backups of our data to an off shore data center, so that in the event of an on premise failure we \\ncan switch over to our backup, is a prime focus for business continuity.  \\n\\uf0b7 \\nAs AWS says, ‘Disaster recovery is a continual process of analysis and improvement, as business and \\nsystems evolve.  For each business service, customers need to establish an acceptable recovery point and \\ntime, and then build an appropriate DR solution.’ \\n\\uf0b7 \\nBackup and DR on Cloud reduces costs by half as compared to maintaining your own redundant data \\ncenters. And if you think about it, it’s really not that surprising.  \\n\\uf0b7 \\nImagine the kind of cost you would entail in buying and maintaining servers and data centers, providing \\nsecure and stable connectivity and not to mention keeping them secure.  \\n\\uf0b7 \\nYou would also be underutilizing severs; and in times of unpredictable traffic rise it would be strenuous to \\nset up new ones. To all these cloud provides a seamless transition reducing cost dramatically. \\n4 Standard Approaches of Backup and Disaster Recovery Using Amazon Cloud  \\n 1. Backup and Recovery \\n\\uf0b7 \\nTo recover your data in the event of any disaster, you must first have your data periodically backed up \\nfrom your system to AWS.  \\n\\uf0b7 \\nBacking up of data can be done through various mechanisms and your choice will be based on the RPO \\n(Recovery Point Objective- So if your disaster struck at 2 pm and your RPO is 1 hr, your Backup & DR will \\nrestore all data till 1 pm.) that will suit your business needs.  \\n\\uf0b7 \\nAWS offers AWS Direct connect and Import Export services that allow for faster backup.  \\n\\uf0b7 \\nFor example, if you have a frequently changing database like say a stock market, then you will need a very \\nhigh RPO. However if your data is mostly static with a low frequency of changes, you can opt for periodic \\nincremental backup.  \\n\\uf0b7 \\nOnce your backup mechanisms are activated you can pre-configure AMIs (operating systems & application \\nsoftware).  \\n\\uf0b7 \\nNow when a disaster strikes, EC2 (Elastic Compute Capacity)  instances in the Cloud using EBS (Elastic Block \\nStore) coupled with AMIs can access your data from the S3 (Simple Storage Service) buckets to revive your \\nsystem and keep it going. \\n2. Pilot Light Approach \\n\\uf0b7 \\nThe name pilot light comes from the gas heater analogy. Just as in a heater you have a small flame that is \\nalways on, and can quickly ignite the entire furnace; a similar approach can be thought of about your data \\nsystem.  \\n\\uf0b7 \\nIn the preparatory phase your on premise database server mirrors data to data volumes on AWS. The \\ndatabase server on cloud is always activated for frequent or continuous incremental backup.  \\n\\n \\nUnit-9 – AWS Billing & Dealing with Disaster \\n \\n \\n4 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\n\\uf0b7 \\nThis core area is the pilot from our gas heater analogy. The application and caching server replica \\nenvironments are created on cloud and kept in standby mode as very few changes take place over time.  \\n\\uf0b7 \\nThese AMIs can be updated periodically. This is the entire furnace from our example. If the on premise \\nsystem fails, then the application and caching servers get activated; further users are rerouted using elastic \\nIP addresses to the ad hoc environment on cloud. Your Recovery takes just a few minutes. \\n3. Warm Standby Approach \\n\\uf0b7 \\nThis Technique is the next level of the pilot light, reducing recovery time to almost zero.  \\n\\uf0b7 \\nYour application and caching servers are set up and always activated based on your business critical \\nactivities but only a minimum sized fleet of EC2 instances are dedicated.  \\n\\uf0b7 \\nThe backup system is not capable of handling production load, but can be used for testing, quality \\nassurance and other internal uses.  \\n\\uf0b7 \\nIn the event of a disaster, when your on premise data center fails, two things happen.  \\n\\uf0b7 \\nFirstly multiple EC2 instances are dedicated (vertical and horizontal scaling) to bring your application and \\ncaching environment up to production load. ELB and Auto Scaling (for distributing traffic) are used to ease \\nscaling up.  \\n\\uf0b7 \\nSecondly using Amazon Route 53 user traffic is rerouted instantly using elastic IP addresses and there is \\ninstant recovery of your system with almost zero down time. \\n4. Multi-Site Approach \\n\\uf0b7 \\nWell this is the optimum technique in backup and DR and is the next step after warm standby.  \\n\\uf0b7 \\nAll activities in the preparatory stage are similar to a warm standby; except that AWS backup on Cloud is \\nalso used to handle some portions of the user traffic using Route 53.  \\n\\uf0b7 \\nWhen a disaster strikes, the rest of the traffic that was pointing to the on premise servers are rerouted to \\nAWS and using auto scaling techniques multiple EC2 instances are deployed to handle full production \\ncapacity.  \\n\\uf0b7 \\nYou can further increase the availability of your multi-site solution by designing Multi-AZ architectures. \\nExamining Logs \\n\\uf0b7 \\nIt is necessary to examine the log files in order to locate an error code or other indication of the issue that \\nyour cluster experienced.  \\n\\uf0b7 \\nIt may take some investigative work to determine what happened.  \\n\\uf0b7 \\nHadoop runs the work of the jobs in task attempts on various nodes in the cluster.  \\n\\uf0b7 \\nAmazon EMR can initiate speculative task attempts, terminating the other task attempts that do not \\ncomplete first.  \\n\\uf0b7 \\nThis generates significant activity that is logged to the controller, stderr and syslog log files as it happens.  \\n\\uf0b7 \\nIn addition, multiple tasks attempts are running simultaneously, but a log file can only display results \\nlinearly. \\n\\uf0b7 \\nStart by checking the bootstrap action logs for errors or unexpected configuration changes during the \\nlaunch of the cluster.  \\n\\uf0b7 \\nFrom there, look in the step logs to identify Hadoop jobs launched as part of a step with errors.  \\n\\uf0b7 \\nExamine the Hadoop job logs to identify the failed task attempts.  \\n\\uf0b7 \\nThe task attempt log will contain details about what caused a task attempt to fail. \\n\\n \\nReferences \\n \\n \\n1 \\n Prof. Vijay M. Shekhat, CE Department \\n| 2180712 – Cloud Infrastructure and Services \\nBook \\n1. Cloud Computing Bible, Barrie Sosinsky, John Wiley & Sons, ISBN-13: 978-0470903568. \\n2. Mastering AWS Security, Albert Anthony, Packt Publishing Ltd., ISBN 978-1-78829-372-3. \\n3. Amazon Web Services for Dummies, Bernard Golden, For Dummies, ISBN-13: 978- 1118571835. \\nWebsites \\n1. www.aws.amazon.com \\n2. www.docs.aws.amazon.com \\n3. www.bluepiit.com \\n4. www.inforisktoday.com \\n5. www.techno-pulse.com \\n6. www.exelanz.com \\n7. www.ibm.com \\n8. www.iarjset.com/upload/2017/july-17/IARJSET%2018.pdf \\n9. www.searchservervirtualization.techtarget.com \\n10. www.docs.eucalyptus.com \\n11. www.cloudacademy.com \\n12. www.searchaws.techtarget.com \\n13. www.searchsecurity.techtarget.com \\n14. www.en.wikipedia.org/wiki/Cloud_computing_security \\n15. www.znetlive.com \\n16. www.en.wikipedia.org/wiki/Virtual_private_cloud \\n17. www.resource.onlinetech.com \\n18. www.globalknowledge.com \\n19. www.blog.blazeclan.com/4-approaches-backup-disaster-recovery-explained-amazon-cloud \\n20. www.zdnet.com/article/what-is-cloud-computing-everything-you-need-to-know-about-the-cloud \\n21. www.javatpoint.com/introduction-to-cloud-computing \\n22. www.javatpoint.com/history-of-cloud-computing \\n23. www.allcloud.io/blog/6-cloud-computing-concerns-facing-2018 \\n24. www.searchitchannel.techtarget.com/definition/cloud-marketplace \\n25. www.en.wikipedia.org/wiki/Amazon_Web_Services \\n26. www.msystechnologies.com/blog/cloud-orchestration-everything-you-want-to-know \\n27. www.linuxacademy.com/blog/linux-academy/elasticity-cloud-computing \\n28. www.searchitchannel.techtarget.com/definition/Eucalyptus \\n29. www.geeksforgeeks.org/virtualization-cloud-computing-types \\n30. www.cloudsearch.blogspot.com \\n31. www.simplilearn.com/tutorials/aws-tutorial/aws-iam \\n32. www.d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf \\n33. www.resources.intenseschool.com/amazon-aws-understanding-ec2-key-pairs-and-how-they-are-used-\\nfor-windows-and-linux-instances/ \\n34. www.pagely.com/blog/amazon-ec2/ \\n35. www.cloudflare.com/learning/cloud/what-is-multitenancy/ \\n36. www.hevodata.com/blog/amazon-redshift-pros-and-cons/ \\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import fitz\n",
        "\n",
        "def extract_text_from_pdf(path):\n",
        "    doc = fitz.open(path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text\n",
        "\n",
        "path = \"/content/gtu_study_materialDataset.pdf\"\n",
        "text_data = extract_text_from_pdf(path)\n",
        "text_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JElzkP-kT2L1"
      },
      "source": [
        "# **Extract Image from PDF :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_gImxGmT84h",
        "outputId": "5d5384fe-4671-4f47-85c7-25f05e98cce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted 103 images and saved in 'Images' folder.\n"
          ]
        }
      ],
      "source": [
        "import fitz\n",
        "import os\n",
        "\n",
        "def extract_images_from_pdf(path, output_folder=\"Images\"):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    doc = fitz.open(path)\n",
        "    image_count = 0\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        for img_index, img in enumerate(doc[page_num].get_images(full=True)):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "            image_ext = base_image[\"ext\"]\n",
        "\n",
        "            image_filename = os.path.join(output_folder, f\"page{page_num+1}_img{img_index+1}.{image_ext}\")\n",
        "            with open(image_filename, \"wb\") as img_file:\n",
        "                img_file.write(image_bytes)\n",
        "\n",
        "            image_count += 1\n",
        "\n",
        "    print(f\"✅ Extracted {image_count} images and saved in '{output_folder}' folder.\")\n",
        "\n",
        "path = \"/content/gtu_study_materialDataset.pdf\"\n",
        "extract_images_from_pdf(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpmYu5eXWPEY",
        "outputId": "d44d91b5-3847-4d82-bb3d-fc7ccc7df74f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Links saved to extracted_links.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://search.examplecompany.com/CompanyDirectory/EmployeeInfo?empname=BernardGolden',\n",
              " 'http://bucket.s3.amazonaws.com/',\n",
              " 'http://bucket.s3-aws-region.amazonaws.com/',\n",
              " 'http://s3.amazonaws.com/bucket',\n",
              " 'http://s3-aws-region.amazonaws.com/bucket',\n",
              " 'https://aws.amazon.com/macie/?c=sc&sec=srv',\n",
              " 'https://aws.amazon.com/kms/?c=sc&sec=srv',\n",
              " 'https://aws.amazon.com/cloudhsm/?c=sc&sec=srv',\n",
              " 'https://aws.amazon.com/certificate-manager/?c=sc&sec=srv',\n",
              " 'https://aws.amazon.com/secrets-manager/?c=sc&sec=srv',\n",
              " 'https://aws.amazon.com/artifact/?c=sc&sec=srv',\n",
              " 'http://hevodata.com/']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def extract_links_from_pdf(path):\n",
        "    doc = fitz.open(path)\n",
        "    links = []\n",
        "    for page in doc:\n",
        "        links += [link[\"uri\"] for link in page.get_links() if \"uri\" in link]\n",
        "    return links\n",
        "\n",
        "\n",
        "\n",
        "links_data = extract_links_from_pdf(path)\n",
        "\n",
        "\n",
        "def save_links_to_txt(links, filename=\"extracted_links.txt\"):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        for link in links:\n",
        "            file.write(link + \"\\n\")\n",
        "    print(f\"✅ Links saved to {filename}\")\n",
        "\n",
        "# Save links\n",
        "save_links_to_txt(links_data)\n",
        "\n",
        "links_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhz1j9VxbCfB"
      },
      "source": [
        "# **Clean the Data :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l6GNa1-bHEw",
        "outputId": "7ec504f2-6444-4072-bc1e-9190ea15e08d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unit-1 – Introduction to Cloud Technologies 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Cloud Computing Cloud computing is the delivery of on-demand computing services, from applications to storage and processing power, typically over the internet and on a pay-as-you-go basis. Introduction to Cloud Computing Fig. Cloud Computing Cloud Computing is the delivery of computing services such as servers, storage, databases, networking, software, analytics, intelligence, and more, over the Cloud (Internet). Cloud Computing provides an alternative to the on-premises datacenter. With an on-premises datacenter, we have to manage everything, such as purchasing and installing hardware, virtualization, installing the operating system, and any other required applications, setting up the network, configuring the firewall, and setting up storage for data. After doing all the set-up, we become responsible for maintaining it through its entire lifecycle. But if we choose Cloud Computing, a cloud vendor is responsible for the hardware purchase and maintenance. They also provide a wide variety of software and platform as a service. We can take any required services on rent. The cloud computing services will be charged based on usage. The cloud environment provides an easily accessible online portal that makes handy for the user to manage the compute, storage, network, and application resources. Characteristics (Features) of Cloud Computing The five essential characteristics of cloud computing: 1. On-demand self-service: A consumer can separately provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider. 2. Broad network access: Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops and workstations). 3. Resource pooling: The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned Unit-1 – Introduction to Cloud Technologies 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services according to consumer demand. There is a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state or datacenter). Examples of resources include storage, processing, memory and network bandwidth. 4. Rapid elasticity: Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward matching with demand. To the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time. 5. Measured service: Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth and active user accounts). Resource usage can be monitored, controlled and reported, providing transparency for the provider and consumer. Advantages of Cloud Computing Cost: It reduces the huge capital costs of buying hardware and software. Speed: Resources can be accessed in minutes, typically within a few clicks. Scalability: We can increase or decrease the requirement of resources according to the business requirements. Productivity: While using cloud computing, we put less operational effort. We do not need to apply patching, as well as no need to maintain hardware and software. So, in this way, the IT team can be more productive and focus on achieving business goals. Reliability: Backup and recovery of data are less expensive and very fast for business continuity. Security: Many cloud vendors offer a broad set of policies, technologies, and controls that strengthen our data security. Disadvantages of Cloud Computing Requires good speed internet with good bandwidth: To access your cloud services, you need to have a good internet connection always with good bandwidth to upload or download files to/from the cloud Downtime: Since the cloud requires high internet speed and good bandwidth, there is always a possibility of service outage, which can result in business downtime. Today, no business can afford revenue or business loss due to downtime or slow down from an interruption in critical business processes. Limited control of infrastructure: Since you are not the owner of the infrastructure of the cloud, hence you don’t have any control or have limited access to the cloud infra. Restricted or limited flexibility: The cloud provides a huge list of services, but consuming them comes with a lot of restrictions and limited flexibility for your applications or developments. Also, platform dependency or ‘vendor lock-in’ can sometimes make it difficult for you to migrate from one provider to another. Ongoing costs: Although you save your cost of spending on whole infrastructure and its management, on the cloud, you need to keep paying for services as long as you use them. But in traditional methods, you only need to invest once. Security: Security of data is a big concern for everyone. Since the public cloud utilizes the internet, your data may become vulnerable. In the case of a public cloud, it depends on the cloud provider to take care of your data. So, before opting for cloud services, it is required that you find a provider who follows maximum compliance policies for data security. Unit-1 – Introduction to Cloud Technologies 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Vendor Lock-in: Although the cloud service providers assure you that they will allow you to switch or migrate to any other service provider whenever you want, it is a very difficult process. You will find it complex to migrate all the cloud services from one service provider to another. During migration, you might end up facing compatibility, interoperability and support issues. To avoid these issues, many customers choose not to change the vendor. Technical issues: Even if you are a tech whiz, the technical issues can occur, and everything can’t be resolved in-house. To avoid interruptions, you will need to contact your service provider for support. However, not every vendor provides 24/7 support to their clients. Difference between Conventional Computing and Cloud Computing Conventional Computing Cloud Computing In conventional computing environment more time is needed for installation, set up, and configuration. Once the cloud computing environment is set up initially, you can gain access faster than conventional computing Cost must be paid in advance Pay-as-you-go Cost is fixed Cost is variable Economic to scale for all organization Economic to scale for large organization only For Scaling manual effort is needed Scaling can be elastic and automatic Environment is mix of physical and virtualized Usually environment is virtualized History of Cloud Computing Before emerging the cloud computing, there was Client/Server computing which is basically a centralized storage in which all the software applications, all the data and all the controls are resided on the server side. If a single user wants to access specific data or run a program, he/she need to connect to the server and then gain appropriate access, and then he/she can do his/her business. Then after, distributed computing came into picture, where all the computers are networked together and share their resources when needed. On the basis of above computing, there was emerged of cloud computing concepts that later implemented. At around in 1961, John MacCharty suggested in a speech at MIT that computing can be sold like a utility, just like a water or electricity. It was a brilliant idea, but like all brilliant ideas, it was ahead of its time, as for the next few decades, despite interest in the model, the technology simply was not ready for it. But of course time has passed and the technology caught that idea and after few years we mentioned that: o In 1999, Salesforce.com started delivering of applications to users using a simple website. The applications were delivered to enterprises over the Internet, and this way the dream of computing sold as utility were true. o In 2002, Amazon started Amazon Web Services, providing services like storage, computation and even human intelligence. However, only starting with the launch of the Elastic Compute Cloud in 2006 a truly commercial service open to everybody existed. o In 2009, Google Apps also started to provide cloud computing enterprise applications. o Of course, all the big players are present in the cloud computing evolution, some were earlier and some were later. In 2009, Microsoft launched Windows Azure, and companies like Oracle and HP have all joined the game. This proves that today, cloud computing has become mainstream. Unit-1 – Introduction to Cloud Technologies 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Cloud Orchestration Cloud Orchestration is a way to manage, co-ordinate, and provision all the components of a cloud platform automatically from a common interface. It orchestrates the physical as well as virtual resources of the cloud platform. Cloud orchestration is a must because cloud services scale up arbitrarily and dynamically, include fulfillment assurance and billing, and require workflows in various business and technical domains. Orchestration tools combine automated tasks by interconnecting the processes running across the heterogeneous platforms in multiple locations. Orchestration tools create declarative templates to convert the interconnected processes into a single workflow. The processes are so orchestrated that the new environment creation workflow is achieved with a single API call. Creation of these declarative templates, though complex and time consuming, is simplified by the orchestration tools. Cloud orchestration includes two types of models: o Single Cloud model o Multi-cloud model In Single cloud model, all the applications designed for a system run on the same IaaS platform (same cloud service provider). Applications, interconnected to create a single workflow, running on various cloud platforms for the same organization define the concept of multi-cloud model. IaaS requirement for some applications, though designed for same system, might vary. This results in availing services of multiple cloud service providers. For example, application with patient’s sensitive medical data might reside in some IaaS, whereas the application for online OPD appointment booking might reside in another IaaS, but they are interconnected to form one system. This is called multi-cloud orchestration. Multi-cloud models provide high redundancy as compared to single IaaS deployments. This reduces the risk of down time. Elasticity in Cloud Elasticity covers the ability to scale up but also the ability to scale down. The idea is that you can quickly provision new infrastructure to handle a high load of traffic. But what happens after that rush? If you leave all of these new instances running, your bill will skyrocket as you will be paying for unused resources. In the worst case scenario, these resources can even cancel out revenue from the sudden rush. An elastic system prevents this from happening. After a scaled up period, your infrastructure can scale back down, meaning you will only be paying for your usual resource usage and some extra for the high traffic period. The key is that this all happens automatically. When resource needs meet a certain threshold (usually measured by traffic), the system “knows” that it needs to de-provision a certain amount of infrastructure, and does so. With a couple hours of training, anyone can use the AWS web console to manually add or subtract instances. But it takes a true Solutions Architect to set up monitoring, account for provisioning time, and configure a system for maximum elasticity. Unit-1 – Introduction to Cloud Technologies 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Cloud Service Options / Cloud Service Models / Cloud Computing Stack Fig.: Cloud Services Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. Although cloud computing has evolved over the time it has been majorly divided into three broad service categories: 1. Infrastructure as a Service(IAAS), 2. Platform as a Service (PAAS) and 3. Software as a Service (SAAS) 1. Infrastructure as a Service (IAAS) Infrastructure as a Service (IAAS) is a form of cloud computing that provides virtualized computing resources over the internet. In an IAAS model, a third party provider hosts hardware, software, servers, storage and other infrastructure components on the behalf of its users. IAAS providers also host users’ applications and handle tasks including system maintenance backup and resiliency planning. IAAS platforms offer highly scalable resources that can be adjusted on-demand which makes it a well- suited for workloads that are temporary, experimental or change unexpectedly. Other characteristics of IAAS environments include the automation of administrative tasks, dynamic scaling, desktop virtualization and policy based services. Technically, the IaaS market has a relatively low barrier of entry, but it may require substantial financial investment in order to build and support the cloud infrastructure. Mature open-source cloud management frameworks like OpenStack are available to everyone, and provide strong a software foundation for companies that want to build their private cloud or become a public cloud provider. IAAS- Network: There are two major network services offered by public cloud service providers: 1. load balancing and 2. DNS (domain name systems). Unit-1 – Introduction to Cloud Technologies 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Load balancing provides a single point of access to multiple servers that run behind it. A load balancer is a network device that distributes network traffic among servers using specific load balancing algorithms. DNS is a hierarchical naming system for computers, or any other naming devices that use IP addressing for network identification – a DNS system associates domain names with IP addresses. 2. Platform as a Service (PAAS) Platform as a Service (PAAS) is a cloud computing model that delivers applications over the internet. In a PAAS model, a cloud provider delivers hardware and software tools, usually those needed for application development, to its users as a service. A PAAS provider hosts the hardware and software on its own infrastructure. As a result, PAAS frees users from having to install in-house hardware and software to develop or run a new application. PAAS doesn’t replace a business' entire infrastructure but instead a business relies on PAAS providers for key services, such as Java development or application hosting. A PAAS provider, however, supports all the underlying computing and software, users only need to login and start using the platform-usually through a Web browser interface. PAAS providers then charge for that access on a per-use basis or on monthly basis. Some of the main characteristics of PAAS are: 1) Scalability and auto-provisioning of the underlying infrastructure. 2) Security and redundancy. 3) Build and deployment tools for rapid application management and deployment. 4) Integration with other infrastructure components such as web services, databases, and LDAP. 5) Multi-tenancy, platform service that can be used by many concurrent users. 6) Logging, reporting, and code instrumentation. 7) Management interfaces and/or API. 3. Software as a Service (SAAS) Software as a Service (SAAS) is a software distribution model in which applications are hosted by a vendor or service provider and made available to customers over a network, typically the Internet. SAAS has become increasingly prevalent delivery model as underlying technologies that support Web services and service- oriented architecture (SOA) mature and new development approaches, such as Ajax, become popular. SAAS is closely related to the ASP (Application service provider) and on demand computing software delivery models. IDC identifies two slightly different delivery models for SAAS which are 1) the hosted application model and 2) the software development model. Some of the core benefits of using SAAS model are: 1) Easier administration. 2) Automatic updates and patch management. 3) Compatibility: all users will have the same version of software. 4) Easier collaboration, for the same reason. 5) Global accessibility. Unit-1 – Introduction to Cloud Technologies 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Issues of SaaS Permanent Internet connection Employees using SaaS software services must be permanently connected to the Internet. Working offline is no longer an option in this situation. We all know an Internet connection is not a problem anymore nowadays for those working in offices or home. Companies needing assurance that their employees always have a connection to their SaaS provider should consider redundant high speed Internet connections. Are you using mobile devices or travelling constantly? The best solution might be Software plus Service. Data security When it comes to migrating traditional local software applications to a cloud based platform, data security may be a problem. When a computer and application is compromised the SaaS multi-tenant application supporting many customers could be exposed to the hackers. Any provider will promise that it will do the best in order for the data to be secure in any circumstances. But just to make sure, you should ask about their infrastructure and application security. Data control Many businesses have no idea how their SaaS provider will secure their data or what backup procedures will be applied when needed. To avoid undesirable effects, before choosing a SaaS vendor, managers should research for providers with good reputations and that the vendor has backup solutions which are precisely described in the Service Level Agreement contract. Data location This means being permanently aware where exactly in the world your data is located. Although the Federal Information Security Management Act in the USA requires customers to keep sensitive data within the country, in virtualized systems, data can move dynamically from one country to another. Ask about the laws for your customers data in respect to where they are located. Cloud Deployment Models Following are the four types of Cloud Deployment Models identified by NIST. 1. Private Cloud 2. Community Cloud 3. Public Cloud 4. Hybrid Cloud Unit-1 – Introduction to Cloud Technologies 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services 1. Private Cloud Fig.: Private Cloud The cloud infrastructure is operated solely for an organization. Contrary to popular belief, private cloud may exist off premises and can be managed by a third party. Thus, two private cloud scenarios exist, as follows: On-site Private Cloud Applies to private clouds implemented at a customer’s premises. Outsourced Private Cloud Applies to private clouds where the server side is outsourced to a hosting company. Examples of Private Cloud: Eucalyptus, Ubuntu Enterprise Cloud - UEC (powered by Eucalyptus), Amazon VPC (Virtual Private Cloud), VMware Cloud Infrastructure Suite, Microsoft ECI data center etc. 2. Community Cloud Fig. Community Cloud Unit-1 – Introduction to Cloud Technologies 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services The cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns (e.g., mission, security requirements, policy, and compliance considerations). Government departments, universities, central banks etc. often find this type of cloud useful. Community cloud also has two possible scenarios: On-site Community Cloud Scenario Applies to community clouds implemented on the premises of the customers composing a community cloud. Outsourced Community Cloud Applies to community clouds where the server side is outsourced to a hosting company. Examples of Community Cloud: Google Apps for Government, Microsoft Government Community Cloud, etc. 3. Public Cloud Fig.: Public Cloud The most ubiquitous, and almost a synonym for, cloud computing. The cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services. Examples of Public Cloud: Google App Engine, Microsoft Windows Azure, IBM Smart Cloud, Amazon EC2, etc. 4. Hybrid Cloud Fig.: Hybrid Cloud The cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load-balancing between clouds). Examples of Hybrid Cloud: Windows Azure (capable of Hybrid Cloud), VMware vCloud (Hybrid Cloud Services), etc. Unit-1 – Introduction to Cloud Technologies 10 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Eucalyptus Eucalyptus is an open source software platform for implementing Infrastructure as a Service (IaaS) in a private or hybrid cloud computing environment. The Eucalyptus cloud platform pools together existing virtualized infrastructure to create cloud resources for infrastructure as a service, network as a service and storage as a service. The name Eucalyptus is an acronym for Elastic Utility Computing Architecture for Linking Your Programs to Useful Systems. Eucalyptus was founded out of a research project in the Computer Science Department at the University of California, Santa Barbara, and became a for-profit business called Eucalyptus Systems in 2009. Eucalyptus Systems announced a formal agreement with Amazon Web Services (AWS) in March 2012, allowing administrators to move instances between a Eucalyptus private cloud and the Amazon Elastic Compute Cloud (EC2) to create a hybrid cloud. The partnership also allows Eucalyptus to work with Amazon’s product teams to develop unique AWS- compatible features. Eucalyptus features Supports both Linux and Windows virtual machines (VMs). Application program interface- (API) compatible with Amazon EC2 platform. Compatible with Amazon Web Services (AWS) and Simple Storage Service (S3). Works with multiple hypervisors including VMware, Xen and KVM. Can be installed and deployed from source code or DEB and RPM packages. Internal processes communications are secured through SOAP and WS-Security. Multiple clusters can be virtualized as a single cloud. Administrative features such as user and group management and reports. Business Concerns in the Cloud Security Due to the nature of cloud computing services and how they involve storing data without knowing its precise physical location, data security remains a concern for both prospective adopters of the technology and existing users. However, the security concerns associated with storing things in the cloud are more nuanced than merely not being able to see where data is stored. A number of data breaches involving cloud systems made the headlines in 2017, including the story of financial giant Deloitte having its cloud data compromised. These combined with the natural carefulness of trusting third parties with data makes information security a persistent challenge in cloud computing. However, with each breach comes enhanced security in cloud systems designed to ensure similar breaches never happen again. Improvements include the use of multi-factor authentication, implemented to ensure users are who they claim to be. Truth be told, security for most cloud providers is watertight, and breaches in the cloud are rare—when they do occur, though, they get all the headlines. To minimize risk, double-check that your cloud provider uses secure user identity management and access controls. It’s also important to check which data security laws your cloud provider must follow. On the whole, cloud data security is as safe, if not safer, than on premise data security. Unit-1 – Introduction to Cloud Technologies 11 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Outages Performance is a consistent challenge in cloud computing, particularly for businesses that rely on cloud providers to help them run mission-critical applications. When a business moves to the cloud it becomes dependent on the cloud provider, meaning that any outages suffered by the cloud provider also affect the business. The risk of outages in the cloud is not negligible—even the major players in cloud computing are susceptible. In February 2017, an AWS Amazon S3 outage caused disruptions for many websites and applications, and even sent them offline. There is a need, therefore, for some kind of site recovery solution for data held in cloud-based services. Disaster recovery as a service (DRaaS)—the replication and hosting of servers by a third party to provide failover in the event of a man-made or natural catastrophe—is a way companies can maintain business continuity even when disaster strikes. Expertise The success of any movement towards cloud adoption comes down to the expertise at your disposal. The complexity of cloud technology and the sheer range of tools makes it difficult to keep up with the options available for all your use cases. Organizations need to strike a balance between having the right expertise and the cost of hiring dedicated cloud specialists. The optimum solution to this challenge is to work with a trusted cloud Managed Service Provider (MSP). Cloud MSPs have the manpower, tools and experience to manage multiple and complex customer environments simultaneously. The MSP takes complete responsibility for cloud processes and implementing them as the customer desires. This way, organizations can stay focused on their business goals. Cost Management All the main cloud providers have quite detailed pricing plans for their services that explicitly define costs of processing and storage data in the cloud. The problem is that cost management is often an issue when using cloud services because of the sheer range of options available. Businesses often waste money on unused workloads or unnecessarily expensive storage, and 26 percent of respondents in this cloud survey cited cost management as a major challenge in the cloud. The solution is for organizations to monitor their cloud usage in detail and constantly optimize their choice of services, instances, and storage. You can monitor and optimize cloud implementation by using a cloud cost management tool such as CloudHealth or consulting a cloud cost expert. There are also some practical cost calculators available which clarify cloud costs, including Amazon’s AWS Simple Monthly Calculator, and NetApp’s calculators for both AWS and Azure cloud storage. Governance Cloud governance, meaning the set of policies and methods used to ensure data security and privacy in the cloud, is a huge challenge. Confusion often arises about who takes responsibility for data stored in the cloud, who should be allowed use cloud resources without first consulting IT personnel, and how employees handle sensitive data. The only solution is for the IT department at your organization to adapt its existing governance and control processes to incorporate the cloud and ensure everyone is on the same page. This way, proper governance, compliance, and risk management can be enforced. Unit-1 – Introduction to Cloud Technologies 12 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Cloud Optimization Strategy Finding the right strategy for cloud adoption is another important challenge. Many businesses moved to the cloud using a segmented approach in which isolated use cases, projects, and applications were migrated to cloud providers. The problem then for many companies is a lack of any holistic organization- wide cloud strategy. Finding the right strategy for cloud adoption comes back to the issue of cloud governance. With everyone on the same page thanks to robust cloud governance and clear policies, organizations can create a unified and optimized strategy for how they use the cloud. Steps to Launch an Application with AWS Elastic Beanstalk Step 1: Create a New Application Now that you’re in the AWS Elastic Beanstalk dashboard, click on Create New Application to create and configure your application. Step 2: Configure your Application Fill out the Application name with “Your-sample-app” and Description field with “Sample App”. Click Next to continue. Step 3: Configure your Environment For this tutorial, we will be creating a web server environment for our sample PHP application. Click on Create web server. Click on Select a platform next to Predefined configuration, then select “Your Plateform”. Next, click on the drop-down menu next to Environment type, then select Single instance. Unit-1 – Introduction to Cloud Technologies 13 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Under Source, select the Upload your own option, then click Choose File to select the “Your-sample-app- v1.zip” file we downloaded earlier. Fill in the values for Environment name with “YourSampleApp-env”. For Environment URL, fill in a globally unique value since this will be your public-facing URL; we will use “YourSampleApp-env” in this tutorial, so please choose something different from this one. Lastly, fill Description with “Your Sample App”. For the Environment URL, make sure to click Check availability to make sure that the URL is not taken. Click Next to continue. Check the box next to Create this environment inside a VPC. Click Next to continue. On the Configuration Details step, you can set configuration options for the instances in your stack. Click Next. On the Environment Tags step, you can tag all the resources in your stack. Click Next. On the VPC Configuration step, select the first AZ listed by checking the box under the EC2 column. Your list of AZs may look different than the one shown as Regions can have different number of AZs. Click Next. Unit-1 – Introduction to Cloud Technologies 14 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services At the Permissions step, leave everything to their default values, then click Next to continue. Then review your environment configuration on the next screen and then click Launch to deploy your application. Step 4: Accessing your Elastic Beanstalk Application Go back to the main Elastic Beanstalk dashboard page by clicking on Elastic Beanstalk. When your application successfully launched, your application’s environment, “YourSampleApp-env”, will show up as a green box. Click on “YourSampleApp-env”, which is the green box. At the top of the page, you should see a URL field, with a value that contains the Environment URL you specified in step 3. Click on this URL field, and you should see a Congratulations page. Congratulations! You have successfully launched a sample PHP application using AWS Elastic Beanstalk. Unit-2 – Virtualization and Cloud Platforms 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Virtualization Virtualization is changing the mindset from physical to logical. Fig.: Virtualization What virtualization means is creating more logical IT resources, called virtual systems, within one physical system. That’s called system virtualization. It most commonly uses the hypervisor for managing the resources for every virtual system. The hypervisor is a software that can virtualize the hardware resources. Benefits of Virtualization More flexible and efficient allocation of resources. Enhance development productivity. It lowers the cost of IT infrastructure. Remote access and rapid scalability. High availability and disaster recovery. Pay per use of the IT infrastructure on demand. Enables running multiple operating system. Types of Virtualization 1. Application Virtualization: Application virtualization helps a user to have a remote access of an application from a server. The server stores all personal information and other characteristics of the application but can still run on a local workstation through internet. Example of this would be a user who needs to run two different versions of the same software. Technologies that use application virtualization are hosted applications and packaged applications. 2. Network Virtualization: The ability to run multiple virtual networks with each has a separate control and data plan. It co-exists together on top of one physical network. It can be managed by individual parties that potentially confidential to each other. Unit-2 – Virtualization and Cloud Platforms 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Network virtualization provides a facility to create and provision virtual networks—logical switches, routers, firewalls, load balancer, Virtual Private Network (VPN), and workload security within days or even in weeks. 3. Desktop Virtualization: Desktop virtualization allows the users’ OS to be remotely stored on a server in the data center. It allows the user to access their desktop virtually, from any location by different machine. Users who wants specific operating systems other than Windows Server will need to have a virtual desktop. Main benefits of desktop virtualization are user mobility, portability, and easy management of software installation, updates and patches. 4. Storage Virtualization: Storage virtualization is an array of servers that are managed by a virtual storage system. The servers aren’t aware of exactly where their data is stored, and instead function more like worker bees in a hive. It makes managing storage from multiple sources to be managed and utilized as a single repository. Storage virtualization software maintains smooth operations, consistent performance and a continuous suite of advanced functions despite changes, break down and differences in the underlying equipment. Full Virtualization Virtual machine simulates hardware to allow an unmodified guest OS to be run in isolation. There is two type of Full virtualizations in the enterprise market. 1. Software assisted full virtualization 2. Hardware-assisted full virtualization On both full virtualization types, guest operating system’s source information will not be modified. 1. Software Assisted – Full Virtualization (BT – Binary Translation) It completely relies on binary translation to trap and virtualize the execution of sensitive, non- virtualizable instructions sets. It emulates the hardware using the software instruction sets. Due to binary translation, it often criticized for performance issue. Here is the list of software which will fall under software assisted (BT). o VMware workstation (32Bit guests) o Virtual PC o VirtualBox (32-bit guests) o VMware Server 2. Hardware-Assisted – Full Virtualization (VT) Hardware-assisted full virtualization eliminates the binary translation and it directly interrupts with hardware using the virtualization technology which has been integrated on X86 processors since 2005 (Intel VT-x and AMD-V). Guest OS’s instructions might allow a virtual context execute privileged instructions directly on the processor, even though it is virtualized. Here is the list of enterprise software which supports hardware-assisted – Full virtualization which falls under hypervisor type 1 (Bare metal ) Unit-2 – Virtualization and Cloud Platforms 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services o VMware ESXi /ESX o KVM o Hyper-V o Xen The following virtualization type of virtualization falls under hypervisor type 2 (Hosted). o VMware Workstation (64-bit guests only ) o Virtual Box (64-bit guests only ) o VMware Server (Retired ) Paravirtualization Paravirtualization works differently from the full virtualization. It doesn’t need to simulate the hardware for the virtual machines. The hypervisor is installed on a physical server (host) and a guest OS is installed into the environment. Virtual guests aware that it has been virtualized, unlike the full virtualization (where the guest doesn’t know that it has been virtualized) to take advantage of the functions. In this virtualization method, guest source codes will be modified with sensitive information to communicate with the host. Guest Operating systems require extensions to make API calls to the hypervisor. In full virtualization, guests will issue a hardware calls but in paravirtualization, guests will directly communicate with the host (hypervisor) using the drivers. Here is the lisf of products which supports paravirtualization. o Xen o IBM LPAR o Oracle VM for SPARC (LDOM) o Oracle VM for X86 (OVM) Hybrid Virtualization (Hardware Virtualized with PV Drivers) In Hardware assisted full virtualization, Guest operating systems are unmodified and it involves many VM traps and thus high CPU overheads which limit the scalability. Paravirtualization is a complex method where guest kernel needs to be modified to inject the API. By considering these issues, engineers have come with hybrid paravirtualization. It’s a combination of both Full & Paravirtualization. The virtual machine uses paravirtualization for specific hardware drivers (where there is a bottleneck with full virtualization, especially with I/O & memory intense workloads), and the host uses full virtualization for other features. The following products support hybrid virtualization. o Oracle VM for x86 o Xen o VMware ESXi OS level Virtualization Operating system-level virtualization is widely used. It also known as “containerization”. Host Operating system kernel allows multiple user spaces also known as instance. In OS-level virtualization, unlike other virtualization technologies, there will be very little or no overhead since its uses the host operating system kernel for execution. Unit-2 – Virtualization and Cloud Platforms 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Oracle Solaris zone is one of the famous containers in the enterprise market. Here is the list of other containers. o Linux LCX o Docker o AIX WPAR Virtual computing Virtual computing refers to the use of a remote computer from a local computer where the actual computer user is located. For example, a user at a home computer could log in to a remote office computer (via the Internet or a network) to perform job tasks. Once logged in via special software, the remote computer can be used as though it were at the user's location, allowing the user to perform tasks via the keyboard, mouse, or other tools. Virtual Machine A virtual machine (VM) is an operating system (OS) or application environment that is installed on software, which reproduces dedicated hardware. The end user has the same experience on a virtual machine as they would have on dedicated hardware. Virtual Machine Conversions in VMM (Virtual Machine Migration) When you use cloud computing, you are accessing pooled resources using a technique called virtualization. Virtualization assigns a logical name for a physical resource and then provides a pointer to that physical resource when a request is made. Virtualization provides a means to manage resources efficiently because the mapping of virtual resources to physical resources can be both dynamic and facile. Virtualization is dynamic in that the mapping can be assigned based on rapidly changing conditions, and it is facile because changes to a mapping assignment can be nearly instantaneous. These are among the different types of virtualization that are characteristic of cloud computing: o Access: A client can request access to a cloud service from any location. o Application: A cloud has multiple application instances and directs requests to an instance based on conditions. o CPU: Computers can be partitioned into a set of virtual machines with each machine being assigned a workload. Alternatively, systems can be virtualized through load-balancing technologies. o Storage: Data is stored across storage devices and often replicated for redundancy. To enable these characteristics, resources must be highly configurable and flexible. You can define the features in software and hardware that enable this flexibility as conforming to one or more of the following mobility patterns: o P2V: Physical to Virtual o V2V: Virtual to Virtual o V2P: Virtual to Physical o P2P: Physical to Physical o D2C: Datacenter to Cloud o C2C: Cloud to Cloud o C2D: Cloud to Datacenter o D2D: Datacenter to Datacenter Unit-2 – Virtualization and Cloud Platforms 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Virtual Machine Types 1. General Purpose This family includes the M1 and M3 VM types. These types provide a balance of CPU, memory, and network resources, which makes them a good choice for many applications. The VM types in this family range in size from one virtual CPU with two GB of RAM to eight virtual CPUs with 30 GB of RAM. The balance of resources makes them ideal for running small and mid-size databases, more memory- hungry data processing tasks, caching fleets, and backend servers. M1 types offer smaller instance sizes with moderate CPU performance. M3 types offer larger number of virtual CPUs that provide higher performance. It is recommended to use M3 instances if you need general-purpose instances with demanding CPU requirements. 2. Compute Optimized This family includes the C1 and CC2 instance types, and is geared towards applications that benefit from high compute power. Compute-optimized VM types have a higher ratio of virtual CPUs to memory than other families but share the NCs (Node Controllers) with non-optimized ones. It is recommended to use these type if you are running any CPU-bound scale-out applications. CC2 instances provide high core count (32 virtual CPUs) and support for cluster networking. C1 instances are available in smaller sizes and are ideal for scaled-out applications at massive scale. 3. Memory Optimized This family includes the CR1 and M2 VM types and is designed for memory-intensive applications. It is recommended to use these VM types for performance-sensitive database, where your application is memory-bound. CR1 VM types provide more memory and faster CPU than do M2 types. CR1 instances also support cluster networking for bandwidth intensive applications. M2 types are available in smaller sizes, and are an excellent option for many memory-bound applications. 4. Micro This Micro family contains the T1 VM type. The T1 micro provides a small amount of consistent CPU resources and allows you to increase CPU capacity in short bursts when additional cycles are available. It is recommended to use this type of VM for lower throughput applications like a proxy server or administrative applications, or for low-traffic websites that occasionally require additional compute cycles. It is not recommended for applications that require sustained CPU performance. Load Balancing In computing, load balancing improves the distribution of workloads across multiple computing resources, such as computers, a computer cluster, network links, central processing units, or disk drives. Unit-2 – Virtualization and Cloud Platforms 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Need of load balancing in cloud computing (i) High Performing applications o Cloud load balancing techniques, unlike their traditional on premise counterparts, are less expensive and simple to implement. Enterprises can make their client applications work faster and deliver better performances, that too at potentially lower costs. (ii) Increased scalability o Cloud balancing takes help of cloud’s scalability and agility to maintain website traffic. By using efficient load balancers, you can easily match up the increased user traffic and distribute it among various servers or network devices. It is especially important for ecommerce websites, who deals with thousands of website visitors every second. During sale or other promotional offers they need such effective load balancers to distribute workloads. (iii) Ability to handle sudden traffic spikes o A normally running University site can completely go down during any result declaration. This is because too many requests can arrive at the same time. If they are using cloud load balancers, they do not need to worry about such traffic surges. No matter how large the request is, it can be wisely distributed among different servers for generating maximum results in less response time. (iv) Business continuity with complete flexibility o The basic objective of using a load balancer is to save or protect a website from sudden outages. When the workload is distributed among various servers or network units, even if one node fails the burden can be shifted to another active node. o Thus, with increased redundancy, scalability and other features load balancing easily handles website or application traffic. Network resources that can be load balanced Servers Routing mechanism Hypervisors It is the part of the private cloud that manages the virtual machines, i.e. it is the part (program) that enables multiple operating systems to share the same hardware. Each operating system could use all the hardware (processor, memory, etc.) if no other operating system is on. That is the maximum hardware available to one operating system in the cloud. Nevertheless, the hypervisor is what controls and allocates what portion of hardware resources each operating system should get, in order every one of them to get what they need and not to disrupt each other. Unit-2 – Virtualization and Cloud Platforms 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services There are two types of hypervisors Fig.: Types of Hypervisors Type 1 hypervisor: hypervisors run directly on the system hardware – A “bare metal” embedded hypervisor. Examples are: 1) VMware ESX and ESXi 2) Microsoft Hyper-V 3) Citrix XenServer 4) Oracle VM Type 2 hypervisor: hypervisors run on a host operating system that provides virtualization services, such as I/O device support and memory management. Examples are: 1) VMware Workstation/Fusion/Player 2) Microsoft Virtual PC 3) Oracle VM VirtualBox 4) Red Hat Enterprise Virtualization Machine Imaging Machine imaging is a process that is used to achieve the goal of system portability, provision, and deploy systems in the cloud through capturing the state of systems using a system image. A system image makes a copy or a clone of the entire computer system inside a single file. The image is made by using a program called system imaging program and can be used later to restore a system image. For example Amazon Machine Image (AMI) is a system image that is used in the cloud computing. The Amazon Web Services uses AMI to store copies of a virtual machine. An AMI is a file system image that contains an operating system, all device drivers, and any applications and state information that the working virtual machine would have. The AMI files are encrypted and compressed for security purpose and stored in Amazon S3 (Simple Storage System) buckets as a set of 10MB chunks. Machine imaging is mostly run on virtualization platform due to this it is also called as Virtual Appliances and running virtual machines are called instances. Because many users share clouds, the cloud helps you track information about images, such as ownership, history, and so on. The IBM SmartCloud Enterprise knows what organization you belong to when you log in. You can choose whether to keep images private, exclusively for your own use, or to share with other users in your organization. If you are an independent software vendor, you can also add your images to the public catalog. Unit-2 – Virtualization and Cloud Platforms 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Cloud Marketplace Overview A cloud marketplace is an online storefront operated by a cloud service provider. A cloud marketplace provides customers with access to software applications and services that are built on, integrate with or complement the cloud provider's offerings. A marketplace typically provides customers with native cloud applications and approved apps created by third-party developers. Applications from third-party developers not only help the cloud provider fill niche gaps in its portfolio and meet the needs of more customers, but they also provide the customer with peace of mind by knowing that all purchases from the vendor's marketplace will integrate with each other smoothly. Examples of cloud marketplaces AWS Marketplace - helps customers find, buy and use software and services that run in the Amazon Elastic Compute Cloud (EC2). Oracle Marketplace - offers a comprehensive list of apps for sales, service, marketing, talent management and human capital management. Microsoft Windows Azure Marketplace - an online market for buying and selling Software as a Service (SaaS) applications and research datasets. Salesforce.com's AppExchange - provides business apps for sales representatives and customer relationship management (CRM). Comparison of Cloud Providers Amazon Web Service Azure Rackspace Introduction Amazon Web Services (AWS) is a collection of remote computing services (also called web services) that together make up a cloud computing platform, offered over the Internet by Amazon.com. Azure is a cloud computing platform and infrastructure, created by Microsoft, for building, deploying and managing applications and services through a global network of Microsoft-managed datacenters. Rackspace is a managed cloud computing provider offering high percentage availability of applications based on RAID10. Distinguishing Features Rich set of services and integrated monitoring tools; competitive pricing model. Easy-to-use administration tool, especially for Windows admins. Easy to use control panel, especially for non-system administrators. Virtualization Xen hypervisor Microsoft Hyper-V Opensource (Xen, Kvm ) and VMware Base OS Linux (+QEMU) and Windows Windows and Linux Ubuntu Pricing model Pay-as-you-go, then subscription Pay-as-you-go Pay-as-you-go Major products Elastic block store, IP addresses, virtual private cloud, cloud watch, Cloud Front, clusters etc. Server Failover Clustering, Network Load Balancing, SNMP Services, Storage Manager for SANs, Windows Internet Name Service, Disaster Recovery to Azure, Azure Caching and Azure Redis Cache. Managed cloud, block storage, monitoring Unit-2 – Virtualization and Cloud Platforms 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Amazon Web Service Azure Rackspace CDN Features Origin-Pull, Purge, Gzip compression, Persistent connections, Caching headers, Custom CNAMEs, Control Panel & stats, Access Logs. Robust security, Lower latencies, Massively scalable, Capacity on demand. Rackspace provide CDN services through a partnership with Akamai’s service. Access interface Web-based, API, console Web interface Web-based control panel Preventive measures Moderate Basic Basic Reactive measures Moderate Basic Basic Reliability Good Average Good Scalability Good Good Good Support Good and chargeable Good Excellent Availability (%) 99.95 99.95 99.99 Server Performance (Over a period) Good Excellent and consistent Average Tools/ framework Amazon machine image (AMI), Java, PHP, Python, Ruby PHP, ASP.NET, Node.js, Python - Database RDS MySQL, MsSQL, Oracle Microsoft SQL Database MySQL Unit-3 – Introduction to AWS 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS History The AWS platform was launched in July 2002. In its early stages, the platform consisted of only a few disparate tools and services. Then in late 2003, the AWS concept was publicly reformulated when Chris Pinkham and Benjamin Black presented a paper describing a vision for Amazon's retail computing infrastructure that was completely standardized, completely automated, and would rely extensively on web services for services such as storage and would draw on internal work already underway. Near the end of their paper, they mentioned the possibility of selling access to virtual servers as a service, proposing the company could generate revenue from the new infrastructure investment. In November 2004, the first AWS service launched for public usage: Simple Queue Service (SQS). Thereafter Pinkham and lead developer Christopher Brown developed the Amazon EC2 service, with a team in Cape Town, South Africa. Amazon Web Services was officially re-launched on March 14, 2006, combining the three initial service offerings of Amazon S3 cloud storage, SQS, and EC2. The AWS platform finally provided an integrated suite of core online services, as Chris Pinkham and Benjamin Black had proposed back in 2003, as a service offered to other developers, web sites, client- side applications, and companies. Andy Jassy, AWS founder and vice president in 2006, said at the time that Amazon S3 (one of the first and most scalable elements of AWS) helps free developers from worrying about where they are going to store data, whether it will be safe and secure, if it will be available when they need it, the costs associated with server maintenance, or whether they have enough storage available. Amazon S3 enables developers to focus on innovating with data, rather than figuring out how to store it. In 2016 Jassy was promoted to CEO of the division. Reflecting the success of AWS, his annual compensation in 2017 hit nearly $36 million. In 2014, AWS launched its partner network entitled APN (AWS Partner Network) which is focused on helping AWS-based companies grow and scale the success of their business with close collaboration and best practices. To support industry-wide training and skills standardization, AWS began offering a certification program for computer engineers, on April 30, 2013, to highlight expertise in cloud computing. In January 2015, Amazon Web Services acquired Annapurna Labs, an Israel-based microelectronics company reputedly for US$350–370M. James Hamilton, an AWS engineer, wrote a retrospective article in 2016 to highlight the ten-year history of the online service from 2006 to 2016. As an early fan and outspoken proponent of the technology, he had joined the AWS engineering team in 2008. In January 2018, Amazon launched an auto scaling service on AWS. In November 2018, AWS announced customized ARM cores for use in its servers. Also in November 2018, AWS is developing ground stations to communicate with customer's satellites. AWS Infrastructure Amazon Web Services (AWS) is a global public cloud provider, and as such, it has to have a global network of infrastructure to run and manage its many growing cloud services that support customers around the world. Now we’ll take a look at the components that make up the AWS Global Infrastructure. 1) Availability Zones (AZs) Unit-3 – Introduction to AWS 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services 2) Regions 3) Edge Locations 4) Regional Edge Caches If you are deploying services on AWS, you’ll want to have a clear understanding of each of these components, how they are linked, and how you can use them within your solution to YOUR maximum benefit. Let’s take a closer look. 1) Availability Zones (AZ) AZs are essentially the physical data centers of AWS. This is where the actual compute, storage, network, and database resources are hosted that we as consumers provision within our Virtual Private Clouds (VPCs). A common misconception is that a single availability zone is equal to a single data center. This is not the case. In fact, it’s likely that multiple data centers located close together form a single availability zone. Each AZ will always have at least one other AZ that is geographically located within the same area, usually a city, linked by highly resilient and very low latency private fiber optic connections. However, each AZ will be isolated from the others using separate power and network connectivity that minimizes impact to other AZs should a single AZ fail. These low latency links between AZs are used by many AWS services to replicate data for high availability and resilience purposes. Multiple AZs within a region allows you to create highly available and resilient applications and services. By architecting your solutions to utilize resources across more than one AZ ensures that minimal or no impact will occur to your infrastructure should an AZ experience a failure, which does happen. Anyone can deploy resources in the cloud, but architecting them in a way that ensures your infrastructure remains stable, available, and resilient when faced with a disaster is a different matter. Making use of at least two AZs in a region helps you maintain high availability of your infrastructure and it’s always a recommended best practice. Fig.: Availability Zone and Region 2) Regions Region is a collection of availability zones that are geographically located close to one other. This is generally indicated by AZs within the same city. AWS has deployed them across the globe to allow its worldwide customer base to take advantage of low latency connections. Each Region will act independently of the others, and each will contain at least two Availability Zones. Example: if an organization based in London was serving customers throughout Europe, there would be no logical sense to deploy services in the Sydney Region simply due to the latency response times for its Unit-3 – Introduction to AWS 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services customers. Instead, the company would select the region most appropriate for them and their customer base, which may be the London, Frankfurt, or Ireland Region. Having global regions also allows for compliance with regulations, laws, and governance relating to data storage (at rest and in transit). Example: you may be required to keep all data within a specific location, such as Europe. Having multiple regions within this location allows an organization to meet this requirement. Similarly to how utilizing multiple AZs within a region creates a level of high availability, the same can be applied to utilizing multiple regions. You may want to use multiple regions if you are a global organization serving customers in different countries that have specific laws and governance about the use of data. In this case, you could even connect different VPCs together in different regions. The number of regions is increasing year after year as AWS works to keep up with the demand for cloud computing services. In July 2017, there are currently 16 Regions and 43 Availability Zones, with 4 Regions and 11 AZs planned. 3) Edge Locations Edge Locations are AWS sites deployed in major cities and highly populated areas across the globe. They far outnumber the number of availability zones available. While Edge Locations are not used to deploy your main infrastructures such as EC2 instances, EBS storage, VPCs, or RDS resources like AZs, they are used by AWS services such as AWS CloudFront and AWS Lambda@Edge (currently in Preview) to cache data and reduce latency for end user access by using the Edge Locations as a global Content Delivery Network (CDN). As a result, Edge Locations are primarily used by end users who are accessing and using your services. For example, you may have your website hosted on EC2 instances and S3 (your origin) within the Ohio region with a configured CloudFront distribution associated. When a user accesses your website from Europe, they would be re-directed to their closest Edge Location (in Europe) where cached data could be read on your website, significantly reducing latency. Fig.: Edge Location and Regional Edge Cache 4) Regional Edge Cache In November 2016, AWS announced a new type of Edge Location, called a Regional Edge Cache. These sit between your CloudFront Origin servers and the Edge Locations. Unit-3 – Introduction to AWS 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services A Regional Edge Cache has a larger cache-width than each of the individual Edge Locations, and because data expires from the cache at the Edge Locations, the data is retained at the Regional Edge Caches. Therefore, when data is requested at the Edge Location that is no longer available, the Edge Location can retrieve the cached data from the Regional Edge Cache instead of the Origin servers, which would have a higher latency. Pods, Aggregation, Silos Workloads support a certain no. of user when the workload reaches the limit of largest virtual machine instance possible, a copy or clone of the instance is required. A group of users within a particular instance is called a pod. Sizing limitation of pod need to be considered when building large cloud-based application. Pods are aggregated into pools within IaaS region or site called an availability zone. When the computing infrastructure isolates user clouds from one another, so that interoperating is impossible this creates an information silo, or simply silo. AWS Services This AWS services list covers the huge catalog of services offered by Amazon Web Services (AWS). These services range from the core compute products like EC2 to newer releases like AWS Deepracer for machine learning. There are currently 190 unique services provided by AWS which divided into 24 categories which are listed below: o Analytics o Application Integration o AR & VR o AWS Cost Management o Blockchain o Business Applications o Compute o Customer Engagement o Database o Developer Tools o End User Computing o Game Tech o Internet of Things o Machine Learning o Management & Governance o Media Services o Migration & Transfer o Mobile o Networking & Content Delivery o Robotics o Satellite o Security, Identity, & Compliance o Storage o Quantum Technologies Unit-3 – Introduction to AWS 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS Ecosystem In general a cloud ecosystem is a complex system of interdependent components that all work together to enable cloud services. In cloud computing, the ecosystem consists of hardware and software as well as cloud customers, cloud engineers, consultants, integrators and partners. Amazon Web Services (AWS) is the market leader in IaaS (Infrastructure-as-a-Service) and PaaS (Platform-as-a-Service) for cloud ecosystems, which can be combined to create a scalable cloud application without worrying about delays related to infrastructure provisioning (compute, storage, and network) and management. With AWS you can select the specific solutions you need, and only pay for exactly what you use, resulting in lower capital expenditure and faster time to value without sacrificing application performance or user experience. New and existing companies can build their digital infrastructure partially or entirely in the cloud with AWS, making the on premise data center a thing of the past. The AWS cloud ensures infrastructure reliability, compliance with security standards, and the ability to instantly grow or shrink your infrastructure to meet your needs and maximize your budget, all without upfront investment in equipment. Unit-4 – Programming, Management console and Storage on AWS 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Basic Understanding APIs Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud. As an API Gateway API developer, you can create APIs for use in your own client applications. Or you can make your APIs available to third-party app developers. For more information, see Who uses API Gateway? API Gateway creates RESTful APIs that: o Are HTTP-based. o Enable stateless client-server communication. o Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE. AWS Programming Interfaces API Gateway API Gateway is an AWS service that supports the following: o Creating, deploying, and managing a RESTful application programming interface (API) to expose backend HTTP endpoints, AWS Lambda functions, or other AWS services. o Creating, deploying, and managing a WebSocket API to expose AWS Lambda functions or other AWS services. o Invoking exposed API methods through the frontend HTTP and WebSocket endpoints. API Gateway REST API A collection of HTTP resources and methods that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can deploy this collection in one or more stages. Typically, API resources are organized in a resource tree according to the application logic. Each API resource can expose one or more API methods that have unique HTTP verbs supported by API Gateway. API Gateway HTTP API A collection of routes and methods that are integrated with backend HTTP endpoints or Lambda functions. You can deploy this collection in one or more stages. Each route can expose one or more API methods that have unique HTTP verbs supported by API Gateway. API Gateway WebSocket API A collection of WebSocket routes and route keys that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can deploy this collection in one or more stages. API methods are invoked through frontend WebSocket connections that you can associate with a registered custom domain name. API Deployment A point-in-time snapshot of your API Gateway API. To be available for clients to use, the deployment must be associated with one or more API stages. API Developer Your AWS account that owns an API Gateway deployment (for example, a service provider that also supports programmatic access). Unit-4 – Programming, Management console and Storage on AWS 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services API Endpoint A hostname for an API in API Gateway that is deployed to a specific Region. The hostname is of the form {api-id}.execute-api.{region}.amazonaws.com. The following types of API endpoints are supported: o Edge-optimized API endpoint  The default hostname of an API Gateway API that is deployed to the specified Region while using a CloudFront distribution to facilitate client access typically from across AWS Regions. API requests are routed to the nearest CloudFront Point of Presence (POP), which typically improves connection time for geographically diverse clients. o Private API endpoint  An API endpoint that is exposed through interface VPC endpoints and allows a client to securely access private API resources inside a VPC. Private APIs are isolated from the public internet, and they can only be accessed using VPC endpoints for API Gateway that have been granted access. o Regional API endpoint  The host name of an API that is deployed to the specified Region and intended to serve clients, such as EC2 instances, in the same AWS Region. API requests are targeted directly to the Region- specific API Gateway API without going through any CloudFront distribution. For in-Region requests, a Regional endpoint bypasses the unnecessary round trip to a CloudFront distribution. API Key An alphanumeric string that API Gateway uses to identify an app developer who uses your REST or WebSocket API. API Gateway can generate API keys on your behalf, or you can import them from a CSV file. You can use API keys together with Lambda authorizers or usage plans to control access to your APIs. WebSocket Connection API Gateway maintains a persistent connection between clients and API Gateway itself. There is no persistent connection between API Gateway and backend integrations such as Lambda functions. Backend services are invoked as needed, based on the content of messages received from clients. Web Services You can choose from a couple of different schools of thought for how web services should be delivered. The older approach, SOAP (short for Simple Object Access Protocol), had widespread industry support, complete with a comprehensive set of standards. Those standards were too comprehensive, unfortunately. The people designing SOAP set it up to be extremely flexible —it can communicate across the web, e-mail, and private networks. To ensure security and manageability, a number of supporting standards that integrate with SOAP were also defined. SOAP is based on a document encoding standard known as Extensible Markup Language (XML, for short), and the SOAP service is defined in such a way that users can then leverage XML no matter what the underlying communication network is. For this system to work, though, the data transferred by SOAP (commonly referred to as the payload) also needs to be in XML format. Notice a pattern here? The push to be comprehensive and flexible (or, to be all things to all people) plus the XML payload requirement meant that SOAP ended up being quite complex, making it a lot of work to use properly. As you might guess, many IT people found SOAP daunting and, consequently, resisted using it. Unit-4 – Programming, Management console and Storage on AWS 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services About a decade ago, a doctoral student defined another web services approach as part of his thesis: REST, or Representational State Transfer. REST, which is far less comprehensive than SOAP, aspires to solve fewer problems. It doesn’t address some aspects of SOAP that seemed important but that, in retrospect, made it more complex to use — security, for example. The most important aspect of REST is that it’s designed to integrate with standard web protocols so that REST services can be called with standard web verbs and URLs. For example, a valid REST call looks like this: http://search.examplecompany.com/CompanyDirectory/EmployeeInfo?empname=BernardGolden That’s all it takes to make a query to the REST service of examplecompany to see my personnel information. The HTTP verb that accompanies this request is GET, asking for information to be returned. To delete information, you use the verb DELETE. To insert my information, you use the verb POST. To update my information, you use the verb PUT. For the POST and PUT actions, additional information would accompany the empname and be separated by an ampersand (&) to indicate another argument to be used by the service. REST imposes no particular formatting requirements on the service payloads. In this respect, it differs from SOAP, which requires XML. For simple interactions, a string of bytes is all you need for the payload. For more complex interactions (say, in addition to returning my employee information, I want to place a request for the employee information of all employees whose names start with G), the encoding convention JSON is used. As you might expect, REST’s simpler use model, its alignment with standard web protocols and verbs, and its less restrictive payload formatting made it catch on with developers like a house on fire. AWS originally launched with SOAP support for interactions with its API, but it has steadily deprecated its SOAP interface in favor of REST. AWS URL Naming You can access your bucket using the Amazon S3 console. Using the console UI, you can perform almost all bucket operations without having to write any code. If you access a bucket programmatically, note that Amazon S3 supports RESTful architecture in which your buckets and objects are resources, each with a resource URI that uniquely identifies the resource. Amazon S3 supports both virtual-hosted–style and path-style URLs to access a bucket. In a virtual-hosted–style URL, the bucket name is part of the domain name in the URL. For example: o http://bucket.s3.amazonaws.com o http://bucket.s3-aws-region.amazonaws.com In a virtual-hosted–style URL, you can use either of these endpoints. If you make a request to the http://bucket.s3.amazonaws.com endpoint, the DNS has sufficient information to route your request directly to the Region where your bucket resides. In a path-style URL, the bucket name is not part of the domain (unless you use a Region-specific endpoint). For example: o US East (N. Virginia) Region endpoint, http://s3.amazonaws.com/bucket o Region-specific endpoint, http://s3-aws-region.amazonaws.com/bucket In a path-style URL, the endpoint you use must match the Region in which the bucket resides. Unit-4 – Programming, Management console and Storage on AWS 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services For example, if your bucket is in the South America (São Paulo) Region, you must use the http://s3-sa- east-1.amazonaws.com/bucket endpoint. If your bucket is in the US East (N. Virginia) Region, you must use the http://s3.amazonaws.com/bucket endpoint. Matching Interfaces and Services In the simplest case, the service interfaces are identical apart from name and category (inbound or outbound), that is, if the outbound service interface and all interface objects, are referenced by this service interface, are copies of the corresponding objects of an inbound service interfaces. If, however, the consumer only wants to call one operation of the inbound service interface, for example, it is not necessary to create the other inbound service interface operations in the outbound service interface as well. Simply put, the operations and corresponding outbound service interface data structures can be a subset of the operations and corresponding data structures of the inbound interface referenced. The service interface editor provides a check for the service interface pairs to determine the compatibility of the inbound and outbound service interface. This check is performed in multiple steps to determine compatibility (starting service interfaces, across operations and down to data types). The following section describes the steps to be able to estimate for a service interface assignment whether two service interfaces match. Matching Service Interfaces Two service interfaces match each other if the following conditions are fulfilled: o One service interface is of the outbound category and the other service interface if of the inbound category. Neither of the service interfaces can be abstract. o Both of the service interfaces have the same interface pattern. o There is a matching operation in the inbound service interface for each of the operations in the outbound service interface. Matching Operations An inbound service interface operation matches an outbound service interface operation (and the other way around) if the following conditions are met: o Both operations must have the name mode (asynchronous or synchronous). o Both operations must have the same Operation Pattern. o The message type for the request, which must be referenced by each operation, must have the same name and same XML Namespace. The names of the operations may differ. The same applies for the response with synchronous communication. o If the inbound service interface operation references a fault message type, the outbound service interface operation must also reference a fault message type with the same name and XML Namespace. o The data types of the message types, which the outbound service interface for the request message references (and, if necessary, for the response and fault message) must be compatible with the corresponding inbound service interface data types. Matching Data Types The check whether the corresponding data types are compatible with each other is sufficient until the comparison of the Facets of an XSD type. Unit-4 – Programming, Management console and Storage on AWS 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services The data types are compared using the same method as other objects: The structures are compatible if they contain the same fields (elements and attributes) and if these fields have compatible types, frequencies, details, and default values. There are however a few restraints, for example the target structure can contain attributes or elements that do not appear in the outbound structure, but if these are not required and where the frequency is optional or prohibited (attributes) or minOccurs=0 (elements). o The data structures compared must both be correct. For example, not all correct facets are skipped or considered in the compatibility check. o Some XSD schema language elements that can appear in a reference to an external message in the data structure are not supported. Therefore, the elements redefine and any, for example, as well as the attributes blockDefault, finalDefault, and substitutionGroup. o The comparison of structures is, for example, restricted to the following:  The details white Space and pattern are not checked  If the facet pattern is used for the outbound structure field, all the other details are not checked.  If the order of sub elements if different between the outbound and target field, a warning is displayed. Elastic Block Store Amazon Elastic Block Store is an AWS block storage system that is best used for storing persistent data. Often incorrectly referred to as Elastic Block Storage, Amazon EBS provides highly available block level storage volumes for use with Amazon Elastic Compute Cloud (EC2) instances. An EC2 instance is a virtual server in Amazon's Elastic Compute Cloud (EC2) for running applications on the Amazon Web Services (AWS) infrastructure. To begin, create an EBS volume (General Purpose, Provisioned IOPS or Magnetic), pick a size for it (up to a terabyte of data) and attach that to any one of your EC2 instances. An EBS volume can only be attached to one instance at a time but if you need to have multiple copies of the volume, you can take a snapshot and create another volume from that snapshot and attach it to another drive. A snapshot file is equivalent to a backup of whatever the EBS volume looks like at the time. For every snapshot you create, you can make an identical EC2 instance. This will allow you to publish identical content on multiple servers. Amazon EBS is ideal if you’re doing any substantial work with EC2, you want to keep data persistently on a file system, and you want to keep that data around even after you shut down your EC2 instance. EC2 instances have local storage that you can use as long as you’re running the instance, but as soon as you shut down the instance you lose the data that was on there. If you want to save anything, you need to save it on Amazon EBS. Because EC2 is like having a local drive on the machine, you can access and read the EBS volumes anytime once you attach the file to an EC2 instance. Amazon Simple Storage Service (S3) Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. Amazon S3 is intentionally built with a minimal feature set that focuses on simplicity and robustness. Following are some of advantages of the Amazon S3 service: Unit-4 – Programming, Management console and Storage on AWS 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services o Create Buckets – Create and name a bucket that stores data. Buckets are the fundamental container in Amazon S3 for data storage. o Store data in Buckets – Store an infinite amount of data in a bucket. Upload as many objects as you like into an Amazon S3 bucket. Each object can contain up to 5 TB of data. Each object is stored and retrieved using a unique developer-assigned key. o Download data – Download your data any time you like or allow others to do the same. o Permissions – Grant or deny access to others who want to upload or download data into your Amazon S3 bucket. o Standard interfaces – Use standards-based REST and SOAP interfaces designed to work with any Internet-development toolkit. Amazon S3 Application Programming Interfaces (API) The Amazon S3 architecture is designed to be programming language-neutral, using their supported interfaces to store and retrieve objects. Amazon S3 provides a REST and a SOAP interface. They are similar, but there are some differences. For example, in the REST interface, metadata is returned in HTTP headers. Because we only support HTTP requests of up to 4 KB (not including the body), the amount of metadata you can supply is restricted. The REST Interface The REST API is an HTTP interface to Amazon S3. Using REST, you use standard HTTP requests to create, fetch, and delete buckets and objects. You can use any toolkit that supports HTTP to use the REST API. You can even use a browser to fetch objects, as long as they are anonymously readable. The REST API uses the standard HTTP headers and status codes, so that standard browsers and toolkits work as expected. In some areas, they have added functionality to HTTP (for example, we added headers to support access control). The SOAP Interface SOAP support over HTTP is deprecated, but it is still available over HTTPS. New Amazon S3 features will not be supported for SOAP. The SOAP API provides a SOAP 1.1 interface using document literal encoding. The most common way to use SOAP is to download the WSDL, and use a SOAP toolkit such as Apache Axis or Microsoft.NET to create bindings, and then write code that uses the bindings to call Amazon S3. Operations we can execute through API Login into Amazon S3. Uploading. Retrieving. Deleting etc. Amazon Glacier (Now Amazon S3 Glacier) - Content Delivery Platforms Amazon Glacier is an extremely low-cost storage service that provides secure, durable, and flexible storage for data backup and archival. With Amazon Glacier, customers can reliably store their data for as little as $0.004 per gigabyte per month. Unit-4 – Programming, Management console and Storage on AWS 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Amazon Glacier enables customers to offload the administrative burdens of operating and scaling storage to AWS, so that they don’t have to worry about capacity planning, hardware provisioning, data replication, hardware failure detection and repair, or time-consuming hardware migrations. Amazon Glacier enables any business or organization to easily and cost effectively retain data for months, years, or decades. With Amazon Glacier, customers can now cost effectively retain more of their data for future analysis or reference, and they can focus on their business rather than operating and maintaining their storage infrastructure. Customers seeking compliance storage can deploy compliance controls using Vault Lock to meet regulatory and compliance archiving requirements. Benefits of Glacier Storage Service. 1. Retrievals as Quick as 1-5 Minutes Amazon Glacier provides three retrieval options to fit your use case. Expedited retrievals typically return data in 1-5 minutes, and are great for Active Archive use cases. Standard retrievals typically complete between 3-5 hours’ work, and work well for less time-sensitive needs like backup data, media editing, or long-term analytics. Bulk retrievals are the lowest-cost retrieval option, returning large amounts of data within 5-12 hours. 2. Unmatched Durability & Scalability Amazon Glacier runs on the world’s largest global cloud infrastructure, and was designed for 99.999999999% of durability. Data is automatically distributed across a minimum of three physical Availability Zones that are geographically separated within an AWS Region, and Amazon Glacier can also automatically replicate data to any other AWS Region. 3. Most Comprehensive Security & Compliance Capabilities Amazon Glacier offers sophisticated integration with AWS CloudTrail to log, monitor and retain storage API call activities for auditing, and supports three different forms of encryption. Amazon Glacier also supports security standards and compliance certifications including SEC Rule 17a-4, PCI-DSS, HIPAA/HITECH, FedRAMP, EU Data Protection Directive, and FISMA, and Amazon Glacier Vault Lock enables WORM storage capabilities, helping satisfy compliance requirements for virtually every regulatory agency around the globe. 4. Low Cost Amazon Glacier is designed to be the lowest cost AWS object storage class, allowing you to archive large amounts of data at a very low cost. This makes it feasible to retain all the data you want for use cases like data lakes, analytics, IoT, machine learning, compliance, and media asset archiving. You pay only for what you need, with no minimum commitments or up-front fees. 5. Most Supported Platform with the Largest Ecosystem In addition to integration with most AWS services, the Amazon object storage ecosystem includes tens of thousands of consulting, systems integrator and independent software vendor partners, with more joining every month. And the AWS Marketplace offers 35 categories and more than 3,500 software listings from over 1,100 ISVs that are pre-configured to deploy on the AWS Cloud. AWS Partner Network partners have adapted their services and software to work with Amazon S3 and Amazon Glacier for solutions like Backup & Recovery, Archiving, and Disaster Recovery. No other cloud provider has more partners with solutions that are pre-integrated to work with their service. Unit-4 – Programming, Management console and Storage on AWS 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services 6. Query in Place Amazon Glacier is the only cloud archive storage service that allows you to query data in place and retrieve only the subset of data you need from within an archive. Amazon Glacier Select helps you reduce the total cost of ownership by extending your data lake into cost-effective archive storage. Unit-5 – AWS identity services, security and compliance 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Identity Management and Access Management (IAM) Identity and access management (IAM) is a framework for business processes that facilitates the management of electronic or digital identities. The framework includes the organizational policies for managing digital identity as well as the technologies needed to support identity management. With IAM technologies, IT managers can control user access to critical information within their organizations. Identity and access management products offer role-based access control, which lets system administrators regulate access to systems or networks based on the roles of individual users within the enterprise. In this context, access is the ability of an individual user to perform a specific task, such as view, create or modify a file. Roles are defined according to job competency, authority and responsibility within the enterprise. Systems used for identity and access management include single sign-on systems, multifactor authentication and access management. These technologies also provide the ability to securely store identity and profile data as well as data governance functions to ensure that only data that is necessary and relevant is shared. These products can be deployed on premises, provided by a third party vendor via a cloud-based subscription model or deployed in a hybrid cloud. How Does IAM Work? The IAM workflow includes the following six elements: 1. A principal is an entity that can perform actions on an AWS resource. A user, a role or an application can be a principal. 2. Authentication is the process of confirming the identity of the principal trying to access an AWS product. The principal must provide its credentials or required keys for authentication. 3. Request: A principal sends a request to AWS specifying the action and which resource should perform it. 4. Authorization: By default, all resources are denied. IAM authorizes a request only if all parts of the request are allowed by a matching policy. After authenticating and authorizing the request, AWS approves the action. 5. Actions are used to view, create, edit or delete a resource. 6. Resources: A set of actions can be performed on a resource related to your AWS account. Identities (Users, Groups, and Roles) IAM identities, which you create to provide authentication for people and processes in your AWS account. IAM groups, which are collections of IAM users that you can manage as a unit. Identities represent the user, and can be authenticated and then authorized to perform actions in AWS. Each of these can be associated with one or more policies to determine what actions a user, role, or member of a group can do with which AWS resources and under what conditions. The AWS Account Root User When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. Unit-5 – AWS identity services, security and compliance 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. IAM Users An IAM user is an entity that you create in AWS. The IAM user represents the person or service who uses the IAM user to interact with AWS. A primary use for IAM users is to give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI. A user in AWS consists of a name, a password to sign into the AWS Management Console, and up to two access keys that can be used with the API or CLI. When you create an IAM user, you grant it permissions by making it a member of a group that has appropriate permission policies attached (recommended), or by directly attaching policies to the user. You can also clone the permissions of an existing IAM user, which automatically makes the new user a member of the same groups and attaches all the same policies. IAM Groups An IAM group is a collection of IAM users. You can use groups to specify permissions for a collection of users, which can make those permissions easier to manage for those users. For example, you could have a group called Admins and give that group the types of permissions that administrators typically need. Any user in that group automatically has the permissions that are assigned to the group. If a new user joins your organization and should have administrator privileges, you can assign the appropriate permissions by adding the user to that group. Similarly, if a person changes jobs in your organization, instead of editing that user's permissions, you can remove him or her from the old groups and add him or her to the appropriate new groups. Note that a group is not truly an identity because it cannot be identified as a Principal in a resource-based or trust policy. It is only a way to attach policies to multiple users at one time. IAM Roles An IAM role is very similar to a user, in that it is an identity with permission policies that determine what the identity can and cannot do in AWS. However, a role does not have any credentials (password or access keys) associated with it. Instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. An IAM user can assume a role to temporarily take on different permissions for a specific task. A role can be assigned to a federated user who signs in by using an external identity provider instead of IAM. AWS uses details passed by the identity provider to determine which role is mapped to the federated user. Temporary Credentials Temporary credentials are primarily used with IAM roles, but there are also other uses. You can request temporary credentials that have a more restricted set of permissions than your standard IAM user. This prevents you from accidentally performing tasks that are not permitted by the more restricted credentials. Unit-5 – AWS identity services, security and compliance 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services A benefit of temporary credentials is that they expire automatically after a set period of time. You have control over the duration that the credentials are valid. Security Policies You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies. IAM policies define permissions for an action regardless of the method that you use to perform the operation. For example, if a policy allows the GetUser action, then a user with that policy can get user information from the AWS Management Console, the AWS CLI, or the AWS API. When you create an IAM user, you can choose to allow console or programmatic access. If console access is allowed, the IAM user can sign in to the console using a user name and password. Or if programmatic access is allowed, the user can use access keys to work with the CLI or API. Policy Types Identity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity. Resource-based policies – Attach inline policies to resources. The most common examples of resource- based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. Permissions boundaries – Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity. Organizations SCPs – Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions. Access control lists (ACLs) – Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account. Session policies – Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user's identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies. Unit-5 – AWS identity services, security and compliance 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services IAM Abilities/Features Shared access to the AWS account. The main feature of IAM is that it allows you to create separate usernames and passwords for individual users or resources and delegate access. Granular permissions. Restrictions can be applied to requests. For example, you can allow the user to download information, but deny the user the ability to update information through the policies. Multifactor authentication (MFA). IAM supports MFA, in which users provide their username and password plus a one-time password from their phone—a randomly generated number used as an additional authentication factor. Identity Federation. If the user is already authenticated, such as through a Facebook or Google account, IAM can be made to trust that authentication method and then allow access based on it. This can also be used to allow users to maintain just one password for both on-premises and cloud environment work. Free to use. There is no additional charge for IAM security. There is no additional charge for creating additional users, groups or policies. PCI DSS compliance. The Payment Card Industry Data Security Standard is an information security standard for organizations that handle branded credit cards from the major card schemes. IAM complies with this standard. Password policy. The IAM password policy allows you to reset a password or rotate passwords remotely. You can also set rules, such as how a user should pick a password or how many attempts a user may make to provide a password before being denied access. IAM Limitations Names of all IAM identities and IAM resources can be alphanumeric. They can include common characters such as plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). Names of IAM identities (users, roles, and groups) must be unique within the AWS account. So you can't have two groups named DEVELOPERS and developers in your AWS account. AWS account ID aliases must be unique across AWS products in your account. It cannot be a 12 digit number. You cannot create more than 100 groups in an AWS account. You cannot create more than 5000 users in an AWS account. AWS recommends the use of temporary security credentials for adding a large number of users in an AWS account. You cannot create more than 500 roles in an AWS account. An IAM user cannot be a member of more than 10 groups. An IAM user cannot be assigned more than 2 access keys. An AWS account cannot have more than 1000 customer managed policies. You cannot attach more than 10 managed policies to each IAM entity (user, groups, or roles). You cannot store more than 20 server certificates in an AWS account. You cannot have more than 100 SAML providers in an AWS account. A policy name should not exceed 128 characters. An alias for an AWS account ID should be between 3 and 63 characters. A username and role name should not exceed 64 characters. A group name should not exceed 128 characters. AWS Physical and Environmental Security AWS data centers are state of the art, utilizing innovative architectural and engineering approaches. Unit-5 – AWS identity services, security and compliance 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Amazon has many years of experience in designing, constructing, and operating large-scale data centers. This experience has been applied to the AWS platform and infrastructure. AWS data centers are housed in facilities that are not branded as AWS facilities. Physical access is strictly controlled both at the perimeter and at building ingress points by professional security staff utilizing video surveillance, intrusion detection systems, and other electronic means. Authorized staff must pass two-factor authentication a minimum of two times to access data center floors. All visitors are required to present identification and are signed in and continually escorted by authorized staff. AWS only provides data center access and information to employees and contractors who have a legitimate business need for such privileges. When an employee no longer has a business need for these privileges, his or her access is immediately revoked, even if they continue to be an employee of Amazon or Amazon Web Services. All physical access to data centers by AWS employees is logged and audited routinely. Fire Detection and Suppression Automatic fire detection and suppression equipment has been installed to reduce risk. The fire detection system utilizes smoke detection sensors in all data center environments, mechanical and electrical infrastructure spaces, chiller rooms and generator equipment rooms. These areas are protected by either wet-pipe, double interlocked pre-action, or gaseous sprinkler systems. Power The data center electrical power systems are designed to be fully redundant and maintainable without impact to operations, 24 hours a day, and seven days a week. Uninterruptible Power Supply (UPS) units provide back-up power in the event of an electrical failure for critical and essential loads in the facility. Data centers use generators to provide back-up power for the entire facility. Climate and Temperature Climate control is required to maintain a constant operating temperature for servers and other hardware, which prevents overheating and reduces the possibility of service outages. Data centers are conditioned to maintain atmospheric conditions at optimal levels. Personnel and systems monitor and control temperature and humidity at appropriate levels. Management AWS monitors electrical, mechanical, and life support systems and equipment so that any issues are immediately identified. Preventative maintenance is performed to maintain the continued operability of equipment. Storage Device Decommissioning When a storage device has reached the end of its useful life, AWS procedures include a decommissioning process that is designed to prevent customer data from being exposed to unauthorized individuals. AWS uses the techniques detailed in NIST 800-88 (“Guidelines for Media Sanitization”) as part of the decommissioning process. Unit-5 – AWS identity services, security and compliance 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS Compliance Initiatives AWS Compliance enables customers to understand the robust controls in place at AWS to maintain security and data protection in the cloud. As systems are built on top of AWS cloud infrastructure, compliance responsibilities are shared. By tying together governance-focused, audit friendly service features with applicable compliance or audit standards, AWS Compliance enablers build on traditional programs; helping customers to establish and operate in an AWS security control environment. The IT infrastructure that AWS provides to its customers is designed and managed in alignment with security best practices and a variety of IT security standards, including: o SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70) o SOC 2 o SOC 3 o FISMA, DIACAP, and FedRAMP o DOD CSM Levels 1-5 o PCI DSS Level 1 o ISO 9001 / ISO 27001 / ISO 27017 / ISO 27018 o ITAR o FIPS 140-2 o MTCS Level 3 o HITRUST In addition, the flexibility and control that the AWS platform provides allows customers to deploy solutions that meet several industry-specific standards, including: o Criminal Justice Information Services (CJIS) o Cloud Security Alliance (CSA) o Family Educational Rights and Privacy Act (FERPA) o Health Insurance Portability and Accountability Act (HIPAA) o Motion Picture Association of America (MPAA) AWS provides a wide range of information regarding its IT control environment to customers through white papers, reports, certifications, accreditations, and other third party attestations. Understanding Public/Private Keys Amazon AWS uses keys to encrypt and decrypt login information. At the basic level, a sender uses a public key to encrypt data, which its receiver then decrypts using another private key. These two keys, public and private, are known as a key pair. You need a key pair to be able to connect to your instances. The way this works on Linux and Windows instances is different. First, when you launch a new instance, you assign a key pair to it. Then, when you log in to it, you use the private key. The difference between Linux and Windows instances is that Linux instances do not have a password already set and you must use the key pair to log in to Linux instances. On the other hand, on Windows instances, you need the key pair to decrypt the administrator password. Using the decrypted password, you can use RDP and then connect to your Windows instance. Amazon EC2 stores only the public key, and you can either generate it inside Amazon EC2 or you can import it. Unit-5 – AWS identity services, security and compliance 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Since the private key is not stored by Amazon, it’s advisable to store it in a secure place as anyone who has this private key can log in on your behalf. AWS API Security API Gateway supports multiple mechanisms of access control, including metering or tracking API uses by clients using API keys. The standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API set or individual methods. Custom authorizers and Amazon Cognito user pools provide customizable authorization and authentication solutions. A. Control Access to an API with IAM Permissions You control access to Amazon API Gateway with IAM permissions by controlling access to the following two API Gateway component processes: o To create, deploy, and manage an API in API Gateway, you must grant the API developer permissions to perform the required actions supported by the API management component of API Gateway. o To call a deployed API or to refresh the API caching, you must grant the API caller permissions to perform required IAM actions supported by the API execution component of API Gateway. B. Use API Gateway Custom Authorizers An Amazon API Gateway custom authorizer is a Lambda function that you provide to control access to your API methods. A custom authorizer uses bearer token authentication strategies, such as OAuth or SAML. It can also use information described by headers, paths, query strings, stage variables, or context variables request parameters. When a client calls your API, API Gateway verifies whether a custom authorizer is configured for the API method. If so, API Gateway calls the Lambda function. In this call, API Gateway supplies the authorization token that is extracted from a specified request header for the token-based authorizer, or passes in the incoming request parameters as the input (for example, the event parameter) to the request parameters-based authorizer function. You can implement various authorization strategies, such as JSON Web Token (JWT) verification and OAuth provider callout. You can also implement a custom scheme based on incoming request parameter values, to return IAM policies that authorize the request. If the returned policy is invalid or the permissions are denied, the API call does not succeed. C. Use Amazon Cognito User Pools In addition to using IAM roles and policies or custom authorizers, you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway. To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized. Unit-5 – AWS identity services, security and compliance 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services D. Use Client-Side SSL Certificates for Authentication by the Backend You can use API Gateway to generate an SSL certificate and use its public key in the backend to verify that HTTP requests to your backend system are from API Gateway. This allows your HTTP backend to control and accept only requests originating from Amazon API Gateway, even if the backend is publicly accessible. The SSL certificates that are generated by API Gateway are self-signed and only the public key of a certificate is visible in the API Gateway console or through the APIs. E. Create and Use API Gateway Usage Plans After you create, test, and deploy your APIs, you can use API Gateway usage plans to extend them as product offerings for your customers. You can provide usage plans to allow specified customers to access selected APIs at agreed-upon request rates and quotas that can meet their business requirements and budget constraints. AWS Security, Identity, & Compliance services Category Use cases AWS service Identity & access management Securely manage access to services and resources AWS Identity & Access Management Cloud single-sign-on (SSO) service AWS Single Sign-On Identity management for your apps Amazon Cognito Managed Microsoft Active Directory AWS Directory Service Simple, secure service to share AWS resources AWS Resource Access Manager Detective controls Unified security and compliance center AWS Security Hub Managed threat detection service Amazon GuardDuty Analyze application security Amazon Inspector Investigate potential security issues Amazon Detective Infrastructure protection DDoS protection AWS Shield Filter malicious web traffic AWS Web Application Firewall (WAF) Central management of firewall rules AWS Firewall Manager Data protection Discover and protect your sensitive data at scale Amazon Macie Key storage and management AWS Key Management Service (KMS) Hardware based key storage for regulatory compliance AWS CloudHSM Provision, manage, and deploy public and private SSL/TLS certificates AWS Certificate Manager Rotate, manage, and retrieve secrets AWS Secrets Manager Compliance No cost, self-service portal for on-demand access to AWS’ compliance reports AWS Artifact Dark Web The dark web is a general term for the seedier corners of the web, where people can interact online without worrying about the watchful eye of the authorities. Usually, these sites are guarded by encryption mechanisms such as Tor that allow users to visit them anonymously. But there are also sites that don't rely on Tor, such as password-protected forums where hackers trade secrets and stolen credit card numbers, that can also be considered part of the dark web. Unit-5 – AWS identity services, security and compliance 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services People use the dark web for a variety of purposes: buying and selling drugs, discussing hacking techniques and selling hacking services and so forth. It's important to remember that the technologies used to facilitate \"dark web\" activities aren't inherently good or bad. The same technologies used by drug dealers to hide their identity can also be used by authorized informers to securely pass information to government agencies. Unit-6 – AWS computing and marketplace 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Elastic Cloud Compute (EC2) Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment. Advantages of EC2 In less than 10 minutes you can rent a slice of Amazon’s vast cloud network and put those computing resources to work on anything from data science to bitcoin mining. EC2 offers a number of benefits and advantages over alternatives. Most notably: Affordability EC2 allows you to take advantage of Amazon’s enormous scale. You can pay a very low rate for the resources you use. The smallest EC2 instance can be rented for as little as $.0058 per hour which works out to about $4.18 per month. Of course, instances with more resources are more expensive but this gives you a sense of how affordable EC2 instances are. With EC2 instances, you’re only paying for what you use in terms of compute hours and bandwidth so there’s little wasted expense. Ease of use Amazon’s goal with EC2 was to make accessing compute resources low friction and, by and large, they’ve succeeded. Launching an instance is simply a matter of logging into the AWS Console, selecting your operating system, instance type, and storage options. At most, it’s a 10 minute process and there aren’t any major technical barriers preventing anyone from spinning up an instance, though it may take some technical knowledge to leverage those resources after launch. Scalability You can easily add EC2 instances as needed, creating your own private cloud of computer resources that perfectly matches your needs. Here at Pagely a common configuration is an EC2 instance to run a WordPress app, an instance to run RDS (a database service), and an EBS so that data can easily be moved and shared between instances as they’re added. AWS offers built-in, rules-based auto scaling so that you can automatically turn instances on or off based on demand. This helps you ensure that you’re never wasting resources but you also have enough resources available to do the job. Integration Perhaps the biggest advantage of EC2, and something no competing solution can claim, is its native integration with the vast ecosystem of AWS services. Unit-6 – AWS computing and marketplace 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Currently there are over 170 services. No other cloud network can claim the breadth, depth, and flexibility AWS can. EC2 Image Builder EC2 Image Builder simplifies the creation, maintenance, validation, sharing, and deployment of Linux or Windows Server images for use with Amazon EC2 and on-premises. Keeping server images up-to-date can be time consuming, resource intensive, and error-prone. Currently, customers either manually update and snapshot VMs or have teams that build automation scripts to maintain images. Image Builder significantly reduces the effort of keeping images up-to-date and secure by providing a simple graphical interface, built-in automation, and AWS-provided security settings. With Image Builder, there are no manual steps for updating an image nor do you have to build your own automation pipeline. Image Builder is offered at no cost, other than the cost of the underlying AWS resources used to create, store, and share the images. Auto Scaling AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple services in minutes. The service provides a simple, powerful user interface that lets you build scaling plans for resources including Amazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon DynamoDB tables and indexes, and Amazon Aurora Replicas. AWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, costs, or balance between them. If you’re already using Amazon EC2 Auto Scaling to dynamically scale your Amazon EC2 instances, you can now combine it with AWS Auto Scaling to scale additional resources for other AWS services. With AWS Auto Scaling, your applications always have the right resources at the right time. It’s easy to get started with AWS Auto Scaling using the AWS Management Console, Command Line Interface (CLI), or SDK. AWS Auto Scaling is available at no additional charge. You pay only for the AWS resources needed to run your applications and Amazon CloudWatch monitoring fees. Elastic Load Balancing Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancing offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant. o Application Load Balancers, o Network Load Balancers, and o Classic Load Balancers. Unit-6 – AWS computing and marketplace 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Application Load Balancer Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including micro services and containers. Operating at the individual request level (Layer 7), Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request. Network Load Balancer Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. Operating at the connection level (Layer 4), Network Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) and is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is also optimized to handle sudden and volatile traffic patterns. Classic Load Balancer Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. Benefits of Elastic Load Balancing for reducing workload Highly Available Elastic Load Balancing automatically distributes incoming traffic across multiple targets – Amazon EC2 instances, containers, and IP addresses – in multiple Availability Zones and ensures only healthy targets receive traffic. Elastic Load Balancing can also load balance across a Region, routing traffic to healthy targets in different Availability Zones. Secure Elastic Load Balancing works with Amazon Virtual Private Cloud (VPC) to provide robust security features, including integrated certificate management and SSL decryption. Together, they give you the flexibility to centrally manage SSL settings and offload CPU intensive workloads from your applications. Elastic Elastic Load Balancing is capable of handling rapid changes in network traffic patterns. Additionally, deep integration with Auto Scaling ensures sufficient application capacity to meet varying levels of application load without requiring manual intervention. Flexible Elastic Load Balancing also allows you to use IP addresses to route requests to application targets. This offers you flexibility in how you virtualize your application targets, allowing you to host more applications on the same instance. This also enables these applications to have individual security groups and use the same network port to further simplify inter-application communication in microservices based architecture. Robust Monitoring and Auditing Elastic Load Balancing allows you to monitor your applications and their performance in real time with Amazon CloudWatch metrics, logging, and request tracing. This improves visibility into the behavior of Unit-6 – AWS computing and marketplace 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services your applications, uncovering issues and identifying performance bottlenecks in your application stack at the granularity of an individual request. Hybrid Load Balancing Elastic Load Balancing offers ability to load balance across AWS and on-premises resources using the same load balancer. This makes it easy for you to migrate, burst, or failover on-premises applications to the cloud. AMIs An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need instances with different configurations. An AMI includes the following: o One or more EBS snapshots, or, for instance-store-backed AMIs, a template for the root volume of the instance (for example, an operating system, an application server, and applications). o Launch permissions that control which AWS accounts can use the AMI to launch instances. o A block device mapping that specifies the volumes to attach to the instance when it's launched. Using an AMI Fig.: The AMI lifecycle (create, register, launch, copy, and deregister) The following diagram summarizes the AMI lifecycle. After you create and register an AMI, you can use it to launch new instances. (You can also launch instances from an AMI if the AMI owner grants you launch permissions.) You can copy an AMI within the same Region or to different Regions. When you no longer require an AMI, you can deregister it. You can search for an AMI that meets the criteria for your instance. You can search for AMIs provided by AWS or AMIs provided by the community. After you launch an instance from an AMI, you can connect to it. When you are connected to an instance, you can use it just like you use any other server. For information about launching, connecting, and using your instance, see Amazon EC2 instances. Unit-6 – AWS computing and marketplace 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Multi Tenancy In cloud computing, multi tenancy means that multiple customers of a cloud vendor are using the same computing resources. Despite the fact that they share resources, cloud customers aren't aware of each other, and their data is kept totally separate. Multi tenancy is a crucial component of cloud computing; without it, cloud services would be far less practical. Multitenant architecture is a feature in many types of public cloud computing, including IaaS, PaaS, SaaS, containers, and server less computing. To understand multi tenancy, think of how banking works. Multiple people can store their money in one bank, and their assets are completely separate even though they're stored in the same place. Customers of the bank don't interact with each other, don't have access to other customers' money, and aren't even aware of each other. Similarly, in public cloud computing, customers of the cloud vendor use the same infrastructure – the same servers, typically – while still keeping their data and their business logic separate and secure. The classic definition of multi tenancy was a single software instance that served multiple users, or tenants. However, in modern cloud computing, the term has taken on a broader meaning, referring to shared cloud infrastructure instead of just a shared software instance. Cataloging the Marketplace AWS Marketplace is a curated digital catalog customers can use to find, buy, deploy, and manage third- party software, data, and services that customers need to build solutions and run their businesses. AWS Marketplace includes thousands of software listings from popular categories such as security, networking, storage, machine learning, business intelligence, database, and DevOps. AWS Marketplace also simplifies software licensing and procurement with flexible pricing options and multiple deployment methods. In addition, AWS Marketplace includes data products available from AWS Data Exchange. Customers can quickly launch preconfigured software with just a few clicks, and choose software solutions in Amazon Machine Images (AMIs), software as a service (SaaS), and other formats. You can browse and subscribe to data products. Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL, and get billed from one source. AWS handles billing and payments, and charges appear on customers’ AWS bill. You can use AWS Marketplace as a buyer (subscriber), seller (provider), or both. Anyone with an AWS account can use AWS Marketplace as a buyer, and can register to become a seller. A seller can be an independent software vendor (ISV), value-added reseller, or individual who has something to offer that works with AWS products and services. Every software product on AWS Marketplace has been through a curation process. On the product page, there can be one or more offerings for the product. When the seller submits a product in AWS Marketplace, they define the price of the product and the terms and conditions of use. Unit-6 – AWS computing and marketplace 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services When a consumer subscribes to a product offering, they agree to the pricing and terms and conditions set for the offer. The product can be free to use or it can have an associated charge. The charge becomes part of your AWS bill, and after you pay, AWS Marketplace pays the seller. Products can take many forms. For example, a product can be offered as an Amazon Machine Image (AMI) that is instantiated using your AWS account. The product can also be configured to use AWS CloudFormation templates for delivery to the consumer. The product can also be software as a service (SaaS) offerings from an ISV, web ACL, set of rules, or conditions for AWS WAF. Software products can be purchased at the listed price using the ISV’s standard end user license agreement (EULA) or offered with customer pricing and EULA. Products can also be purchased under a contract with specified time or usage boundaries. After the product subscriptions are in place, the consumer can copy the product to their AWS Service Catalog to manage how the product is accessed and used in the consumer’s organization. Selling On the Marketplace. As a seller, go to the AWS Marketplace Management Portal to register. If you're providing a data product or you're charging for use of your software product, you must also provide tax and banking information as part of your registration. When you register, you create a profile for your company or for yourself that is discoverable on AWS Marketplace. You also use the AWS Marketplace Management Portal to create and manage product pages for your products. Eligible partners can programmatically list AWS Marketplace products outside of AWS Marketplace. For information about becoming an eligible partner, contact your AWS Marketplace business development partner. Unit-7 – AWS networking and databases 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Virtual private clouds A virtual private cloud (VPC) is an on-demand configurable pool of shared computing resources allocated within a public cloud environment, providing a certain level of isolation between the different organizations. You already know that there are three major types of clouds: Public, Private and Hybrid. Now, there’s a newer player in the game: Virtual Private Clouds. What makes these different from public and private clouds, and what is the benefit? Is it just a fancy name for public cloud, or is it a private one? VPC are related to the public cloud, but they are not the same. Instead of sharing resources and space in a public infrastructure, you get a changeable allotment of resources to configure. There is a certain level of isolation between you and other users, via a private IP subnet and virtual communication construct (such as a VLAN) on a per user basis. This ensures a secure method of remotely accessing your cloud resources. This isolation within a public cloud lends the name “virtual private” because you are essentially operating a private cloud within a public cloud. That also doesn’t mean Virtual Private Clouds and private clouds are the same. Private clouds are entirely dedicated to your organization, and that includes the hardware. Virtual Private clouds do not have the same hardware dedication; it just creates a more secure environment on public infrastructure. Think of it as operating like a VPN: You use them to send messages over the public internet in a secure way as if you had your own personal network, but it’s not the same as actually having your own. What’s the benefit to this? Wouldn’t it just be easier to have a private cloud? Not necessarily. Private clouds are expensive to operate, and because the hardware as well as the resources required to run it belong to you alone, there is no one to share that cost with. Virtual Private Clouds give you the best of both worlds: A private cloud for security and compliance purposes, reduced infrastructure costs that come with public clouds. The allotment of resources is yours to use, so there is no worry about running out or having to share with others. You simply are sharing the infrastructure. Virtual Private Clouds are commonly used with Infrastructure as a Service (IaaS) providers. Because the shared resources (CPU, RAM, etc.) are not always the responsibility of the hardware provider, it is possible to have different infrastructure and VPC providers. However, having the same VPC and infrastructure provider can help cut down on the confusion and communication process between you and your vendor. Amazon Route 53 Announces Private DNS within Amazon VPC You can now use Amazon Route 53, AWS's highly available and scalable DNS service, to easily manage your internal domain names with the same simplicity, security, and cost effectiveness that Route 53 already provides for external DNS names. You can use the Route 53 Private DNS feature to manage authoritative DNS within your Virtual Private Clouds (VPCs), so you can use custom domain names for your internal AWS resources without exposing DNS data to the public Internet. You can use Route 53 Private DNS to manage internal DNS hostnames for resources like application servers, database servers, and web servers. Unit-7 – AWS networking and databases 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Route 53 will only respond to queries for these names when the queries originate from within the VPC(s) that you authorize. Using custom internal DNS names (rather than IP addresses or AWS-provided names such as ec2-10-1-2- 3.us-west-2.compute.amazonaws.com) has a variety of benefits, for example, being able to flip from one database to another just by changing the mapping of a domain name such as internal.example.com to point to a new IP address. Route 53 also supports split-view DNS, so you can configure public and private hosted zones to return different external and internal IP addresses for the same domain names. Relational Database Service Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. It frees you to focus on your applications so you can give them the fast performance, high availability, security and compatibility they need. Amazon RDS is available on several database instance types optimized for memory, performance or I/O and provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. You can use the AWS Database Migration Service to easily migrate or replicate your existing databases to Amazon RDS. Advantages/Benefits of Relational Database Service (i) Easy to Administer Amazon RDS makes it easy to go from project conception to deployment. Use the AWS Management Console, the AWS RDS Command-Line Interface, or simple API calls to access the capabilities of a production-ready relational database in minutes. No need for infrastructure provisioning, and no need for installing and maintaining database software. (ii) Highly Scalable We can scale our database's compute and storage resources with only a few mouse clicks or an API call, often with no downtime. Many Amazon RDS engine types allow you to launch one or more Read Replicas to offload read traffic from your primary database instance. (iii) Available and Durable Amazon RDS runs on the same highly reliable infrastructure used by other Amazon Web Services. When you provision a Multi-AZ DB Instance, Amazon RDS synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Amazon RDS has many other features that enhance reliability for critical production databases, including automated backups, database snapshots, and automatic host replacement. (iv) Fast Amazon RDS supports the most demanding database applications. You can choose between two SSD- backed storage options: one optimized for high-performance OLTP applications, and the other for cost- effective general-purpose use. In addition, Amazon Aurora provides performance on par with commercial databases at 1/10th the cost. Unit-7 – AWS networking and databases 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services (v) Secure Amazon RDS makes it easy to control network access to your database. Amazon RDS also lets you run your database instances in Amazon Virtual Private Cloud (Amazon VPC), which enables you to isolate your database instances and to connect to your existing IT infrastructure through an industry-standard encrypted IPsec VPN. Many Amazon RDS engine types offer encryption at rest and encryption in transit. (vi) Inexpensive You pay very low rates and only for the resources you actually consume. In addition, you benefit from the option of On-Demand pricing with no up-front or long-term commitments, or even lower hourly rates via Reserved Instance pricing. DynamoDB Amazon DynamoDB -- also known as Dynamo Database or DDB -- is a fully managed NoSQL database service provided by Amazon Web Services. DynamoDB is known for low latencies and scalability. According to AWS, DynamoDB makes it simple and cost-effective to store and retrieve any amount of data, as well as serve any level of request traffic. All data items are stored on solid-state drives, which provide high I/O performance and can more efficiently handle high-scale requests. An AWS user interacts with the service by using the AWS Management Console or a DynamoDB API. DynamoDB uses a NoSQL database model, which is nonrelational, allowing documents, graphs and columnar among its data models. A user stores data in DynamoDB tables, then interacts with it via GET and PUT queries, which are read and write operations, respectively. DynamoDB supports basic CRUD operations and conditional operations. Each DynamoDB query is executed by a primary key identified by the user, which uniquely identifies each item. Scalability, Availability and Durability DynamoDB enforces replication across three availability zones for high availability, durability and read consistency. A user can also opt for cross-region replication, which creates a backup copy of a DynamoDB table in one or more global geographic locations. The DynamoDB scan API provides two consistency options when reading DynamoDB data: o Eventually consistent reads o Strongly consistent reads The former, which is the AWS default setting, maximizes throughput at the potential expense of not having a read reflect the latest write or update. The latter reflects all writes and updates. There are no DynamoDB limits on data storage per user, nor a maximum throughput per table. Security Amazon DynamoDB offers Fine-Grained Access Control (FGAC) for an administrator to protect data in a table. The admin or table owner can specify who can access which items or attributes in a table and what actions that person can perform. FGAC is based on the AWS Identity and Access Management service, which manages credentials and permissions. Unit-7 – AWS networking and databases 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services As with other AWS products, the cloud provider recommends a policy of least privilege when granting access to items and attributes. An admin can view usage metrics for DynamoDB with Amazon CloudWatch. Additional DynamoDB Features The DynamoDB Triggers feature integrates with AWS Lambda to allow a developer to code actions based on updates to items in a DynamoDB table, such as sending a notification or connecting a table to another data source. The developer associates a Lambda function, which stores the logic code, with the stream on a DynamoDB table. AWS Lambda then reads updates to a table from a stream and executes the function. The DynamoDB Streams feature provides a 24-hour chronological sequence of updates to items in a table. An admin can access the stream via an API call to take action based on updates, such as synchronizing information with another data store. An admin enables DynamoDB Streams on a per-table basis. Advantages of DynamoDB Performance at scale DynamoDB supports some of the world’s largest scale applications by providing consistent, single-digit millisecond response times at any scale. You can build applications with virtually unlimited throughput and storage. DynamoDB global tables replicate your data across multiple AWS Regions to give you fast, local access to data for your globally distributed applications. For use cases that require even faster access with microsecond latency, DynamoDB Accelerator (DAX) provides a fully managed in-memory cache. No servers to manage DynamoDB is server less with no servers to provision, patch, or manage and no software to install, maintain, or operate. DynamoDB automatically scales tables up and down to adjust for capacity and maintain performance. Availability and fault tolerance are built in, eliminating the need to architect your applications for these capabilities. DynamoDB provides both provisioned and on-demand capacity modes so that you can optimize costs by specifying capacity per workload, or paying for only the resources you consume. Enterprise ready DynamoDB supports ACID transactions to enable you to build business-critical applications at scale. DynamoDB encrypts all data by default and provides fine-grained identity and access control on all your tables. You can create full backups of hundreds of terabytes of data instantly with no performance impact to your tables, and recover to any point in time in the preceding 35 days with no downtime. DynamoDB is also backed by a service level agreement for guaranteed availability. ElastiCache ElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud. Unit-7 – AWS networking and databases 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services It provides a high-performance, scalable, and cost-effective caching solution, while removing the complexity associated with deploying and managing a distributed cache environment. With ElastiCache, you can quickly deploy your cache environment, without having to provision hardware or install software. You can choose from Memcached or Redis protocol-compliant cache engine software, and let ElastiCache perform software upgrades and patch management for you. For enhanced security, ElastiCache can be run in the Amazon Virtual Private Cloud (Amazon VPC) environment, giving you complete control over network access to your clusters. With just a few clicks in the AWS Management Console, you can add or remove resources such as nodes, clusters, or read replicas to your ElastiCache environment to meet your business needs and application requirements. Existing applications that use Memcached or Redis can use ElastiCache with almost no modification. Your applications simply need to know the host names and port numbers of the ElastiCache nodes that you have deployed. The ElastiCache Auto Discovery feature for Memcached lets your applications identify all of the nodes in a cache cluster and connect to them, rather than having to maintain a list of available host names and port numbers. In this way, your applications are effectively insulated from changes to node membership in a cluster. ElastiCache has multiple features to enhance reliability for critical production deployments: o Automatic detection and recovery from cache node failures. o Multi-AZ with Automatic Failover of a failed primary cluster to a read replica in Redis clusters that support replication (called replication groups in the ElastiCache API and AWS CLI. o Flexible Availability Zone placement of nodes and clusters. o Integration with other AWS services such as Amazon EC2, Amazon CloudWatch, AWS CloudTrail, and Amazon SNS to provide a secure, high-performance, managed in-memory caching solution. ElastiCache Nodes A node is the smallest building block of an ElastiCache deployment. A node can exist in isolation from or in some relationship to other nodes. A node is a fixed-size chunk of secure, network-attached RAM. Each node runs an instance of the engine and version that was chosen when you created your cluster. If necessary, you can scale the nodes in a cluster up or down to a different instance type. Every node within a cluster is the same instance type and runs the same cache engine. Each cache node has its own Domain Name Service (DNS) name and port. Multiple types of cache nodes are supported, each with varying amounts of associated memory. You can purchase nodes on a pay-as-you-go basis, where you only pay for your use of a node. Or you can purchase reserved nodes at a much-reduced hourly rate. If your usage rate is high, purchasing reserved nodes can save you money. ElastiCache for Redis Shards A Redis shard (called a node group in the API and CLI) is a grouping of one to six related nodes. A Redis (cluster mode disabled) cluster always has one shard. A Redis (cluster mode enabled) cluster can have 1–90 shards. A multiple node shard implements replication by having one read/write primary node and 1–5 replica nodes. Unit-7 – AWS networking and databases 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services ElastiCache for Redis Clusters A Redis cluster is a logical grouping of one or more ElastiCache for Redis Shards. Data is partitioned across the shards in a Redis (cluster mode enabled) cluster. Many ElastiCache operations are targeted at clusters: o Creating a cluster o Modifying a cluster o Taking snapshots of a cluster (all versions of Redis) o Deleting a cluster o Viewing the elements in a cluster o Adding or removing cost allocation tags to and from a cluster Redshift Perhaps one of the most exciting outcomes of the public cloud was addressing the shortcomings of traditional enterprise data warehouse (EDW) storage and processing. The fast provisioning, commodity costs, infinite scale, and pay-as-you-grow pricing of public cloud are a natural fit for EDW needs, providing even the smallest of users the ability to now get valuable answers to business intelligence (BI) questions. Amazon Redshift is one such system built to address EDW needs, and it boasts low costs, an easy SQL- based access model, easy integration to other Amazon Web Services (AWS) solutions, and most importantly, high query performance. Amazon Redshift gets its name from the astronomical phenomenon noticed by Hubble, which explained the expansion of the universe. By adopting the Amazon Redshift moniker, AWS wanted to relay to customers that the service was built to handle the perpetual expansion of their data. Fig.: Amazon Redshift Architecture An Amazon Redshift cluster consists of one leader node (which clients submit queries to) and one or more follower (or “compute”) nodes, which actually perform the queries on locally stored data. By allowing for unlimited expansion of follower nodes, Amazon Redshift ensures that customers can continue to grow their cluster as their data needs grow. Unit-7 – AWS networking and databases 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Customers can start with a “cluster” as small as a single node (acting as both leader and follower), and for the smallest supported instance type (a DW2), that could be as low cost as $0.25/hour or about $180/month. By using “Reservations” (paying an up-front fee in exchange for a lower hourly running cost) for the underlying instances, Amazon Redshift can cost as little as $1,000/TB/year — upwards of one-fifth to one-tenth of the cost of a traditional EDW. Because Amazon Redshift provides native Open Database Connectivity (ODBC) and Database Connectivity (JDBC) connectivity (in addition to PostgresSQL driver support), most third-party BI tools (like Tableu, Qlikview, and MicroStrategy) work right out of the box. Amazon Redshift also uses the ubiquitous Structured Query Language (SQL) language for queries, ensuring that your current resources can quickly and easily become productive with the technology. Amazon Redshift was custom designed from the ParAccel engine — an analytic database which used columnar storage and parallel processing to achieve very fast I/O. Columns of data in Amazon Redshift are stored physically adjacent on disk, meaning that queries and scans on those columns (common in online analytical processing [OLAP] queries) run very fast. Additionally, Amazon Redshift uses 10GB Ethernet interconnects, and specialized EC2 instances (with between three and 24 spindles per node) to achieve high throughput and low latency. For even faster queries, Amazon Redshift allows customers to use column-level compression to both greatly reduce the amount of data that needs stored, and reduce the amount of disk I/O. Amazon Redshift, like many of AWS’s most popular services, is also fully managed, meaning that low-level, time-consuming administrative tasks like OS patching, backups, replacing failed hardware, and software upgrades are handled automatically and transparently. With Amazon Redshift, users simply provision a cluster, load it with their data, and begin executing queries. All data is continuously, incrementally, automatically backed up in the highly durable S3, and enabling disaster recovery across regions can be accomplished with just a few clicks. Spinning a cluster up can be as simple as a few mouse clicks, and as fast as a few minutes. A very exciting aspect of Amazon Redshift, and something that is not possible in traditional EDWs, is the ability to easily scale a provisioned cluster up and down. In Amazon Redshift, this scaling is transparent to the customer—when a resize is requested, data is copied in parallel from the source cluster (which continues to function in read-only mode) to a new cluster, and once all data is live migrated, DNS is flipped to the new cluster and the old cluster is de-provisioned. This allows customers to easily scale up and down, and each scaling event nicely re-stripes the data across the new cluster for a balanced workload. Amazon Redshift offers mature, native, and tunable security. Clusters can be deployed into a Virtual Private Cloud (VPC), and encryption of data is supported via hardware accelerated AES-256 (for data at rest) and SSL (for data on the wire). Compliance teams will be pleased to learn that users can manage their own encryption keys via AWS’s Hardware Security Module (HSM) service, and that Amazon Redshift provides a full audit trail of all SQL connection attempts, queries, and modifications of the cluster. Advantages of Amazon Redshift Exceptionally fast Redshift is very fast when it comes to loading data and querying it for analytical and reporting purposes. Redshift has Massively Parallel Processing (MPP) Architecture which allows you to load data at blazing fast speed. In addition, using this architecture, Redshift distributes and parallelize your queries across multiple nodes. Unit-7 – AWS networking and databases 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Redshift gives you an option to use Dense Compute nodes which are SSD based data warehouses. Using this you can run most complex queries in very less time. High Performance As discussed in the previous point, Redshift gains high performance using massive parallelism, efficient data compression, query optimization, and distribution. MPP enables Redshift to parallelize data loading, backup and restore operation. Furthermore, queries that you execute get distributed across multiple nodes. Redshift is a columnar storage database, which is optimized for huge and repetitive type of data. Using columnar storage, reduces the I/O operations on disk drastically, improving performance as a result. Redshift gives you an option to define column-based encoding for data compression. If not specified by the user, redshift automatically assigns compression encoding. Data compression helps in reducing memory footprint and significantly improves the I/O speed. Horizontally Scalable Scalability is a very crucial point for any Data warehousing solution and Redshift does pretty well job in that. Redshift is horizontally scalable. Whenever you need to increase the storage or need it to run faster, just add more nodes using AWS console or Cluster API and it will upscale immediately. During this process, your existing cluster will remain available for read operations so your application stays uninterrupted. During the scaling operation, Redshift moves data parallel between compute nodes of old and new clusters. Therefore enabling the transition to complete smoothly and as quickly as possible. Massive Storage capacity As expected from a Data warehousing solution, Redshift provides massive storage capacity. A basic setup can give you a petabyte range of data storage. In addition, Redshift gives you an option to choose Dense Storage type of compute nodes which can provide large storage space using Hard Disk Drives for a very low price. You can further increase the storage by adding more nodes to your cluster and it can go well beyond petabyte of data range. Attractive and transparent pricing Pricing is a very strong point in favor of Redshift, it is considerably cheaper than alternatives or an on premise solution. Redshift has 2 pricing models, pay as you go and reserved instance. Hence this gives you the flexibility to categorize this expense as an operational expense or capital expense. If your use case requires more data storage, then with 3 years reserved instance Dense Storage plan, effective price per terabyte per year can be as low as $935. Comparing this to traditional on premise storage, which roughly costs around $19k-$25k per terabyte, Redshift is significantly cheaper. SQL interface Redshift Query Engine is based on ParAccel which has the same interface as PostgreSQL If you are already familiar with SQL, you don’t need to learn a lot of new techs to start using query module of Redshift. Since Redshift uses SQL, it works with existing Postgres JDBC/ODBC drivers, readily connecting to most of the Business Intelligence tools. Unit-7 – AWS networking and databases 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS ecosystem Many businesses are running their infrastructure on AWS already, EC2 for servers, S3 for long-term storage, RDS for database and this number is constantly increasing. Redshift works very well if the rest of your infra is already on AWS and you get the benefit of data locality and cost of data transport is comparatively low. For a lot of businesses, S3 has become the de-facto destination for cloud storage. Since Redshift is virtually co-located with S3 and it can access formatted data on S3 with single COPY command. When loading or dumping data on S3, Redshift uses Massive Parallel Processing which can move data at a very fast speed. Security Amazon Redshift comes packed with various security features. There are options like VPC for network isolation, various ways to handle access control, data encryption etc. Data encryption option is available at multiple places in Redshift. To encrypt data stored in your cluster you can enable cluster encryption at the time of launching the cluster. Also, to encrypt data in transit, you can enable SSL encryption. When loading data from S3, redshift allows you to use either server-side encryption or client-side encryption. Finally, at the time of loading data, S3 or Redshift copy command handles the decryption respectively. Amazon Redshift clusters can be launched inside your infrastructure Virtual Private Cloud (VPC). Hence you can define VPC security groups to restrict inbound or outbound access to your redshift clusters. Using the robust Access Control system of AWS, you can grant privilege to specific users or maintain access on specific database level. Additionally, you can even define users and groups to have access to specific data in tables. Amazon Redshift Limitations Doesn’t enforce uniqueness There is no way in redshift to enforce uniqueness on inserted data. Hence, if you have a distributed system and it writes data on Redshift, you will have to handle the uniqueness yourself either on the application layer or by using some method of data de-duplication. Only S3, DynamoDB and Amazon EMR support for parallel upload If your data is in Amazon S3 or relational DynamoDB or on Amazon EMR, Redshift can load it using Massively Parallel Processing which is very fast. But for all other sources, parallel loading is not supported. You will either have to use JDBC inserts or some scripts to load data into Redshift. Alternatively, you can use an ETL solution like Hevo which can load your data into Redshift parallel from 100s of sources. Requires a good understanding of Sort and Distribution keys Sort keys and Distribution keys decide how data is stored and indexed across all Redshift nodes. Therefore, you need to have a solid understanding of these concepts and you need to properly set them on your tables for optimal performance. Unit-7 – AWS networking and databases 10 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services There can be only one distribution key for a table and that cannot be changed later on, which means you have to think carefully and anticipate future workloads before deciding Distribution key. Can’t be used as live app database While Redshift is very fast when running queries on a huge amount of data or running reporting and analytics, but it is not fast enough for live web apps. So you will have to pull data into a caching layer or a vanilla Postgres instance to serve redshift data to web apps. Data on Cloud Though it is a good thing for most of the people, in some use cases it could be a point of concern. So if you are concerned with the privacy of data or your data has extremely sensitive content, you may not be comfortable putting it on the cloud. High performance AWS Networking. High performance AWS Networking is nothing but use of various network services provided by AWS for better performance. AWS Networking include following services: 1. Private DNS Servers o The Private DNS are name servers that reflect your domain name rather than our default ones. o Having private nameservers could be useful if you intend to resell hosting services or want to brand your business. o Also, when using Private DNS, if a domain name is migrated to another server, there is no need to change any nameservers and the domain names will automatically point to the new location. 2. Virtual Private Clouds (Explain Earlier) 3. Cloud Models (Explain Earlier) etc. Unit-8 – Other AWS Services & Management Services 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Big Data Analytics Big data analytics is the often complex process of examining large and varied data sets, or big data, to uncover information -- such as hidden patterns, unknown correlations, market trends and customer preferences -- that can help organizations make informed business decisions. AWS Analytics Services Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena is easy to use. Simply point to your data in Amazon S3, deﬁne the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, there’s no need for complex extract, transform, and load (ETL) jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets. Athena is out-of-the-box integrated with AWS Glue Data Catalog, allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning. You can also use Glue’s fully-managed ETL capabilities to transform data or convert it into columnar formats to optimize cost and improve performance. Amazon EMR Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-eﬀective to process vast amounts of data across dynamically scalable Amazon EC2 instances. You can also run other popular distributed frameworks such as Apache Spark, HBase, Presto, and Flink in Amazon EMR, and interact with data in other AWS data stores such as Amazon S3 and Amazon DynamoDB. EMR Notebooks, based on the popular Jupyter Notebook, provide a development and collaboration environment for ad hoc querying and exploratory analysis. Amazon EMR securely and reliably handles a broad set of big data use cases, including log analysis, web indexing, data transformations (ETL), machine learning, ﬁnancial analysis, scientiﬁc simulation, and bioinformatics. Amazon CloudSearch Amazon CloudSearch is a managed service in the AWS Cloud that makes it simple and cost-effective to set up, manage, and scale a search solution for your website or application. Amazon CloudSearch supports 34 languages and popular search features such as highlighting, autocomplete, and geospatial search. Amazon Elasticsearch Service Amazon Elasticsearch Service makes it easy to deploy, secure, operate, and scale Elasticsearch to search, analyze, and visualize data in real-time. With Amazon Elasticsearch Service, you get easy-to-use APIs and real-time analytics capabilities to power use-cases such as log analytics, full-text search, application monitoring, and clickstream analytics, with enterprise-grade availability, scalability, and security. Unit-8 – Other AWS Services & Management Services 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services The service oﬀers integrations with open-source tools like Kibana and Logstash for data ingestion and visualization. It also integrates seamlessly with other AWS services such as Amazon Virtual Private Cloud (Amazon VPC), AWS Key Management Service (AWS KMS), Amazon Kinesis Data Firehose, AWS Lambda, AWS Identity and Access Management (IAM), Amazon Cognito, and Amazon CloudWatch, so that you can go from raw data to actionable insights quickly. Amazon Kinesis Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin. Amazon Kinesis currently oﬀers four services: Kinesis Data Firehose, Kinesis Data Analytics, Kinesis Data Streams, and Kinesis Video Streams. Amazon Kinesis Data Firehose o Amazon Kinesis Firehose is the easiest way to reliably load streaming data into data stores and analytics tools. o It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. o It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. o It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. o You can easily create a Firehose delivery stream from the AWS Management Console, conﬁgure it with a few clicks, and start sending data to the stream from hundreds of thousands of data sources to be loaded continuously to AWS—all in just a few minutes. o You can also configure your delivery stream to automatically convert the incoming data to columnar formats like Apache Parquet and Apache ORC, before the data is delivered to Amazon S3, for cost- effective storage and analytics. Amazon Kinesis Data Analytics o Amazon Kinesis Data Analytics is the easiest way to analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. o Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating streaming applications with other AWS services. o SQL users can easily query streaming data or build entire streaming applications using templates and an interactive SQL editor. o Java developers can quickly build sophisticated streaming applications using open source Java libraries and AWS integrations to transform and analyze data in real-time. o Amazon Kinesis Data Analytics takes care of everything required to run your queries continuously and scales automatically to match the volume and throughput rate of your incoming data. Unit-8 – Other AWS Services & Management Services 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Amazon Kinesis Data Streams o Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. o KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. o The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more. Amazon Kinesis Video Streams o Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), playback, and other processing. o Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices. o It also durably stores, encrypts, and indexes video data in your streams, and allows you to access your data through easy-to-use APIs. o Kinesis Video Streams enables you to playback video for live and on-demand viewing, and quickly build applications that take advantage of computer vision and video analytics through integration with Amazon Recognition Video, and libraries for ML frameworks such as Apache MxNet, TensorFlow, and OpenCV. Amazon Redshift Amazon Redshift is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake. Redshift delivers ten times faster performance than other data warehouses by using machine learning, massively parallel query execution, and columnar storage on high-performance disk. You can setup and deploy a new data warehouse in minutes, and run queries across petabytes of data in your Redshift data warehouse, and exabytes of data in your data lake built on Amazon S3. You can start small for just $0.25 per hour and scale to $250 per terabyte per year, less than one-tenth the cost of other solutions. Amazon QuickSight Amazon QuickSight is a fast, cloud-powered business intelligence (BI) service that makes it easy for you to deliver insights to everyone in your organization. QuickSight lets you create and publish interactive dashboards that can be accessed from browsers or mobile devices. You can embed dashboards into your applications, providing your customers with powerful self-service analytics. QuickSight easily scales to tens of thousands of users without any software to install, servers to deploy, or infrastructure to manage. AWS Data Pipeline AWS Data Pipeline is a web service that helps you reliably process and move data between diﬀerent AWS compute and storage services, as well as on-premises data sources, at speciﬁed intervals. With AWS Data Pipeline, you can regularly access your data where it’s stored, transform and process it at scale, and eﬃciently transfer the results to AWS services such as Amazon S3, Amazon RDS, Amazon DynamoDB, and Amazon EMR. Unit-8 – Other AWS Services & Management Services 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS Data Pipeline helps you easily create complex data processing workloads that are fault tolerant, repeatable, and highly available. You don’t have to worry about ensuring resource availability, managing inter-task dependencies, retrying transient failures or timeouts in individual tasks, or creating a failure notification system. AWS Data Pipeline also allows you to move and process data that was previously locked up in on-premises data silos. AWS Glue AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You can create and run an ETL job with a few clicks in the AWS Management Console. You simply point AWS Glue to your data stored on AWS, and AWS Glue discovers your data and stores the associated metadata (e.g. table definition and schema) in the AWS Glue Data Catalog. Once cataloged, your data is immediately searchable, queryable, and available for ETL. AWS Lake Formation AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. A data lake enables you to break down data silos and combine different types of analytics to gain insights and guide better business decisions. However, setting up and managing data lakes today involves a lot of manual, complicated, and time- consuming tasks. This work includes loading data from diverse sources, monitoring those data flows, setting up partitions, turning on encryption and managing keys, defining transformation jobs and monitoring their operation, re-organizing data into a columnar format, configuring access control settings, deduplicating redundant data, matching linked records, granting access to data sets, and auditing access over time. Creating a data lake with Lake Formation is as simple as defining where your data resides and what data access and security policies you want to apply. Lake Formation then collects and catalogs data from databases and object storage, moves the data into your new Amazon S3 data lake, cleans and classifies data using machine learning algorithms, and secures access to your sensitive data. Your users can then access a centralized catalog of data which describes available data sets and their appropriate usage. Your users then leverage these data sets with their choice of analytics and machine learning services, like Amazon EMR for Apache Spark, Amazon Redshift, Amazon Athena, Amazon SageMaker, and Amazon QuickSight. Amazon Managed Streaming for Kafka (MSK) Amazon Managed Streaming for Kafka (MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Apache Kafka is an open-source platform for building real-time streaming data pipelines and applications. With Amazon MSK, you can use Apache Kafka APIs to populate data lakes, stream changes to and from databases, and power machine learning and analytics applications. Apache Kafka clusters are challenging to setup, scale, and manage in production. Unit-8 – Other AWS Services & Management Services 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services When you run Apache Kafka on your own, you need to provision servers, configure Apache Kafka manually, replace servers when they fail, orchestrate server patches and upgrades, architect the cluster for high availability, ensure data is durably stored and secured, setup monitoring and alarms, and carefully plan scaling events to support load changes. Amazon Managed Streaming for Kafka makes it easy for you to build and run production applications on Apache Kafka without needing Apache Kafka infrastructure management expertise. That means you spend less time managing infrastructure and more time building applications. With a few clicks in the Amazon MSK console you can create highly available Apache Kafka clusters with settings and configuration based on Apache Kafka’s deployment best practices. Amazon MSK automatically provisions and runs your Apache Kafka clusters. Amazon MSK continuously monitors cluster health and automatically replaces unhealthy nodes with no downtime to your application. In addition, Amazon MSK secures your Apache Kafka cluster by encrypting data at rest. Application Services Tracking Software Licenses with AWS Service Catalog and AWS Step Functions Enterprises have many business requirements for tracking how software product licenses are used in their organization for financial, governance, and compliance reasons. By tracking license usage, organizations can stay within budget, track expenditures, and avoid unplanned true-up bills from their vendors’ true-up processes. The goal is to track the usage licenses as resources are deployed. In this post, you learn how to use AWS Service Catalog to deploy services and applications while tracking the licenses being consumed by end users, and how to prevent license overruns on AWS. This solution uses the following AWS services. Most of the resources are set up for you with an AWS CloudFormation stack: o AWS Service Catalog o AWS Lambda o AWS Step Functions o AWS CloudFormation o Amazon DynamoDB o Amazon SES Secure Serverless Development Using AWS Service Catalog Serverless computing allows you to build and run applications and services without having to manage servers. AWS Service Catalog allows you to create and manage catalogs of services that are approved for use on AWS. Combining Serverless and Service Catalog together is a great way to safely allow developers to create products and services in the cloud. In this post, I demonstrate how to combine the controls of Service Catalog with AWS Lambda and Amazon API Gateway and allow your developers to build a Serverless application without full AWS access. How to secure infrequently used EC2 instances with AWS Systems Manager Many organizations have predictable spikes in the usage of their applications and services. For example, retailers see large spikes in usage during Black Friday or Cyber Monday. Unit-8 – Other AWS Services & Management Services 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services The beauty of Amazon Elastic Compute Cloud (Amazon EC2) is that it allows customers to quickly scale up their compute power to meet these demands. However, some customers might require more time-consuming setup for their software running on EC2 instances. Instead of creating and terminating instances to meet demand, these customers turn off instances and then turn them on again when they are needed. Eventually the patches on those instances become out of date, and they require updates. How Cloudticity Automates Security Patches for Linux and Windows using Amazon EC2 Systems Manager and AWS Step Functions As a provider of HIPAA-compliant solutions using AWS, Cloudticity always has security as the base of everything we do. HIPAA breaches would be an end-of-life event for most of our customers. Having been born in the cloud with automation in our DNA, Cloudticity embeds automation into all levels of infrastructure management including security, monitoring, and continuous compliance. As mandated by the HIPAA Security Rule (45 CFR Part 160 and Subparts A and C of Part 164), patches at the operating system and application level are required to prevent security vulnerabilities. As a result, patches are a major component of infrastructure management. Cloudticity strives to provide consistent and reliable services to all of our customers. As such, we needed to create a custom patching solution that supports both Linux and Windows. The minimum requirements for such a solution were to read from a manifest file that contains instance names and a list of knowledge base articles (KBs) or security packages to apply to each instance. Below is a simplified, high-level process overview. Fig.: High-Level Process Overview Unit-8 – Other AWS Services & Management Services 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services There were a few guidelines to be considered when designing the solution: o Each customer has a defined maintenance window that patches can be completed within. As such, the solution must be able to perform the updates within the specified maintenance window. o The solution must be able to provide patches to one or many instances and finish within the maintenance window. o The solution should use as many AWS services as possible to reduce time-to-market and take advantage of the built-in scaling that many AWS services provide. o Code reusability is essential. Cloud Security A number of security threats are associated with cloud data services: not only traditional security threats, such as network eavesdropping, illegal invasion, and denial of service attacks, but also specific cloud computing threats, such as side channel attacks, virtualization vulnerabilities, and abuse of cloud services. The following security requirements limit the threats if we achieve that requirement than we can say our data is safe on cloud. Identity management o Every enterprise will have its own identity management system to control access to information and computing resources. o Cloud providers either integrate the customer’s identity management system into their own infrastructure, using federation or SSO technology, or a biometric-based identification system, or provide an identity management system of their own. o CloudID, for instance, provides privacy-preserving cloud-based and cross-enterprise biometric identification. o It links the confidential information of the users to their biometrics and stores it in an encrypted fashion. o Making use of a searchable encryption technique, biometric identification is performed in encrypted domain to make sure that the cloud provider or potential attackers do not gain access to any sensitive data or even the contents of the individual queries. Physical security o Cloud service providers physically secure the IT hardware (servers, routers, cables etc.) against unauthorized access, interference, theft, fires, floods etc. and ensure that essential supplies (such as electricity) are sufficiently robust to minimize the possibility of disruption. o This is normally achieved by serving cloud applications from 'world-class' (i.e. professionally specified, designed, constructed, managed, monitored and maintained) data centers. Personnel security o Various information security concerns relating to the IT and other professionals associated with cloud services are typically handled through pre-, para- and post-employment activities such as security screening potential recruits, security awareness and training programs, proactive. Privacy o Providers ensure that all critical data (credit card numbers, for example) are masked or encrypted and that only authorized users have access to data in its entirety. Moreover, digital identities and credentials must be protected as should any data that the provider collects or produces about customer activity in the cloud. Confidentiality o Data confidentiality is the property that data contents are not made available or disclosed to illegal users. Unit-8 – Other AWS Services & Management Services 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services o Outsourced data is stored in a cloud and out of the owners' direct control. Only authorized users can access the sensitive data while others, including CSPs, should not gain any information of the data. o Meanwhile, data owners expect to fully utilize cloud data services, e.g., data search, data computation, and data sharing, without the leakage of the data contents to CSPs or other adversaries. Access controllability o Access controllability means that a data owner can perform the selective restriction of access to her or his data outsourced to cloud. o Legal users can be authorized by the owner to access the data, while others cannot access it without permissions. o Further, it is desirable to enforce fine-grained access control to the outsourced data, i.e., different users should be granted different access privileges with regard to different data pieces. o The access authorization must be controlled only by the owner in untrusted cloud environments. Integrity o Data integrity demands maintaining and assuring the accuracy and completeness of data. o A data owner always expects that her or his data in a cloud can be stored correctly and trustworthily. o It means that the data should not be illegally tampered, improperly modified, deliberately deleted, or maliciously fabricated. o If any undesirable operations corrupt or delete the data, the owner should be able to detect the corruption or loss. o Further, when a portion of the outsourced data is corrupted or lost, it can still be retrieved by the data users. CloudWatch Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources. Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application running smoothly. CloudFormation AWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This file serves as the single source of truth for your cloud environment. AWS CloudFormation is available at no additional charge, and you pay only for the AWS resources needed to run your applications. Unit-8 – Other AWS Services & Management Services 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Advantage of Cloud Formation Model it all AWS CloudFormation allows you to model your entire infrastructure in a text file. This template becomes the single source of truth for your infrastructure. This helps you to standardize infrastructure components used across your organization, enabling configuration compliance and faster troubleshooting. Automate and deploy AWS CloudFormation provisions your resources in a safe, repeatable manner, allowing you to build and rebuild your infrastructure and applications, without having to perform manual actions or write custom scripts. CloudFormation takes care of determining the right operations to perform when managing your stack, and rolls back changes automatically if errors are detected. It's just code Codifying your infrastructure allows you to treat your infrastructure as just code. You can author it with any code editor, check it into a version control system, and review the files with team members before deploying into production. CloudTrail AWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. CloudTrail is enabled on your AWS account when you create it. When activity occurs in your AWS account, that activity is recorded in a CloudTrail event. You can easily view recent events in the CloudTrail console by going to Event history. For an ongoing record of activity and events in your AWS account, create a trail. Visibility into your AWS account activity is a key aspect of security and operational best practices. You can use CloudTrail to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. You can identify who or what took which action, what resources were acted upon, when the event occurred, and other details to help you analyze and respond to activity in your AWS account. Optionally, you can enable AWS CloudTrail Insights on a trail to help you identify and respond to unusual activity. You can integrate CloudTrail into applications using the API, automate trail creation for your organization, check the status of trails you create, and control how users view CloudTrail events. Working of CloudTrail You can create two types of trails for an AWS account: A trail that applies to all regions o When you create a trail that applies to all regions, CloudTrail records events in each region and delivers the CloudTrail event log files to an S3 bucket that you specify. o If a region is added after you create a trail that applies to all regions that new region is automatically included, and events in that region are logged. o This is the default option when you create a trail in the CloudTrail console. Unit-8 – Other AWS Services & Management Services 10 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services A trail that applies to one region o When you create a trail that applies to one region, CloudTrail records the events in that region only. o It then delivers the CloudTrail event log files to an Amazon S3 bucket that you specify. o If you create additional single trails, you can have those trails deliver CloudTrail event log files to the same Amazon S3 bucket or to separate buckets. o This is the default option when you create a trail using the AWS CLI or the CloudTrail API. Beginning on April 12, 2019, trails will be viewable only in the AWS Regions where they log events. If you create a trail that logs events in all AWS Regions, it will appear in the console in all AWS Regions. If you create a trail that only logs events in a single AWS Region, you can view and manage it only in that AWS Region. If you have created an organization in AWS Organizations, you can also create a trail that will log all events for all AWS accounts in that organization. This is referred to as an organization trail. Organization trails can apply to all AWS Regions or one Region. Organization trails must be created in the master account, and when specified as applying to an organization, are automatically applied to all member accounts in the organization. Member accounts will be able to see the organization trail, but cannot modify or delete it. By default, member accounts will not have access to the log files for the organization trail in the Amazon S3 bucket. You can change the configuration of a trail after you create it, including whether it logs events in one region or all regions. You can also change whether it logs data or CloudTrail Insights events. Changing whether a trail logs events in one region or in all regions affects which events are logged. By default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE). You can also choose to encrypt your log files with an AWS Key Management Service (AWS KMS) key. You can store your log files in your bucket for as long as you want. You can also define Amazon S3 lifecycle rules to archive or delete log files automatically. If you want notifications about log file delivery and validation, you can set up Amazon SNS notifications. CloudTrail typically delivers log files within 15 minutes of account activity. In addition, CloudTrail publishes log files multiple times an hour, about every five minutes. These log files contain API calls from services in the account that support CloudTrail. Benefits of CloudTrail Simplified compliance With AWS CloudTrail, simplify your compliance audits by automatically recording and storing event logs for actions made within your AWS account. Integration with Amazon CloudWatch Logs provides a convenient way to search through log data, identify out-of-compliance events, accelerate incident investigations, and expedite responses to auditor requests. Security analysis and troubleshooting With AWS CloudTrail, you can discover and troubleshoot security and operational issues by capturing a comprehensive history of changes that occurred in your AWS account within a specified period of time. Visibility into user and resource activity AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. Unit-8 – Other AWS Services & Management Services 11 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred. Security automation AWS CloudTrail allows you track and automatically respond to account activity threatening the security of your AWS resources. With Amazon CloudWatch Events integration, you can define workflows that execute when events that can result in security vulnerabilities are detected. For example, you can create a workflow to add a specific policy to an Amazon S3 bucket when CloudTrail logs an API call that makes that bucket public. OpsWorks AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. OpsWorks has three offerings, AWS Opsworks for Chef Automate, AWS OpsWorks for Puppet Enterprise, and AWS OpsWorks Stacks. AWS OpsWorks for Chef Automate AWS OpsWorks for Chef Automate is a fully managed configuration management service that hosts Chef Automate, a suite of automation tools from Chef for configuration management, compliance and security, and continuous deployment. OpsWorks also maintains your Chef server by automatically patching, updating, and backing up your server. OpsWorks eliminates the need to operate your own configuration management systems or worry about maintaining its infrastructure. OpsWorks gives you access to all of the Chef Automate features, such as configuration and compliance management, which you manage through the Chef Console or command line tools like Knife. It also works seamlessly with your existing Chef cookbooks. Choose AWS OpsWorks for Chef Automate if you are an existing Chef user. AWS OpsWorks for Puppet Enterprise AWS OpsWorks for Puppet Enterprise is a fully managed configuration management service that hosts Puppet Enterprise, a set of automation tools from Puppet for infrastructure and application management. OpsWorks also maintains your Puppet master server by automatically patching, updating, and backing up your server. OpsWorks eliminates the need to operate your own configuration management systems or worry about maintaining its infrastructure. OpsWorks gives you access to all of the Puppet Enterprise features, which you manage through the Puppet console. It also works seamlessly with your existing Puppet code. Choose AWS OpsWorks for Puppet Enterprise if you are an existing Puppet user. Unit-8 – Other AWS Services & Management Services 12 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS OpsWorks Stacks AWS OpsWorks Stacks is an application and server management service. With OpsWorks Stacks, you can model your application as a stack containing different layers, such as load balancing, database, and application server. Within each layer, you can provision Amazon EC2 instances, enable automatic scaling, and configure your instances with Chef recipes using Chef Solo. This allows you to automate tasks such as installing packages and programming languages or frameworks, configuring software, and more. Choose AWS OpsWorks Stacks if you need a solution for application modeling and management. OpenID Connect (OIDC) IAM OIDC identity providers are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Salesforce. You use an IAM OIDC identity provider when you want to establish trust between an OIDC-compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign-in code or manage your own user identities. You can create and manage an IAM OIDC identity provider using the AWS Management Console, the AWS Command Line Interface, the Tools for Windows PowerShell, or the IAM API. When you create an OpenID Connect (OIDC) identity provider in IAM, you must supply a thumbprint. IAM requires the thumbprint for the root certificate authority (CA) that signed the certificate used by the external identity provider (IdP). The thumbprint is a signature for the CA's certificate that was used to issue the certificate for the OIDC- compatible IdP. When you create an IAM OIDC identity provider, you are trusting identities authenticated by that IdP to have access to your AWS account. By supplying the CA's certificate thumbprint, you trust any certificate issued by that CA with the same DNS name as the one registered. This eliminates the need to update trusts in each account when you renew the IdP's signing certificate. You can create an IAM OIDC identity provider with the AWS Command Line Interface, the Tools for Windows PowerShell, or the IAM API. When you use these methods, you must obtain the thumbprint manually and supply it to AWS. When you create an OIDC identity provider with the IAM console, the console attempts to fetch the thumbprint for you. We recommend that you also obtain the thumbprint for your OIDC IdP manually and verify that the console fetched the correct thumbprint. Unit-9 – AWS Billing & Dealing with Disaster 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Managing Costs, Utilization and Tracking The cloud allows you to trade capital expenses (such as data centers and physical servers) for variable expenses, and only pay for IT as you consume it. And, because of the economies of scale, the variable expenses are much lower than what you would pay to do it yourself. Whether you were born in the cloud, or you are just starting your migration journey to the cloud, AWS has a set of solutions to help you manage and optimize your spend. During this unprecedented time, many businesses and organizations are facing disruption to their operations, budgets, and revenue. AWS has a set of solutions to help you with cost management and optimization. This includes services, tools, and resources to organize and track cost and usage data, enhance control through consolidated billing and access permission, enable better planning through budgeting and forecasts, and further lower cost with resources and pricing optimizations. AWS Cost Management Solutions Organize and Report Cost and Usage Based on User-Defined Methods You need complete, near real-time visibility of your cost and usage information to make informed decisions. AWS equips you with tools to organize your resources based on your needs, visualize and analyze cost and usage data in a single pane of glass, and accurately chargeback to appropriate entities (e.g. department, project, and product). Rather than centrally policing the cost, you can provide real-time cost data that makes sense to your engineering, application, and business teams. The detailed, allocable cost data allows teams to have the visibility and details to be accountable of their own spend. Billing with Built-in Control Business and organization leaders need a simple and easy way to access AWS billing information, including a spend summary, a breakdown of all service costs incurred by accounts across the organization, along with discounts and credits. Customer can choose to consolidate your bills and take advantage of higher volume discounts based on aggregated usage across your bills. Leaders also need to set appropriate guardrails in place so you can maintain control over cost, governance, and security. AWS helps organizations balance freedom and control by enabling the governance of granular user permission. Improved Planning with Flexible Forecasting and Budgeting Businesses and organizations need to plan and set expectations around cloud costs for your projects, applications, and more. The emergence of the cloud allowed teams to acquire and deprecate resources on an ongoing basis, without relying on teams to approve, procure and install infrastructure. However, this flexibility requires organizations to adapt to the new, dynamic forecasting and budgeting process. Unit-9 – AWS Billing & Dealing with Disaster 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS provides forecasts based on your cost and usage history and allows you to set budget threshold and alerts, so you can stay informed whenever cost and usage is forecasted to, or exceeds the threshold limit. You can also set reservation utilization and/or coverage targets for your Reserved Instances and Savings Plans and monitor how they are progressing towards your target. Optimize Costs with Resource and Pricing Recommendations With AWS, customers can take control of your cost and continuously optimize your spend. There are a variety of AWS pricing models and resources you can choose from to meet requirements for both performance and cost efficiency, and adjust as needed. When evaluating AWS services for your architectural and business needs, you will have the flexibility to choose from a variety of elements, such as operating systems, instance types, availability zones, and purchase options. AWS offers resources optimization recommendations to simplify the evaluation process so you can efficiently select the cost-optimized resources. We also provide recommendations around pricing models (up to 72% with Reserved Instances and Savings Plans and up to 90% with Spot Instances) based on your utilization patterns, so you can further drive down your cost without compromising workload performance. Monitor, Track, and Analyze Your AWS Costs & Usage Appropriate management, tracking and measurement are fundamental in achieving the full benefits of cost optimization. Amazon CloudWatch Amazon CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS and on- premises servers. AWS Trusted Advisor AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices. AWS Cost Explorer AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. Bottom Line Impact As AWS provide large range of service and we can utilize it for our business on pay as you go basis so it will save our cost and time. Due to that company can reduce their cost and increase revenue by focusing on core work and other service management is done by cloud providers. It will create bottom line impact for organization. Geographic Concerns The AWS Global Cloud Infrastructure is the most secure, extensive, and reliable cloud platform, offering over 175 fully featured services from data centers globally. Unit-9 – AWS Billing & Dealing with Disaster 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Whether you need to deploy your application workloads across the globe in a single click, or you want to build and deploy specific applications closer to your end-users with single-digit millisecond latency, AWS provides you the cloud infrastructure where and when you need it. With millions of active customers and tens of thousands of partners globally, AWS has the largest and most dynamic ecosystem. Customers across virtually every industry and of every size, including start-ups, enterprises, and public sector organizations, are running every imaginable use case on AWS. Failure plans / Disaster Recovery (DR) Our data is the most precious asset that we have and protecting it is our top priority. Creating backups of our data to an off shore data center, so that in the event of an on premise failure we can switch over to our backup, is a prime focus for business continuity. As AWS says, ‘Disaster recovery is a continual process of analysis and improvement, as business and systems evolve. For each business service, customers need to establish an acceptable recovery point and time, and then build an appropriate DR solution.’ Backup and DR on Cloud reduces costs by half as compared to maintaining your own redundant data centers. And if you think about it, it’s really not that surprising. Imagine the kind of cost you would entail in buying and maintaining servers and data centers, providing secure and stable connectivity and not to mention keeping them secure. You would also be underutilizing severs; and in times of unpredictable traffic rise it would be strenuous to set up new ones. To all these cloud provides a seamless transition reducing cost dramatically. 4 Standard Approaches of Backup and Disaster Recovery Using Amazon Cloud 1. Backup and Recovery To recover your data in the event of any disaster, you must first have your data periodically backed up from your system to AWS. Backing up of data can be done through various mechanisms and your choice will be based on the RPO (Recovery Point Objective- So if your disaster struck at 2 pm and your RPO is 1 hr, your Backup & DR will restore all data till 1 pm.) that will suit your business needs. AWS offers AWS Direct connect and Import Export services that allow for faster backup. For example, if you have a frequently changing database like say a stock market, then you will need a very high RPO. However if your data is mostly static with a low frequency of changes, you can opt for periodic incremental backup. Once your backup mechanisms are activated you can pre-configure AMIs (operating systems & application software). Now when a disaster strikes, EC2 (Elastic Compute Capacity) instances in the Cloud using EBS (Elastic Block Store) coupled with AMIs can access your data from the S3 (Simple Storage Service) buckets to revive your system and keep it going. 2. Pilot Light Approach The name pilot light comes from the gas heater analogy. Just as in a heater you have a small flame that is always on, and can quickly ignite the entire furnace; a similar approach can be thought of about your data system. In the preparatory phase your on premise database server mirrors data to data volumes on AWS. The database server on cloud is always activated for frequent or continuous incremental backup. Unit-9 – AWS Billing & Dealing with Disaster 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services This core area is the pilot from our gas heater analogy. The application and caching server replica environments are created on cloud and kept in standby mode as very few changes take place over time. These AMIs can be updated periodically. This is the entire furnace from our example. If the on premise system fails, then the application and caching servers get activated; further users are rerouted using elastic IP addresses to the ad hoc environment on cloud. Your Recovery takes just a few minutes. 3. Warm Standby Approach This Technique is the next level of the pilot light, reducing recovery time to almost zero. Your application and caching servers are set up and always activated based on your business critical activities but only a minimum sized fleet of EC2 instances are dedicated. The backup system is not capable of handling production load, but can be used for testing, quality assurance and other internal uses. In the event of a disaster, when your on premise data center fails, two things happen. Firstly multiple EC2 instances are dedicated (vertical and horizontal scaling) to bring your application and caching environment up to production load. ELB and Auto Scaling (for distributing traffic) are used to ease scaling up. Secondly using Amazon Route 53 user traffic is rerouted instantly using elastic IP addresses and there is instant recovery of your system with almost zero down time. 4. Multi-Site Approach Well this is the optimum technique in backup and DR and is the next step after warm standby. All activities in the preparatory stage are similar to a warm standby; except that AWS backup on Cloud is also used to handle some portions of the user traffic using Route 53. When a disaster strikes, the rest of the traffic that was pointing to the on premise servers are rerouted to AWS and using auto scaling techniques multiple EC2 instances are deployed to handle full production capacity. You can further increase the availability of your multi-site solution by designing Multi-AZ architectures. Examining Logs It is necessary to examine the log files in order to locate an error code or other indication of the issue that your cluster experienced. It may take some investigative work to determine what happened. Hadoop runs the work of the jobs in task attempts on various nodes in the cluster. Amazon EMR can initiate speculative task attempts, terminating the other task attempts that do not complete first. This generates significant activity that is logged to the controller, stderr and syslog log files as it happens. In addition, multiple tasks attempts are running simultaneously, but a log file can only display results linearly. Start by checking the bootstrap action logs for errors or unexpected configuration changes during the launch of the cluster. From there, look in the step logs to identify Hadoop jobs launched as part of a step with errors. Examine the Hadoop job logs to identify the failed task attempts. The task attempt log will contain details about what caused a task attempt to fail. References 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Book 1. Cloud Computing Bible, Barrie Sosinsky, John Wiley & Sons, ISBN-13: 978-0470903568. 2. Mastering AWS Security, Albert Anthony, Packt Publishing Ltd., ISBN 978-1-78829-372-3. 3. Amazon Web Services for Dummies, Bernard Golden, For Dummies, ISBN-13: 978- 1118571835. Websites 1. www.aws.amazon.com 2. www.docs.aws.amazon.com 3. www.bluepiit.com 4. www.inforisktoday.com 5. www.techno-pulse.com 6. www.exelanz.com 7. www.ibm.com 8. www.iarjset.com/upload/2017/july-17/IARJSET%2018.pdf 9. www.searchservervirtualization.techtarget.com 10. www.docs.eucalyptus.com 11. www.cloudacademy.com 12. www.searchaws.techtarget.com 13. www.searchsecurity.techtarget.com 14. www.en.wikipedia.org/wiki/Cloud_computing_security 15. www.znetlive.com 16. www.en.wikipedia.org/wiki/Virtual_private_cloud 17. www.resource.onlinetech.com 18. www.globalknowledge.com 19. www.blog.blazeclan.com/4-approaches-backup-disaster-recovery-explained-amazon-cloud 20. www.zdnet.com/article/what-is-cloud-computing-everything-you-need-to-know-about-the-cloud 21. www.javatpoint.com/introduction-to-cloud-computing 22. www.javatpoint.com/history-of-cloud-computing 23. www.allcloud.io/blog/6-cloud-computing-concerns-facing-2018 24. www.searchitchannel.techtarget.com/definition/cloud-marketplace 25. www.en.wikipedia.org/wiki/Amazon_Web_Services 26. www.msystechnologies.com/blog/cloud-orchestration-everything-you-want-to-know 27. www.linuxacademy.com/blog/linux-academy/elasticity-cloud-computing 28. www.searchitchannel.techtarget.com/definition/Eucalyptus 29. www.geeksforgeeks.org/virtualization-cloud-computing-types 30. www.cloudsearch.blogspot.com 31. www.simplilearn.com/tutorials/aws-tutorial/aws-iam 32. www.d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf 33. www.resources.intenseschool.com/amazon-aws-understanding-ec2-key-pairs-and-how-they-are-used- for-windows-and-linux-instances/ 34. www.pagely.com/blog/amazon-ec2/ 35. www.cloudflare.com/learning/cloud/what-is-multitenancy/ 36. www.hevodata.com/blog/amazon-redshift-pros-and-cons/\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text_data):\n",
        "    text_data = re.sub(r'[\\uf0b7●•▪▶]', '', text_data)\n",
        "    text_data = re.sub(r'\\s+', ' ', text_data).strip()\n",
        "    text_data = re.sub(r'\\s+([.,!?;:])', r'\\1', text_data)\n",
        "\n",
        "    return text_data\n",
        "\n",
        "cleaned_text = clean_text(text_data)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX92i45DdMSi"
      },
      "source": [
        "# **Split Data into chunks :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VGPuRu1dQXK",
        "outputId": "8cf65fdf-1690-499c-b18b-ee428ed7bc7d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Unit-1 – Introduction to Cloud Technologies 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Cloud Computing Cloud computing is the delivery of on-demand computing services, from applications to storage and processing power, typically over the internet and on a'),\n",
              " Document(metadata={}, page_content='typically over the internet and on a pay-as-you-go basis. Introduction to Cloud Computing Fig. Cloud Computing Cloud Computing is the delivery of computing services such as servers, storage, databases, networking, software, analytics, intelligence, and more, over the Cloud (Internet). Cloud'),\n",
              " Document(metadata={}, page_content='more, over the Cloud (Internet). Cloud Computing provides an alternative to the on-premises datacenter. With an on-premises datacenter, we have to manage everything, such as purchasing and installing hardware, virtualization, installing the operating system, and any other required applications,'),\n",
              " Document(metadata={}, page_content='and any other required applications, setting up the network, configuring the firewall, and setting up storage for data. After doing all the set-up, we become responsible for maintaining it through its entire lifecycle. But if we choose Cloud Computing, a cloud vendor is responsible for the hardware'),\n",
              " Document(metadata={}, page_content='vendor is responsible for the hardware purchase and maintenance. They also provide a wide variety of software and platform as a service. We can take any required services on rent. The cloud computing services will be charged based on usage. The cloud environment provides an easily accessible online'),\n",
              " Document(metadata={}, page_content='provides an easily accessible online portal that makes handy for the user to manage the compute, storage, network, and application resources. Characteristics (Features) of Cloud Computing The five essential characteristics of cloud computing: 1. On-demand self-service: A consumer can separately'),\n",
              " Document(metadata={}, page_content='self-service: A consumer can separately provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider. 2. Broad network access: Capabilities are available over the network and accessed through'),\n",
              " Document(metadata={}, page_content=\"over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops and workstations). 3. Resource pooling: The provider's computing resources are pooled to serve multiple consumers using a multi-tenant\"),\n",
              " Document(metadata={}, page_content='multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned Unit-1 – Introduction to Cloud Technologies 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services according to consumer demand. There is a'),\n",
              " Document(metadata={}, page_content='to consumer demand. There is a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state or datacenter). Examples of resources'),\n",
              " Document(metadata={}, page_content='or datacenter). Examples of resources include storage, processing, memory and network bandwidth. 4. Rapid elasticity: Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward matching with demand. To the consumer, the capabilities'),\n",
              " Document(metadata={}, page_content='To the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time. 5. Measured service: Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction'),\n",
              " Document(metadata={}, page_content='capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth and active user accounts). Resource usage can be monitored, controlled and reported, providing transparency for the provider and consumer. Advantages of Cloud Computing Cost: It reduces'),\n",
              " Document(metadata={}, page_content='of Cloud Computing Cost: It reduces the huge capital costs of buying hardware and software. Speed: Resources can be accessed in minutes, typically within a few clicks. Scalability: We can increase or decrease the requirement of resources according to the business requirements. Productivity: While'),\n",
              " Document(metadata={}, page_content='requirements. Productivity: While using cloud computing, we put less operational effort. We do not need to apply patching, as well as no need to maintain hardware and software. So, in this way, the IT team can be more productive and focus on achieving business goals. Reliability: Backup and'),\n",
              " Document(metadata={}, page_content='business goals. Reliability: Backup and recovery of data are less expensive and very fast for business continuity. Security: Many cloud vendors offer a broad set of policies, technologies, and controls that strengthen our data security. Disadvantages of Cloud Computing Requires good speed internet'),\n",
              " Document(metadata={}, page_content='Computing Requires good speed internet with good bandwidth: To access your cloud services, you need to have a good internet connection always with good bandwidth to upload or download files to/from the cloud Downtime: Since the cloud requires high internet speed and good bandwidth, there is always'),\n",
              " Document(metadata={}, page_content='and good bandwidth, there is always a possibility of service outage, which can result in business downtime. Today, no business can afford revenue or business loss due to downtime or slow down from an interruption in critical business processes. Limited control of infrastructure: Since you are not'),\n",
              " Document(metadata={}, page_content='of infrastructure: Since you are not the owner of the infrastructure of the cloud, hence you don’t have any control or have limited access to the cloud infra. Restricted or limited flexibility: The cloud provides a huge list of services, but consuming them comes with a lot of restrictions and'),\n",
              " Document(metadata={}, page_content='comes with a lot of restrictions and limited flexibility for your applications or developments. Also, platform dependency or ‘vendor lock-in’ can sometimes make it difficult for you to migrate from one provider to another. Ongoing costs: Although you save your cost of spending on whole'),\n",
              " Document(metadata={}, page_content='you save your cost of spending on whole infrastructure and its management, on the cloud, you need to keep paying for services as long as you use them. But in traditional methods, you only need to invest once. Security: Security of data is a big concern for everyone. Since the public cloud utilizes'),\n",
              " Document(metadata={}, page_content='Since the public cloud utilizes the internet, your data may become vulnerable. In the case of a public cloud, it depends on the cloud provider to take care of your data. So, before opting for cloud services, it is required that you find a provider who follows maximum compliance policies for data'),\n",
              " Document(metadata={}, page_content='maximum compliance policies for data security. Unit-1 – Introduction to Cloud Technologies 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Vendor Lock-in: Although the cloud service providers assure you that they will allow you to switch or migrate to any other'),\n",
              " Document(metadata={}, page_content='you to switch or migrate to any other service provider whenever you want, it is a very difficult process. You will find it complex to migrate all the cloud services from one service provider to another. During migration, you might end up facing compatibility, interoperability and support issues. To'),\n",
              " Document(metadata={}, page_content='interoperability and support issues. To avoid these issues, many customers choose not to change the vendor. Technical issues: Even if you are a tech whiz, the technical issues can occur, and everything can’t be resolved in-house. To avoid interruptions, you will need to contact your service'),\n",
              " Document(metadata={}, page_content='you will need to contact your service provider for support. However, not every vendor provides 24/7 support to their clients. Difference between Conventional Computing and Cloud Computing Conventional Computing Cloud Computing In conventional computing environment more time is needed for'),\n",
              " Document(metadata={}, page_content='environment more time is needed for installation, set up, and configuration. Once the cloud computing environment is set up initially, you can gain access faster than conventional computing Cost must be paid in advance Pay-as-you-go Cost is fixed Cost is variable Economic to scale for all'),\n",
              " Document(metadata={}, page_content='is variable Economic to scale for all organization Economic to scale for large organization only For Scaling manual effort is needed Scaling can be elastic and automatic Environment is mix of physical and virtualized Usually environment is virtualized History of Cloud Computing Before emerging the'),\n",
              " Document(metadata={}, page_content='of Cloud Computing Before emerging the cloud computing, there was Client/Server computing which is basically a centralized storage in which all the software applications, all the data and all the controls are resided on the server side. If a single user wants to access specific data or run a'),\n",
              " Document(metadata={}, page_content='wants to access specific data or run a program, he/she need to connect to the server and then gain appropriate access, and then he/she can do his/her business. Then after, distributed computing came into picture, where all the computers are networked together and share their resources when needed.'),\n",
              " Document(metadata={}, page_content='and share their resources when needed. On the basis of above computing, there was emerged of cloud computing concepts that later implemented. At around in 1961, John MacCharty suggested in a speech at MIT that computing can be sold like a utility, just like a water or electricity. It was a'),\n",
              " Document(metadata={}, page_content='like a water or electricity. It was a brilliant idea, but like all brilliant ideas, it was ahead of its time, as for the next few decades, despite interest in the model, the technology simply was not ready for it. But of course time has passed and the technology caught that idea and after few years'),\n",
              " Document(metadata={}, page_content='caught that idea and after few years we mentioned that: o In 1999, Salesforce.com started delivering of applications to users using a simple website. The applications were delivered to enterprises over the Internet, and this way the dream of computing sold as utility were true. o In 2002, Amazon'),\n",
              " Document(metadata={}, page_content='as utility were true. o In 2002, Amazon started Amazon Web Services, providing services like storage, computation and even human intelligence. However, only starting with the launch of the Elastic Compute Cloud in 2006 a truly commercial service open to everybody existed. o In 2009, Google Apps'),\n",
              " Document(metadata={}, page_content='existed. o In 2009, Google Apps also started to provide cloud computing enterprise applications. o Of course, all the big players are present in the cloud computing evolution, some were earlier and some were later. In 2009, Microsoft launched Windows Azure, and companies like Oracle and HP have all'),\n",
              " Document(metadata={}, page_content='companies like Oracle and HP have all joined the game. This proves that today, cloud computing has become mainstream. Unit-1 – Introduction to Cloud Technologies 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Cloud Orchestration Cloud Orchestration is a way to'),\n",
              " Document(metadata={}, page_content='Cloud Orchestration is a way to manage, co-ordinate, and provision all the components of a cloud platform automatically from a common interface. It orchestrates the physical as well as virtual resources of the cloud platform. Cloud orchestration is a must because cloud services scale up arbitrarily'),\n",
              " Document(metadata={}, page_content='cloud services scale up arbitrarily and dynamically, include fulfillment assurance and billing, and require workflows in various business and technical domains. Orchestration tools combine automated tasks by interconnecting the processes running across the heterogeneous platforms in multiple'),\n",
              " Document(metadata={}, page_content='the heterogeneous platforms in multiple locations. Orchestration tools create declarative templates to convert the interconnected processes into a single workflow. The processes are so orchestrated that the new environment creation workflow is achieved with a single API call. Creation of these'),\n",
              " Document(metadata={}, page_content='a single API call. Creation of these declarative templates, though complex and time consuming, is simplified by the orchestration tools. Cloud orchestration includes two types of models: o Single Cloud model o Multi-cloud model In Single cloud model, all the applications designed for a system run'),\n",
              " Document(metadata={}, page_content='applications designed for a system run on the same IaaS platform (same cloud service provider). Applications, interconnected to create a single workflow, running on various cloud platforms for the same organization define the concept of multi-cloud model. IaaS requirement for some applications,'),\n",
              " Document(metadata={}, page_content='IaaS requirement for some applications, though designed for same system, might vary. This results in availing services of multiple cloud service providers. For example, application with patient’s sensitive medical data might reside in some IaaS, whereas the application for online OPD appointment'),\n",
              " Document(metadata={}, page_content='application for online OPD appointment booking might reside in another IaaS, but they are interconnected to form one system. This is called multi-cloud orchestration. Multi-cloud models provide high redundancy as compared to single IaaS deployments. This reduces the risk of down time. Elasticity in'),\n",
              " Document(metadata={}, page_content='the risk of down time. Elasticity in Cloud Elasticity covers the ability to scale up but also the ability to scale down. The idea is that you can quickly provision new infrastructure to handle a high load of traffic. But what happens after that rush? If you leave all of these new instances running,'),\n",
              " Document(metadata={}, page_content='all of these new instances running, your bill will skyrocket as you will be paying for unused resources. In the worst case scenario, these resources can even cancel out revenue from the sudden rush. An elastic system prevents this from happening. After a scaled up period, your infrastructure can'),\n",
              " Document(metadata={}, page_content='up period, your infrastructure can scale back down, meaning you will only be paying for your usual resource usage and some extra for the high traffic period. The key is that this all happens automatically. When resource needs meet a certain threshold (usually measured by traffic), the system'),\n",
              " Document(metadata={}, page_content='measured by traffic), the system “knows” that it needs to de-provision a certain amount of infrastructure, and does so. With a couple hours of training, anyone can use the AWS web console to manually add or subtract instances. But it takes a true Solutions Architect to set up monitoring, account'),\n",
              " Document(metadata={}, page_content='Architect to set up monitoring, account for provisioning time, and configure a system for maximum elasticity. Unit-1 – Introduction to Cloud Technologies 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Cloud Service Options / Cloud Service Models / Cloud'),\n",
              " Document(metadata={}, page_content='Options / Cloud Service Models / Cloud Computing Stack Fig.: Cloud Services Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be'),\n",
              " Document(metadata={}, page_content='applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. Although cloud computing has evolved over the time it has been majorly divided into three broad service categories: 1. Infrastructure as a Service(IAAS), 2.'),\n",
              " Document(metadata={}, page_content='Infrastructure as a Service(IAAS), 2. Platform as a Service (PAAS) and 3. Software as a Service (SAAS) 1. Infrastructure as a Service (IAAS) Infrastructure as a Service (IAAS) is a form of cloud computing that provides virtualized computing resources over the internet. In an IAAS model, a third'),\n",
              " Document(metadata={}, page_content='the internet. In an IAAS model, a third party provider hosts hardware, software, servers, storage and other infrastructure components on the behalf of its users. IAAS providers also host users’ applications and handle tasks including system maintenance backup and resiliency planning. IAAS platforms'),\n",
              " Document(metadata={}, page_content='and resiliency planning. IAAS platforms offer highly scalable resources that can be adjusted on-demand which makes it a well- suited for workloads that are temporary, experimental or change unexpectedly. Other characteristics of IAAS environments include the automation of administrative tasks,'),\n",
              " Document(metadata={}, page_content='the automation of administrative tasks, dynamic scaling, desktop virtualization and policy based services. Technically, the IaaS market has a relatively low barrier of entry, but it may require substantial financial investment in order to build and support the cloud infrastructure. Mature'),\n",
              " Document(metadata={}, page_content='the cloud infrastructure. Mature open-source cloud management frameworks like OpenStack are available to everyone, and provide strong a software foundation for companies that want to build their private cloud or become a public cloud provider. IAAS- Network: There are two major network services'),\n",
              " Document(metadata={}, page_content='There are two major network services offered by public cloud service providers: 1. load balancing and 2. DNS (domain name systems). Unit-1 – Introduction to Cloud Technologies 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Load balancing provides a single'),\n",
              " Document(metadata={}, page_content='Load balancing provides a single point of access to multiple servers that run behind it. A load balancer is a network device that distributes network traffic among servers using specific load balancing algorithms. DNS is a hierarchical naming system for computers, or any other naming devices that'),\n",
              " Document(metadata={}, page_content='or any other naming devices that use IP addressing for network identification – a DNS system associates domain names with IP addresses. 2. Platform as a Service (PAAS) Platform as a Service (PAAS) is a cloud computing model that delivers applications over the internet. In a PAAS model, a cloud'),\n",
              " Document(metadata={}, page_content='the internet. In a PAAS model, a cloud provider delivers hardware and software tools, usually those needed for application development, to its users as a service. A PAAS provider hosts the hardware and software on its own infrastructure. As a result, PAAS frees users from having to install in-house'),\n",
              " Document(metadata={}, page_content=\"users from having to install in-house hardware and software to develop or run a new application. PAAS doesn’t replace a business' entire infrastructure but instead a business relies on PAAS providers for key services, such as Java development or application hosting. A PAAS provider, however,\"),\n",
              " Document(metadata={}, page_content='hosting. A PAAS provider, however, supports all the underlying computing and software, users only need to login and start using the platform-usually through a Web browser interface. PAAS providers then charge for that access on a per-use basis or on monthly basis. Some of the main characteristics'),\n",
              " Document(metadata={}, page_content='basis. Some of the main characteristics of PAAS are: 1) Scalability and auto-provisioning of the underlying infrastructure. 2) Security and redundancy. 3) Build and deployment tools for rapid application management and deployment. 4) Integration with other infrastructure components such as web'),\n",
              " Document(metadata={}, page_content='infrastructure components such as web services, databases, and LDAP. 5) Multi-tenancy, platform service that can be used by many concurrent users. 6) Logging, reporting, and code instrumentation. 7) Management interfaces and/or API. 3. Software as a Service (SAAS) Software as a Service (SAAS) is a'),\n",
              " Document(metadata={}, page_content='Software as a Service (SAAS) is a software distribution model in which applications are hosted by a vendor or service provider and made available to customers over a network, typically the Internet. SAAS has become increasingly prevalent delivery model as underlying technologies that support Web'),\n",
              " Document(metadata={}, page_content='technologies that support Web services and service- oriented architecture (SOA) mature and new development approaches, such as Ajax, become popular. SAAS is closely related to the ASP (Application service provider) and on demand computing software delivery models. IDC identifies two slightly'),\n",
              " Document(metadata={}, page_content='models. IDC identifies two slightly different delivery models for SAAS which are 1) the hosted application model and 2) the software development model. Some of the core benefits of using SAAS model are: 1) Easier administration. 2) Automatic updates and patch management. 3) Compatibility: all users'),\n",
              " Document(metadata={}, page_content='management. 3) Compatibility: all users will have the same version of software. 4) Easier collaboration, for the same reason. 5) Global accessibility. Unit-1 – Introduction to Cloud Technologies 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Issues of SaaS'),\n",
              " Document(metadata={}, page_content='and Services Issues of SaaS Permanent Internet connection Employees using SaaS software services must be permanently connected to the Internet. Working offline is no longer an option in this situation. We all know an Internet connection is not a problem anymore nowadays for those working in offices'),\n",
              " Document(metadata={}, page_content='nowadays for those working in offices or home. Companies needing assurance that their employees always have a connection to their SaaS provider should consider redundant high speed Internet connections. Are you using mobile devices or travelling constantly? The best solution might be Software plus'),\n",
              " Document(metadata={}, page_content='best solution might be Software plus Service. Data security When it comes to migrating traditional local software applications to a cloud based platform, data security may be a problem. When a computer and application is compromised the SaaS multi-tenant application supporting many customers could'),\n",
              " Document(metadata={}, page_content='supporting many customers could be exposed to the hackers. Any provider will promise that it will do the best in order for the data to be secure in any circumstances. But just to make sure, you should ask about their infrastructure and application security. Data control Many businesses have no idea'),\n",
              " Document(metadata={}, page_content='control Many businesses have no idea how their SaaS provider will secure their data or what backup procedures will be applied when needed. To avoid undesirable effects, before choosing a SaaS vendor, managers should research for providers with good reputations and that the vendor has backup'),\n",
              " Document(metadata={}, page_content='and that the vendor has backup solutions which are precisely described in the Service Level Agreement contract. Data location This means being permanently aware where exactly in the world your data is located. Although the Federal Information Security Management Act in the USA requires customers to'),\n",
              " Document(metadata={}, page_content='Act in the USA requires customers to keep sensitive data within the country, in virtualized systems, data can move dynamically from one country to another. Ask about the laws for your customers data in respect to where they are located. Cloud Deployment Models Following are the four types of Cloud'),\n",
              " Document(metadata={}, page_content='Following are the four types of Cloud Deployment Models identified by NIST. 1. Private Cloud 2. Community Cloud 3. Public Cloud 4. Hybrid Cloud Unit-1 – Introduction to Cloud Technologies 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services 1. Private Cloud Fig.:'),\n",
              " Document(metadata={}, page_content='and Services 1. Private Cloud Fig.: Private Cloud The cloud infrastructure is operated solely for an organization. Contrary to popular belief, private cloud may exist off premises and can be managed by a third party. Thus, two private cloud scenarios exist, as follows: On-site Private Cloud Applies'),\n",
              " Document(metadata={}, page_content='follows: On-site Private Cloud Applies to private clouds implemented at a customer’s premises. Outsourced Private Cloud Applies to private clouds where the server side is outsourced to a hosting company. Examples of Private Cloud: Eucalyptus, Ubuntu Enterprise Cloud - UEC (powered by Eucalyptus),'),\n",
              " Document(metadata={}, page_content='Cloud - UEC (powered by Eucalyptus), Amazon VPC (Virtual Private Cloud), VMware Cloud Infrastructure Suite, Microsoft ECI data center etc. 2. Community Cloud Fig. Community Cloud Unit-1 – Introduction to Cloud Technologies 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and'),\n",
              " Document(metadata={}, page_content='| 2180712 – Cloud Infrastructure and Services The cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns (e.g., mission, security requirements, policy, and compliance considerations). Government departments, universities, central banks'),\n",
              " Document(metadata={}, page_content='universities, central banks etc. often find this type of cloud useful. Community cloud also has two possible scenarios: On-site Community Cloud Scenario Applies to community clouds implemented on the premises of the customers composing a community cloud. Outsourced Community Cloud Applies to'),\n",
              " Document(metadata={}, page_content='Outsourced Community Cloud Applies to community clouds where the server side is outsourced to a hosting company. Examples of Community Cloud: Google Apps for Government, Microsoft Government Community Cloud, etc. 3. Public Cloud Fig.: Public Cloud The most ubiquitous, and almost a synonym for,'),\n",
              " Document(metadata={}, page_content='ubiquitous, and almost a synonym for, cloud computing. The cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services. Examples of Public Cloud: Google App Engine, Microsoft Windows Azure, IBM Smart Cloud, Amazon'),\n",
              " Document(metadata={}, page_content='Windows Azure, IBM Smart Cloud, Amazon EC2, etc. 4. Hybrid Cloud Fig.: Hybrid Cloud The cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and'),\n",
              " Document(metadata={}, page_content='technology that enables data and application portability (e.g., cloud bursting for load-balancing between clouds). Examples of Hybrid Cloud: Windows Azure (capable of Hybrid Cloud), VMware vCloud (Hybrid Cloud Services), etc. Unit-1 – Introduction to Cloud Technologies 10 Prof. Vijay M. Shekhat, CE'),\n",
              " Document(metadata={}, page_content='10 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Eucalyptus Eucalyptus is an open source software platform for implementing Infrastructure as a Service (IaaS) in a private or hybrid cloud computing environment. The Eucalyptus cloud platform pools together'),\n",
              " Document(metadata={}, page_content='cloud platform pools together existing virtualized infrastructure to create cloud resources for infrastructure as a service, network as a service and storage as a service. The name Eucalyptus is an acronym for Elastic Utility Computing Architecture for Linking Your Programs to Useful Systems.'),\n",
              " Document(metadata={}, page_content='Your Programs to Useful Systems. Eucalyptus was founded out of a research project in the Computer Science Department at the University of California, Santa Barbara, and became a for-profit business called Eucalyptus Systems in 2009. Eucalyptus Systems announced a formal agreement with Amazon Web'),\n",
              " Document(metadata={}, page_content='a formal agreement with Amazon Web Services (AWS) in March 2012, allowing administrators to move instances between a Eucalyptus private cloud and the Amazon Elastic Compute Cloud (EC2) to create a hybrid cloud. The partnership also allows Eucalyptus to work with Amazon’s product teams to develop'),\n",
              " Document(metadata={}, page_content='with Amazon’s product teams to develop unique AWS- compatible features. Eucalyptus features Supports both Linux and Windows virtual machines (VMs). Application program interface- (API) compatible with Amazon EC2 platform. Compatible with Amazon Web Services (AWS) and Simple Storage Service (S3).'),\n",
              " Document(metadata={}, page_content='(AWS) and Simple Storage Service (S3). Works with multiple hypervisors including VMware, Xen and KVM. Can be installed and deployed from source code or DEB and RPM packages. Internal processes communications are secured through SOAP and WS-Security. Multiple clusters can be virtualized as a single'),\n",
              " Document(metadata={}, page_content='clusters can be virtualized as a single cloud. Administrative features such as user and group management and reports. Business Concerns in the Cloud Security Due to the nature of cloud computing services and how they involve storing data without knowing its precise physical location, data security'),\n",
              " Document(metadata={}, page_content='physical location, data security remains a concern for both prospective adopters of the technology and existing users. However, the security concerns associated with storing things in the cloud are more nuanced than merely not being able to see where data is stored. A number of data breaches'),\n",
              " Document(metadata={}, page_content='is stored. A number of data breaches involving cloud systems made the headlines in 2017, including the story of financial giant Deloitte having its cloud data compromised. These combined with the natural carefulness of trusting third parties with data makes information security a persistent'),\n",
              " Document(metadata={}, page_content='makes information security a persistent challenge in cloud computing. However, with each breach comes enhanced security in cloud systems designed to ensure similar breaches never happen again. Improvements include the use of multi-factor authentication, implemented to ensure users are who they'),\n",
              " Document(metadata={}, page_content='to ensure users are who they claim to be. Truth be told, security for most cloud providers is watertight, and breaches in the cloud are rare—when they do occur, though, they get all the headlines. To minimize risk, double-check that your cloud provider uses secure user identity management and'),\n",
              " Document(metadata={}, page_content='secure user identity management and access controls. It’s also important to check which data security laws your cloud provider must follow. On the whole, cloud data security is as safe, if not safer, than on premise data security. Unit-1 – Introduction to Cloud Technologies 11 Prof. Vijay M.'),\n",
              " Document(metadata={}, page_content='to Cloud Technologies 11 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Outages Performance is a consistent challenge in cloud computing, particularly for businesses that rely on cloud providers to help them run mission-critical applications. When a business'),\n",
              " Document(metadata={}, page_content='applications. When a business moves to the cloud it becomes dependent on the cloud provider, meaning that any outages suffered by the cloud provider also affect the business. The risk of outages in the cloud is not negligible—even the major players in cloud computing are susceptible. In February'),\n",
              " Document(metadata={}, page_content='computing are susceptible. In February 2017, an AWS Amazon S3 outage caused disruptions for many websites and applications, and even sent them offline. There is a need, therefore, for some kind of site recovery solution for data held in cloud-based services. Disaster recovery as a service'),\n",
              " Document(metadata={}, page_content='Disaster recovery as a service (DRaaS)—the replication and hosting of servers by a third party to provide failover in the event of a man-made or natural catastrophe—is a way companies can maintain business continuity even when disaster strikes. Expertise The success of any movement towards cloud'),\n",
              " Document(metadata={}, page_content='success of any movement towards cloud adoption comes down to the expertise at your disposal. The complexity of cloud technology and the sheer range of tools makes it difficult to keep up with the options available for all your use cases. Organizations need to strike a balance between having the'),\n",
              " Document(metadata={}, page_content='to strike a balance between having the right expertise and the cost of hiring dedicated cloud specialists. The optimum solution to this challenge is to work with a trusted cloud Managed Service Provider (MSP). Cloud MSPs have the manpower, tools and experience to manage multiple and complex'),\n",
              " Document(metadata={}, page_content='to manage multiple and complex customer environments simultaneously. The MSP takes complete responsibility for cloud processes and implementing them as the customer desires. This way, organizations can stay focused on their business goals. Cost Management All the main cloud providers have quite'),\n",
              " Document(metadata={}, page_content='All the main cloud providers have quite detailed pricing plans for their services that explicitly define costs of processing and storage data in the cloud. The problem is that cost management is often an issue when using cloud services because of the sheer range of options available. Businesses'),\n",
              " Document(metadata={}, page_content='range of options available. Businesses often waste money on unused workloads or unnecessarily expensive storage, and 26 percent of respondents in this cloud survey cited cost management as a major challenge in the cloud. The solution is for organizations to monitor their cloud usage in detail and'),\n",
              " Document(metadata={}, page_content='monitor their cloud usage in detail and constantly optimize their choice of services, instances, and storage. You can monitor and optimize cloud implementation by using a cloud cost management tool such as CloudHealth or consulting a cloud cost expert. There are also some practical cost calculators'),\n",
              " Document(metadata={}, page_content='also some practical cost calculators available which clarify cloud costs, including Amazon’s AWS Simple Monthly Calculator, and NetApp’s calculators for both AWS and Azure cloud storage. Governance Cloud governance, meaning the set of policies and methods used to ensure data security and privacy in'),\n",
              " Document(metadata={}, page_content='to ensure data security and privacy in the cloud, is a huge challenge. Confusion often arises about who takes responsibility for data stored in the cloud, who should be allowed use cloud resources without first consulting IT personnel, and how employees handle sensitive data. The only solution is'),\n",
              " Document(metadata={}, page_content='sensitive data. The only solution is for the IT department at your organization to adapt its existing governance and control processes to incorporate the cloud and ensure everyone is on the same page. This way, proper governance, compliance, and risk management can be enforced. Unit-1 –'),\n",
              " Document(metadata={}, page_content='management can be enforced. Unit-1 – Introduction to Cloud Technologies 12 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Cloud Optimization Strategy Finding the right strategy for cloud adoption is another important challenge. Many businesses moved to the cloud'),\n",
              " Document(metadata={}, page_content='Many businesses moved to the cloud using a segmented approach in which isolated use cases, projects, and applications were migrated to cloud providers. The problem then for many companies is a lack of any holistic organization- wide cloud strategy. Finding the right strategy for cloud adoption'),\n",
              " Document(metadata={}, page_content='the right strategy for cloud adoption comes back to the issue of cloud governance. With everyone on the same page thanks to robust cloud governance and clear policies, organizations can create a unified and optimized strategy for how they use the cloud. Steps to Launch an Application with AWS'),\n",
              " Document(metadata={}, page_content='Steps to Launch an Application with AWS Elastic Beanstalk Step 1: Create a New Application Now that you’re in the AWS Elastic Beanstalk dashboard, click on Create New Application to create and configure your application. Step 2: Configure your Application Fill out the Application name with'),\n",
              " Document(metadata={}, page_content='Fill out the Application name with “Your-sample-app” and Description field with “Sample App”. Click Next to continue. Step 3: Configure your Environment For this tutorial, we will be creating a web server environment for our sample PHP application. Click on Create web server. Click on Select a'),\n",
              " Document(metadata={}, page_content='on Create web server. Click on Select a platform next to Predefined configuration, then select “Your Plateform”. Next, click on the drop-down menu next to Environment type, then select Single instance. Unit-1 – Introduction to Cloud Technologies 13 Prof. Vijay M. Shekhat, CE Department | 2180712 –'),\n",
              " Document(metadata={}, page_content='M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Under Source, select the Upload your own option, then click Choose File to select the “Your-sample-app- v1.zip” file we downloaded earlier. Fill in the values for Environment name with “YourSampleApp-env”. For Environment URL,'),\n",
              " Document(metadata={}, page_content='For Environment URL, fill in a globally unique value since this will be your public-facing URL; we will use “YourSampleApp-env” in this tutorial, so please choose something different from this one. Lastly, fill Description with “Your Sample App”. For the Environment URL, make sure to click Check'),\n",
              " Document(metadata={}, page_content='URL, make sure to click Check availability to make sure that the URL is not taken. Click Next to continue. Check the box next to Create this environment inside a VPC. Click Next to continue. On the Configuration Details step, you can set configuration options for the instances in your stack. Click'),\n",
              " Document(metadata={}, page_content='for the instances in your stack. Click Next. On the Environment Tags step, you can tag all the resources in your stack. Click Next. On the VPC Configuration step, select the first AZ listed by checking the box under the EC2 column. Your list of AZs may look different than the one shown as Regions'),\n",
              " Document(metadata={}, page_content='different than the one shown as Regions can have different number of AZs. Click Next. Unit-1 – Introduction to Cloud Technologies 14 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services At the Permissions step, leave everything to their default values, then click Next'),\n",
              " Document(metadata={}, page_content='their default values, then click Next to continue. Then review your environment configuration on the next screen and then click Launch to deploy your application. Step 4: Accessing your Elastic Beanstalk Application Go back to the main Elastic Beanstalk dashboard page by clicking on Elastic'),\n",
              " Document(metadata={}, page_content='dashboard page by clicking on Elastic Beanstalk. When your application successfully launched, your application’s environment, “YourSampleApp-env”, will show up as a green box. Click on “YourSampleApp-env”, which is the green box. At the top of the page, you should see a URL field, with a value that'),\n",
              " Document(metadata={}, page_content='see a URL field, with a value that contains the Environment URL you specified in step 3. Click on this URL field, and you should see a Congratulations page. Congratulations! You have successfully launched a sample PHP application using AWS Elastic Beanstalk. Unit-2 – Virtualization and Cloud'),\n",
              " Document(metadata={}, page_content='Unit-2 – Virtualization and Cloud Platforms 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Virtualization Virtualization is changing the mindset from physical to logical. Fig.: Virtualization What virtualization means is creating more logical IT resources,'),\n",
              " Document(metadata={}, page_content='is creating more logical IT resources, called virtual systems, within one physical system. That’s called system virtualization. It most commonly uses the hypervisor for managing the resources for every virtual system. The hypervisor is a software that can virtualize the hardware resources. Benefits'),\n",
              " Document(metadata={}, page_content='the hardware resources. Benefits of Virtualization More flexible and efficient allocation of resources. Enhance development productivity. It lowers the cost of IT infrastructure. Remote access and rapid scalability. High availability and disaster recovery. Pay per use of the IT infrastructure on'),\n",
              " Document(metadata={}, page_content='Pay per use of the IT infrastructure on demand. Enables running multiple operating system. Types of Virtualization 1. Application Virtualization: Application virtualization helps a user to have a remote access of an application from a server. The server stores all personal information and other'),\n",
              " Document(metadata={}, page_content='all personal information and other characteristics of the application but can still run on a local workstation through internet. Example of this would be a user who needs to run two different versions of the same software. Technologies that use application virtualization are hosted applications and'),\n",
              " Document(metadata={}, page_content='are hosted applications and packaged applications. 2. Network Virtualization: The ability to run multiple virtual networks with each has a separate control and data plan. It co-exists together on top of one physical network. It can be managed by individual parties that potentially confidential to'),\n",
              " Document(metadata={}, page_content='that potentially confidential to each other. Unit-2 – Virtualization and Cloud Platforms 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Network virtualization provides a facility to create and provision virtual networks—logical switches, routers, firewalls,'),\n",
              " Document(metadata={}, page_content='switches, routers, firewalls, load balancer, Virtual Private Network (VPN), and workload security within days or even in weeks. 3. Desktop Virtualization: Desktop virtualization allows the users’ OS to be remotely stored on a server in the data center. It allows the user to access their desktop'),\n",
              " Document(metadata={}, page_content='allows the user to access their desktop virtually, from any location by different machine. Users who wants specific operating systems other than Windows Server will need to have a virtual desktop. Main benefits of desktop virtualization are user mobility, portability, and easy management of'),\n",
              " Document(metadata={}, page_content='portability, and easy management of software installation, updates and patches. 4. Storage Virtualization: Storage virtualization is an array of servers that are managed by a virtual storage system. The servers aren’t aware of exactly where their data is stored, and instead function more like'),\n",
              " Document(metadata={}, page_content='stored, and instead function more like worker bees in a hive. It makes managing storage from multiple sources to be managed and utilized as a single repository. Storage virtualization software maintains smooth operations, consistent performance and a continuous suite of advanced functions despite'),\n",
              " Document(metadata={}, page_content='suite of advanced functions despite changes, break down and differences in the underlying equipment. Full Virtualization Virtual machine simulates hardware to allow an unmodified guest OS to be run in isolation. There is two type of Full virtualizations in the enterprise market. 1. Software'),\n",
              " Document(metadata={}, page_content='in the enterprise market. 1. Software assisted full virtualization 2. Hardware-assisted full virtualization On both full virtualization types, guest operating system’s source information will not be modified. 1. Software Assisted – Full Virtualization (BT – Binary Translation) It completely relies'),\n",
              " Document(metadata={}, page_content='Translation) It completely relies on binary translation to trap and virtualize the execution of sensitive, non- virtualizable instructions sets. It emulates the hardware using the software instruction sets. Due to binary translation, it often criticized for performance issue. Here is the list of'),\n",
              " Document(metadata={}, page_content='performance issue. Here is the list of software which will fall under software assisted (BT). o VMware workstation (32Bit guests) o Virtual PC o VirtualBox (32-bit guests) o VMware Server 2. Hardware-Assisted – Full Virtualization (VT) Hardware-assisted full virtualization eliminates the binary'),\n",
              " Document(metadata={}, page_content='virtualization eliminates the binary translation and it directly interrupts with hardware using the virtualization technology which has been integrated on X86 processors since 2005 (Intel VT-x and AMD-V). Guest OS’s instructions might allow a virtual context execute privileged instructions directly'),\n",
              " Document(metadata={}, page_content='privileged instructions directly on the processor, even though it is virtualized. Here is the list of enterprise software which supports hardware-assisted – Full virtualization which falls under hypervisor type 1 (Bare metal ) Unit-2 – Virtualization and Cloud Platforms 3 Prof. Vijay M. Shekhat, CE'),\n",
              " Document(metadata={}, page_content='Platforms 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services o VMware ESXi /ESX o KVM o Hyper-V o Xen The following virtualization type of virtualization falls under hypervisor type 2 (Hosted). o VMware Workstation (64-bit guests only ) o Virtual Box (64-bit'),\n",
              " Document(metadata={}, page_content='guests only ) o Virtual Box (64-bit guests only ) o VMware Server (Retired ) Paravirtualization Paravirtualization works differently from the full virtualization. It doesn’t need to simulate the hardware for the virtual machines. The hypervisor is installed on a physical server (host) and a guest'),\n",
              " Document(metadata={}, page_content='on a physical server (host) and a guest OS is installed into the environment. Virtual guests aware that it has been virtualized, unlike the full virtualization (where the guest doesn’t know that it has been virtualized) to take advantage of the functions. In this virtualization method, guest source'),\n",
              " Document(metadata={}, page_content='virtualization method, guest source codes will be modified with sensitive information to communicate with the host. Guest Operating systems require extensions to make API calls to the hypervisor. In full virtualization, guests will issue a hardware calls but in paravirtualization, guests will'),\n",
              " Document(metadata={}, page_content='but in paravirtualization, guests will directly communicate with the host (hypervisor) using the drivers. Here is the lisf of products which supports paravirtualization. o Xen o IBM LPAR o Oracle VM for SPARC (LDOM) o Oracle VM for X86 (OVM) Hybrid Virtualization (Hardware Virtualized with PV'),\n",
              " Document(metadata={}, page_content='(Hardware Virtualized with PV Drivers) In Hardware assisted full virtualization, Guest operating systems are unmodified and it involves many VM traps and thus high CPU overheads which limit the scalability. Paravirtualization is a complex method where guest kernel needs to be modified to inject the'),\n",
              " Document(metadata={}, page_content='needs to be modified to inject the API. By considering these issues, engineers have come with hybrid paravirtualization. It’s a combination of both Full & Paravirtualization. The virtual machine uses paravirtualization for specific hardware drivers (where there is a bottleneck with full'),\n",
              " Document(metadata={}, page_content='(where there is a bottleneck with full virtualization, especially with I/O & memory intense workloads), and the host uses full virtualization for other features. The following products support hybrid virtualization. o Oracle VM for x86 o Xen o VMware ESXi OS level Virtualization Operating'),\n",
              " Document(metadata={}, page_content='ESXi OS level Virtualization Operating system-level virtualization is widely used. It also known as “containerization”. Host Operating system kernel allows multiple user spaces also known as instance. In OS-level virtualization, unlike other virtualization technologies, there will be very little or'),\n",
              " Document(metadata={}, page_content='there will be very little or no overhead since its uses the host operating system kernel for execution. Unit-2 – Virtualization and Cloud Platforms 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Oracle Solaris zone is one of the famous containers in the'),\n",
              " Document(metadata={}, page_content='is one of the famous containers in the enterprise market. Here is the list of other containers. o Linux LCX o Docker o AIX WPAR Virtual computing Virtual computing refers to the use of a remote computer from a local computer where the actual computer user is located. For example, a user at a home'),\n",
              " Document(metadata={}, page_content=\"located. For example, a user at a home computer could log in to a remote office computer (via the Internet or a network) to perform job tasks. Once logged in via special software, the remote computer can be used as though it were at the user's location, allowing the user to perform tasks via the\"),\n",
              " Document(metadata={}, page_content='the user to perform tasks via the keyboard, mouse, or other tools. Virtual Machine A virtual machine (VM) is an operating system (OS) or application environment that is installed on software, which reproduces dedicated hardware. The end user has the same experience on a virtual machine as they'),\n",
              " Document(metadata={}, page_content='experience on a virtual machine as they would have on dedicated hardware. Virtual Machine Conversions in VMM (Virtual Machine Migration) When you use cloud computing, you are accessing pooled resources using a technique called virtualization. Virtualization assigns a logical name for a physical'),\n",
              " Document(metadata={}, page_content='assigns a logical name for a physical resource and then provides a pointer to that physical resource when a request is made. Virtualization provides a means to manage resources efficiently because the mapping of virtual resources to physical resources can be both dynamic and facile. Virtualization'),\n",
              " Document(metadata={}, page_content='both dynamic and facile. Virtualization is dynamic in that the mapping can be assigned based on rapidly changing conditions, and it is facile because changes to a mapping assignment can be nearly instantaneous. These are among the different types of virtualization that are characteristic of cloud'),\n",
              " Document(metadata={}, page_content='that are characteristic of cloud computing: o Access: A client can request access to a cloud service from any location. o Application: A cloud has multiple application instances and directs requests to an instance based on conditions. o CPU: Computers can be partitioned into a set of virtual'),\n",
              " Document(metadata={}, page_content='be partitioned into a set of virtual machines with each machine being assigned a workload. Alternatively, systems can be virtualized through load-balancing technologies. o Storage: Data is stored across storage devices and often replicated for redundancy. To enable these characteristics, resources'),\n",
              " Document(metadata={}, page_content='enable these characteristics, resources must be highly configurable and flexible. You can define the features in software and hardware that enable this flexibility as conforming to one or more of the following mobility patterns: o P2V: Physical to Virtual o V2V: Virtual to Virtual o V2P: Virtual to'),\n",
              " Document(metadata={}, page_content='Virtual to Virtual o V2P: Virtual to Physical o P2P: Physical to Physical o D2C: Datacenter to Cloud o C2C: Cloud to Cloud o C2D: Cloud to Datacenter o D2D: Datacenter to Datacenter Unit-2 – Virtualization and Cloud Platforms 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure'),\n",
              " Document(metadata={}, page_content='| 2180712 – Cloud Infrastructure and Services Virtual Machine Types 1. General Purpose This family includes the M1 and M3 VM types. These types provide a balance of CPU, memory, and network resources, which makes them a good choice for many applications. The VM types in this family range in size'),\n",
              " Document(metadata={}, page_content='VM types in this family range in size from one virtual CPU with two GB of RAM to eight virtual CPUs with 30 GB of RAM. The balance of resources makes them ideal for running small and mid-size databases, more memory- hungry data processing tasks, caching fleets, and backend servers. M1 types offer'),\n",
              " Document(metadata={}, page_content='and backend servers. M1 types offer smaller instance sizes with moderate CPU performance. M3 types offer larger number of virtual CPUs that provide higher performance. It is recommended to use M3 instances if you need general-purpose instances with demanding CPU requirements. 2. Compute Optimized'),\n",
              " Document(metadata={}, page_content='CPU requirements. 2. Compute Optimized This family includes the C1 and CC2 instance types, and is geared towards applications that benefit from high compute power. Compute-optimized VM types have a higher ratio of virtual CPUs to memory than other families but share the NCs (Node Controllers) with'),\n",
              " Document(metadata={}, page_content='share the NCs (Node Controllers) with non-optimized ones. It is recommended to use these type if you are running any CPU-bound scale-out applications. CC2 instances provide high core count (32 virtual CPUs) and support for cluster networking. C1 instances are available in smaller sizes and are'),\n",
              " Document(metadata={}, page_content='are available in smaller sizes and are ideal for scaled-out applications at massive scale. 3. Memory Optimized This family includes the CR1 and M2 VM types and is designed for memory-intensive applications. It is recommended to use these VM types for performance-sensitive database, where your'),\n",
              " Document(metadata={}, page_content='database, where your application is memory-bound. CR1 VM types provide more memory and faster CPU than do M2 types. CR1 instances also support cluster networking for bandwidth intensive applications. M2 types are available in smaller sizes, and are an excellent option for many memory-bound'),\n",
              " Document(metadata={}, page_content='excellent option for many memory-bound applications. 4. Micro This Micro family contains the T1 VM type. The T1 micro provides a small amount of consistent CPU resources and allows you to increase CPU capacity in short bursts when additional cycles are available. It is recommended to use this type'),\n",
              " Document(metadata={}, page_content='It is recommended to use this type of VM for lower throughput applications like a proxy server or administrative applications, or for low-traffic websites that occasionally require additional compute cycles. It is not recommended for applications that require sustained CPU performance. Load'),\n",
              " Document(metadata={}, page_content='require sustained CPU performance. Load Balancing In computing, load balancing improves the distribution of workloads across multiple computing resources, such as computers, a computer cluster, network links, central processing units, or disk drives. Unit-2 – Virtualization and Cloud Platforms 6'),\n",
              " Document(metadata={}, page_content='– Virtualization and Cloud Platforms 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Need of load balancing in cloud computing (i) High Performing applications o Cloud load balancing techniques, unlike their traditional on premise counterparts, are less'),\n",
              " Document(metadata={}, page_content='on premise counterparts, are less expensive and simple to implement. Enterprises can make their client applications work faster and deliver better performances, that too at potentially lower costs. (ii) Increased scalability o Cloud balancing takes help of cloud’s scalability and agility to'),\n",
              " Document(metadata={}, page_content='of cloud’s scalability and agility to maintain website traffic. By using efficient load balancers, you can easily match up the increased user traffic and distribute it among various servers or network devices. It is especially important for ecommerce websites, who deals with thousands of website'),\n",
              " Document(metadata={}, page_content='who deals with thousands of website visitors every second. During sale or other promotional offers they need such effective load balancers to distribute workloads. (iii) Ability to handle sudden traffic spikes o A normally running University site can completely go down during any result'),\n",
              " Document(metadata={}, page_content='completely go down during any result declaration. This is because too many requests can arrive at the same time. If they are using cloud load balancers, they do not need to worry about such traffic surges. No matter how large the request is, it can be wisely distributed among different servers for'),\n",
              " Document(metadata={}, page_content='distributed among different servers for generating maximum results in less response time. (iv) Business continuity with complete flexibility o The basic objective of using a load balancer is to save or protect a website from sudden outages. When the workload is distributed among various servers or'),\n",
              " Document(metadata={}, page_content='is distributed among various servers or network units, even if one node fails the burden can be shifted to another active node. o Thus, with increased redundancy, scalability and other features load balancing easily handles website or application traffic. Network resources that can be load balanced'),\n",
              " Document(metadata={}, page_content='resources that can be load balanced Servers Routing mechanism Hypervisors It is the part of the private cloud that manages the virtual machines, i.e. it is the part (program) that enables multiple operating systems to share the same hardware. Each operating system could use all the hardware'),\n",
              " Document(metadata={}, page_content='system could use all the hardware (processor, memory, etc.) if no other operating system is on. That is the maximum hardware available to one operating system in the cloud. Nevertheless, the hypervisor is what controls and allocates what portion of hardware resources each operating system should'),\n",
              " Document(metadata={}, page_content='resources each operating system should get, in order every one of them to get what they need and not to disrupt each other. Unit-2 – Virtualization and Cloud Platforms 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services There are two types of hypervisors Fig.:'),\n",
              " Document(metadata={}, page_content='are two types of hypervisors Fig.: Types of Hypervisors Type 1 hypervisor: hypervisors run directly on the system hardware – A “bare metal” embedded hypervisor. Examples are: 1) VMware ESX and ESXi 2) Microsoft Hyper-V 3) Citrix XenServer 4) Oracle VM Type 2 hypervisor: hypervisors run on a host'),\n",
              " Document(metadata={}, page_content='2 hypervisor: hypervisors run on a host operating system that provides virtualization services, such as I/O device support and memory management. Examples are: 1) VMware Workstation/Fusion/Player 2) Microsoft Virtual PC 3) Oracle VM VirtualBox 4) Red Hat Enterprise Virtualization Machine Imaging'),\n",
              " Document(metadata={}, page_content='Virtualization Machine Imaging Machine imaging is a process that is used to achieve the goal of system portability, provision, and deploy systems in the cloud through capturing the state of systems using a system image. A system image makes a copy or a clone of the entire computer system inside a'),\n",
              " Document(metadata={}, page_content='of the entire computer system inside a single file. The image is made by using a program called system imaging program and can be used later to restore a system image. For example Amazon Machine Image (AMI) is a system image that is used in the cloud computing. The Amazon Web Services uses AMI to'),\n",
              " Document(metadata={}, page_content='The Amazon Web Services uses AMI to store copies of a virtual machine. An AMI is a file system image that contains an operating system, all device drivers, and any applications and state information that the working virtual machine would have. The AMI files are encrypted and compressed for security'),\n",
              " Document(metadata={}, page_content='encrypted and compressed for security purpose and stored in Amazon S3 (Simple Storage System) buckets as a set of 10MB chunks. Machine imaging is mostly run on virtualization platform due to this it is also called as Virtual Appliances and running virtual machines are called instances. Because many'),\n",
              " Document(metadata={}, page_content='are called instances. Because many users share clouds, the cloud helps you track information about images, such as ownership, history, and so on. The IBM SmartCloud Enterprise knows what organization you belong to when you log in. You can choose whether to keep images private, exclusively for your'),\n",
              " Document(metadata={}, page_content='images private, exclusively for your own use, or to share with other users in your organization. If you are an independent software vendor, you can also add your images to the public catalog. Unit-2 – Virtualization and Cloud Platforms 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud'),\n",
              " Document(metadata={}, page_content='CE Department | 2180712 – Cloud Infrastructure and Services Cloud Marketplace Overview A cloud marketplace is an online storefront operated by a cloud service provider. A cloud marketplace provides customers with access to software applications and services that are built on, integrate with or'),\n",
              " Document(metadata={}, page_content=\"that are built on, integrate with or complement the cloud provider's offerings. A marketplace typically provides customers with native cloud applications and approved apps created by third-party developers. Applications from third-party developers not only help the cloud provider fill niche gaps in\"),\n",
              " Document(metadata={}, page_content=\"the cloud provider fill niche gaps in its portfolio and meet the needs of more customers, but they also provide the customer with peace of mind by knowing that all purchases from the vendor's marketplace will integrate with each other smoothly. Examples of cloud marketplaces AWS Marketplace - helps\"),\n",
              " Document(metadata={}, page_content='marketplaces AWS Marketplace - helps customers find, buy and use software and services that run in the Amazon Elastic Compute Cloud (EC2). Oracle Marketplace - offers a comprehensive list of apps for sales, service, marketing, talent management and human capital management. Microsoft Windows Azure'),\n",
              " Document(metadata={}, page_content=\"management. Microsoft Windows Azure Marketplace - an online market for buying and selling Software as a Service (SaaS) applications and research datasets. Salesforce.com's AppExchange - provides business apps for sales representatives and customer relationship management (CRM). Comparison of Cloud\"),\n",
              " Document(metadata={}, page_content='management (CRM). Comparison of Cloud Providers Amazon Web Service Azure Rackspace Introduction Amazon Web Services (AWS) is a collection of remote computing services (also called web services) that together make up a cloud computing platform, offered over the Internet by Amazon.com. Azure is a'),\n",
              " Document(metadata={}, page_content='the Internet by Amazon.com. Azure is a cloud computing platform and infrastructure, created by Microsoft, for building, deploying and managing applications and services through a global network of Microsoft-managed datacenters. Rackspace is a managed cloud computing provider offering high'),\n",
              " Document(metadata={}, page_content='cloud computing provider offering high percentage availability of applications based on RAID10. Distinguishing Features Rich set of services and integrated monitoring tools; competitive pricing model. Easy-to-use administration tool, especially for Windows admins. Easy to use control panel,'),\n",
              " Document(metadata={}, page_content='admins. Easy to use control panel, especially for non-system administrators. Virtualization Xen hypervisor Microsoft Hyper-V Opensource (Xen, Kvm ) and VMware Base OS Linux (+QEMU) and Windows Windows and Linux Ubuntu Pricing model Pay-as-you-go, then subscription Pay-as-you-go Pay-as-you-go Major'),\n",
              " Document(metadata={}, page_content='Pay-as-you-go Pay-as-you-go Major products Elastic block store, IP addresses, virtual private cloud, cloud watch, Cloud Front, clusters etc. Server Failover Clustering, Network Load Balancing, SNMP Services, Storage Manager for SANs, Windows Internet Name Service, Disaster Recovery to Azure, Azure'),\n",
              " Document(metadata={}, page_content='Disaster Recovery to Azure, Azure Caching and Azure Redis Cache. Managed cloud, block storage, monitoring Unit-2 – Virtualization and Cloud Platforms 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Amazon Web Service Azure Rackspace CDN Features Origin-Pull,'),\n",
              " Document(metadata={}, page_content='Rackspace CDN Features Origin-Pull, Purge, Gzip compression, Persistent connections, Caching headers, Custom CNAMEs, Control Panel & stats, Access Logs. Robust security, Lower latencies, Massively scalable, Capacity on demand. Rackspace provide CDN services through a partnership with Akamai’s'),\n",
              " Document(metadata={}, page_content='through a partnership with Akamai’s service. Access interface Web-based, API, console Web interface Web-based control panel Preventive measures Moderate Basic Basic Reactive measures Moderate Basic Basic Reliability Good Average Good Scalability Good Good Good Support Good and chargeable Good'),\n",
              " Document(metadata={}, page_content='Good Support Good and chargeable Good Excellent Availability (%) 99.95 99.95 99.99 Server Performance (Over a period) Good Excellent and consistent Average Tools/ framework Amazon machine image (AMI), Java, PHP, Python, Ruby PHP, ASP.NET, Node.js, Python - Database RDS MySQL, MsSQL, Oracle'),\n",
              " Document(metadata={}, page_content='- Database RDS MySQL, MsSQL, Oracle Microsoft SQL Database MySQL Unit-3 – Introduction to AWS 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS History The AWS platform was launched in July 2002. In its early stages, the platform consisted of only a few'),\n",
              " Document(metadata={}, page_content=\"the platform consisted of only a few disparate tools and services. Then in late 2003, the AWS concept was publicly reformulated when Chris Pinkham and Benjamin Black presented a paper describing a vision for Amazon's retail computing infrastructure that was completely standardized, completely\"),\n",
              " Document(metadata={}, page_content='was completely standardized, completely automated, and would rely extensively on web services for services such as storage and would draw on internal work already underway. Near the end of their paper, they mentioned the possibility of selling access to virtual servers as a service, proposing the'),\n",
              " Document(metadata={}, page_content='servers as a service, proposing the company could generate revenue from the new infrastructure investment. In November 2004, the first AWS service launched for public usage: Simple Queue Service (SQS). Thereafter Pinkham and lead developer Christopher Brown developed the Amazon EC2 service, with a'),\n",
              " Document(metadata={}, page_content='the Amazon EC2 service, with a team in Cape Town, South Africa. Amazon Web Services was officially re-launched on March 14, 2006, combining the three initial service offerings of Amazon S3 cloud storage, SQS, and EC2. The AWS platform finally provided an integrated suite of core online services, as'),\n",
              " Document(metadata={}, page_content='suite of core online services, as Chris Pinkham and Benjamin Black had proposed back in 2003, as a service offered to other developers, web sites, client- side applications, and companies. Andy Jassy, AWS founder and vice president in 2006, said at the time that Amazon S3 (one of the first and most'),\n",
              " Document(metadata={}, page_content='Amazon S3 (one of the first and most scalable elements of AWS) helps free developers from worrying about where they are going to store data, whether it will be safe and secure, if it will be available when they need it, the costs associated with server maintenance, or whether they have enough'),\n",
              " Document(metadata={}, page_content='or whether they have enough storage available. Amazon S3 enables developers to focus on innovating with data, rather than figuring out how to store it. In 2016 Jassy was promoted to CEO of the division. Reflecting the success of AWS, his annual compensation in 2017 hit nearly $36 million. In 2014,'),\n",
              " Document(metadata={}, page_content='2017 hit nearly $36 million. In 2014, AWS launched its partner network entitled APN (AWS Partner Network) which is focused on helping AWS-based companies grow and scale the success of their business with close collaboration and best practices. To support industry-wide training and skills'),\n",
              " Document(metadata={}, page_content='industry-wide training and skills standardization, AWS began offering a certification program for computer engineers, on April 30, 2013, to highlight expertise in cloud computing. In January 2015, Amazon Web Services acquired Annapurna Labs, an Israel-based microelectronics company reputedly for'),\n",
              " Document(metadata={}, page_content='microelectronics company reputedly for US$350–370M. James Hamilton, an AWS engineer, wrote a retrospective article in 2016 to highlight the ten-year history of the online service from 2006 to 2016. As an early fan and outspoken proponent of the technology, he had joined the AWS engineering team in'),\n",
              " Document(metadata={}, page_content=\"had joined the AWS engineering team in 2008. In January 2018, Amazon launched an auto scaling service on AWS. In November 2018, AWS announced customized ARM cores for use in its servers. Also in November 2018, AWS is developing ground stations to communicate with customer's satellites. AWS\"),\n",
              " Document(metadata={}, page_content=\"with customer's satellites. AWS Infrastructure Amazon Web Services (AWS) is a global public cloud provider, and as such, it has to have a global network of infrastructure to run and manage its many growing cloud services that support customers around the world. Now we’ll take a look at the\"),\n",
              " Document(metadata={}, page_content='the world. Now we’ll take a look at the components that make up the AWS Global Infrastructure. 1) Availability Zones (AZs) Unit-3 – Introduction to AWS 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services 2) Regions 3) Edge Locations 4) Regional Edge Caches If you'),\n",
              " Document(metadata={}, page_content='4) Regional Edge Caches If you are deploying services on AWS, you’ll want to have a clear understanding of each of these components, how they are linked, and how you can use them within your solution to YOUR maximum benefit. Let’s take a closer look. 1) Availability Zones (AZ) AZs are essentially'),\n",
              " Document(metadata={}, page_content='Zones (AZ) AZs are essentially the physical data centers of AWS. This is where the actual compute, storage, network, and database resources are hosted that we as consumers provision within our Virtual Private Clouds (VPCs). A common misconception is that a single availability zone is equal to a'),\n",
              " Document(metadata={}, page_content='single availability zone is equal to a single data center. This is not the case. In fact, it’s likely that multiple data centers located close together form a single availability zone. Each AZ will always have at least one other AZ that is geographically located within the same area, usually a'),\n",
              " Document(metadata={}, page_content='located within the same area, usually a city, linked by highly resilient and very low latency private fiber optic connections. However, each AZ will be isolated from the others using separate power and network connectivity that minimizes impact to other AZs should a single AZ fail. These low'),\n",
              " Document(metadata={}, page_content='AZs should a single AZ fail. These low latency links between AZs are used by many AWS services to replicate data for high availability and resilience purposes. Multiple AZs within a region allows you to create highly available and resilient applications and services. By architecting your solutions'),\n",
              " Document(metadata={}, page_content='By architecting your solutions to utilize resources across more than one AZ ensures that minimal or no impact will occur to your infrastructure should an AZ experience a failure, which does happen. Anyone can deploy resources in the cloud, but architecting them in a way that ensures your'),\n",
              " Document(metadata={}, page_content='them in a way that ensures your infrastructure remains stable, available, and resilient when faced with a disaster is a different matter. Making use of at least two AZs in a region helps you maintain high availability of your infrastructure and it’s always a recommended best practice. Fig.:'),\n",
              " Document(metadata={}, page_content='a recommended best practice. Fig.: Availability Zone and Region 2) Regions Region is a collection of availability zones that are geographically located close to one other. This is generally indicated by AZs within the same city. AWS has deployed them across the globe to allow its worldwide customer'),\n",
              " Document(metadata={}, page_content='globe to allow its worldwide customer base to take advantage of low latency connections. Each Region will act independently of the others, and each will contain at least two Availability Zones. Example: if an organization based in London was serving customers throughout Europe, there would be no'),\n",
              " Document(metadata={}, page_content='throughout Europe, there would be no logical sense to deploy services in the Sydney Region simply due to the latency response times for its Unit-3 – Introduction to AWS 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services customers. Instead, the company would select'),\n",
              " Document(metadata={}, page_content='Instead, the company would select the region most appropriate for them and their customer base, which may be the London, Frankfurt, or Ireland Region. Having global regions also allows for compliance with regulations, laws, and governance relating to data storage (at rest and in transit). Example:'),\n",
              " Document(metadata={}, page_content='(at rest and in transit). Example: you may be required to keep all data within a specific location, such as Europe. Having multiple regions within this location allows an organization to meet this requirement. Similarly to how utilizing multiple AZs within a region creates a level of high'),\n",
              " Document(metadata={}, page_content='within a region creates a level of high availability, the same can be applied to utilizing multiple regions. You may want to use multiple regions if you are a global organization serving customers in different countries that have specific laws and governance about the use of data. In this case, you'),\n",
              " Document(metadata={}, page_content='the use of data. In this case, you could even connect different VPCs together in different regions. The number of regions is increasing year after year as AWS works to keep up with the demand for cloud computing services. In July 2017, there are currently 16 Regions and 43 Availability Zones, with'),\n",
              " Document(metadata={}, page_content='Regions and 43 Availability Zones, with 4 Regions and 11 AZs planned. 3) Edge Locations Edge Locations are AWS sites deployed in major cities and highly populated areas across the globe. They far outnumber the number of availability zones available. While Edge Locations are not used to deploy your'),\n",
              " Document(metadata={}, page_content='Locations are not used to deploy your main infrastructures such as EC2 instances, EBS storage, VPCs, or RDS resources like AZs, they are used by AWS services such as AWS CloudFront and AWS Lambda@Edge (currently in Preview) to cache data and reduce latency for end user access by using the Edge'),\n",
              " Document(metadata={}, page_content='for end user access by using the Edge Locations as a global Content Delivery Network (CDN). As a result, Edge Locations are primarily used by end users who are accessing and using your services. For example, you may have your website hosted on EC2 instances and S3 (your origin) within the Ohio'),\n",
              " Document(metadata={}, page_content='and S3 (your origin) within the Ohio region with a configured CloudFront distribution associated. When a user accesses your website from Europe, they would be re-directed to their closest Edge Location (in Europe) where cached data could be read on your website, significantly reducing latency.'),\n",
              " Document(metadata={}, page_content='significantly reducing latency. Fig.: Edge Location and Regional Edge Cache 4) Regional Edge Cache In November 2016, AWS announced a new type of Edge Location, called a Regional Edge Cache. These sit between your CloudFront Origin servers and the Edge Locations. Unit-3 – Introduction to AWS 4 Prof.'),\n",
              " Document(metadata={}, page_content='Unit-3 – Introduction to AWS 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services A Regional Edge Cache has a larger cache-width than each of the individual Edge Locations, and because data expires from the cache at the Edge Locations, the data is retained at the'),\n",
              " Document(metadata={}, page_content='Locations, the data is retained at the Regional Edge Caches. Therefore, when data is requested at the Edge Location that is no longer available, the Edge Location can retrieve the cached data from the Regional Edge Cache instead of the Origin servers, which would have a higher latency. Pods,'),\n",
              " Document(metadata={}, page_content='would have a higher latency. Pods, Aggregation, Silos Workloads support a certain no. of user when the workload reaches the limit of largest virtual machine instance possible, a copy or clone of the instance is required. A group of users within a particular instance is called a pod. Sizing'),\n",
              " Document(metadata={}, page_content='instance is called a pod. Sizing limitation of pod need to be considered when building large cloud-based application. Pods are aggregated into pools within IaaS region or site called an availability zone. When the computing infrastructure isolates user clouds from one another, so that'),\n",
              " Document(metadata={}, page_content='user clouds from one another, so that interoperating is impossible this creates an information silo, or simply silo. AWS Services This AWS services list covers the huge catalog of services offered by Amazon Web Services (AWS). These services range from the core compute products like EC2 to newer'),\n",
              " Document(metadata={}, page_content='core compute products like EC2 to newer releases like AWS Deepracer for machine learning. There are currently 190 unique services provided by AWS which divided into 24 categories which are listed below: o Analytics o Application Integration o AR & VR o AWS Cost Management o Blockchain o Business'),\n",
              " Document(metadata={}, page_content='Cost Management o Blockchain o Business Applications o Compute o Customer Engagement o Database o Developer Tools o End User Computing o Game Tech o Internet of Things o Machine Learning o Management & Governance o Media Services o Migration & Transfer o Mobile o Networking & Content Delivery o'),\n",
              " Document(metadata={}, page_content='o Networking & Content Delivery o Robotics o Satellite o Security, Identity, & Compliance o Storage o Quantum Technologies Unit-3 – Introduction to AWS 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS Ecosystem In general a cloud ecosystem is a complex'),\n",
              " Document(metadata={}, page_content='general a cloud ecosystem is a complex system of interdependent components that all work together to enable cloud services. In cloud computing, the ecosystem consists of hardware and software as well as cloud customers, cloud engineers, consultants, integrators and partners. Amazon Web Services'),\n",
              " Document(metadata={}, page_content='and partners. Amazon Web Services (AWS) is the market leader in IaaS (Infrastructure-as-a-Service) and PaaS (Platform-as-a-Service) for cloud ecosystems, which can be combined to create a scalable cloud application without worrying about delays related to infrastructure provisioning (compute,'),\n",
              " Document(metadata={}, page_content='infrastructure provisioning (compute, storage, and network) and management. With AWS you can select the specific solutions you need, and only pay for exactly what you use, resulting in lower capital expenditure and faster time to value without sacrificing application performance or user experience.'),\n",
              " Document(metadata={}, page_content='performance or user experience. New and existing companies can build their digital infrastructure partially or entirely in the cloud with AWS, making the on premise data center a thing of the past. The AWS cloud ensures infrastructure reliability, compliance with security standards, and the ability'),\n",
              " Document(metadata={}, page_content='security standards, and the ability to instantly grow or shrink your infrastructure to meet your needs and maximize your budget, all without upfront investment in equipment. Unit-4 – Programming, Management console and Storage on AWS 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud'),\n",
              " Document(metadata={}, page_content='CE Department | 2180712 – Cloud Infrastructure and Services Basic Understanding APIs Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web'),\n",
              " Document(metadata={}, page_content='APIs that access AWS or other web services, as well as data stored in the AWS Cloud. As an API Gateway API developer, you can create APIs for use in your own client applications. Or you can make your APIs available to third-party app developers. For more information, see Who uses API Gateway? API'),\n",
              " Document(metadata={}, page_content='see Who uses API Gateway? API Gateway creates RESTful APIs that: o Are HTTP-based. o Enable stateless client-server communication. o Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE. AWS Programming Interfaces API Gateway API Gateway is an AWS service that supports the'),\n",
              " Document(metadata={}, page_content='is an AWS service that supports the following: o Creating, deploying, and managing a RESTful application programming interface (API) to expose backend HTTP endpoints, AWS Lambda functions, or other AWS services. o Creating, deploying, and managing a WebSocket API to expose AWS Lambda functions or'),\n",
              " Document(metadata={}, page_content='API to expose AWS Lambda functions or other AWS services. o Invoking exposed API methods through the frontend HTTP and WebSocket endpoints. API Gateway REST API A collection of HTTP resources and methods that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You'),\n",
              " Document(metadata={}, page_content='functions, or other AWS services. You can deploy this collection in one or more stages. Typically, API resources are organized in a resource tree according to the application logic. Each API resource can expose one or more API methods that have unique HTTP verbs supported by API Gateway. API'),\n",
              " Document(metadata={}, page_content='verbs supported by API Gateway. API Gateway HTTP API A collection of routes and methods that are integrated with backend HTTP endpoints or Lambda functions. You can deploy this collection in one or more stages. Each route can expose one or more API methods that have unique HTTP verbs supported by'),\n",
              " Document(metadata={}, page_content='have unique HTTP verbs supported by API Gateway. API Gateway WebSocket API A collection of WebSocket routes and route keys that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can deploy this collection in one or more stages. API methods are invoked through'),\n",
              " Document(metadata={}, page_content='stages. API methods are invoked through frontend WebSocket connections that you can associate with a registered custom domain name. API Deployment A point-in-time snapshot of your API Gateway API. To be available for clients to use, the deployment must be associated with one or more API stages. API'),\n",
              " Document(metadata={}, page_content='with one or more API stages. API Developer Your AWS account that owns an API Gateway deployment (for example, a service provider that also supports programmatic access). Unit-4 – Programming, Management console and Storage on AWS 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud'),\n",
              " Document(metadata={}, page_content='CE Department | 2180712 – Cloud Infrastructure and Services API Endpoint A hostname for an API in API Gateway that is deployed to a specific Region. The hostname is of the form {api-id}.execute-api.{region}.amazonaws.com. The following types of API endpoints are supported: o Edge-optimized API'),\n",
              " Document(metadata={}, page_content='are supported: o Edge-optimized API endpoint \\uf0a7 The default hostname of an API Gateway API that is deployed to the specified Region while using a CloudFront distribution to facilitate client access typically from across AWS Regions. API requests are routed to the nearest CloudFront Point of Presence'),\n",
              " Document(metadata={}, page_content='nearest CloudFront Point of Presence (POP), which typically improves connection time for geographically diverse clients. o Private API endpoint \\uf0a7 An API endpoint that is exposed through interface VPC endpoints and allows a client to securely access private API resources inside a VPC. Private APIs'),\n",
              " Document(metadata={}, page_content='resources inside a VPC. Private APIs are isolated from the public internet, and they can only be accessed using VPC endpoints for API Gateway that have been granted access. o Regional API endpoint \\uf0a7 The host name of an API that is deployed to the specified Region and intended to serve clients, such'),\n",
              " Document(metadata={}, page_content='and intended to serve clients, such as EC2 instances, in the same AWS Region. API requests are targeted directly to the Region- specific API Gateway API without going through any CloudFront distribution. For in-Region requests, a Regional endpoint bypasses the unnecessary round trip to a CloudFront'),\n",
              " Document(metadata={}, page_content='unnecessary round trip to a CloudFront distribution. API Key An alphanumeric string that API Gateway uses to identify an app developer who uses your REST or WebSocket API. API Gateway can generate API keys on your behalf, or you can import them from a CSV file. You can use API keys together with'),\n",
              " Document(metadata={}, page_content='You can use API keys together with Lambda authorizers or usage plans to control access to your APIs. WebSocket Connection API Gateway maintains a persistent connection between clients and API Gateway itself. There is no persistent connection between API Gateway and backend integrations such as'),\n",
              " Document(metadata={}, page_content='and backend integrations such as Lambda functions. Backend services are invoked as needed, based on the content of messages received from clients. Web Services You can choose from a couple of different schools of thought for how web services should be delivered. The older approach, SOAP (short for'),\n",
              " Document(metadata={}, page_content='The older approach, SOAP (short for Simple Object Access Protocol), had widespread industry support, complete with a comprehensive set of standards. Those standards were too comprehensive, unfortunately. The people designing SOAP set it up to be extremely flexible —it can communicate across the'),\n",
              " Document(metadata={}, page_content='flexible —it can communicate across the web, e-mail, and private networks. To ensure security and manageability, a number of supporting standards that integrate with SOAP were also defined. SOAP is based on a document encoding standard known as Extensible Markup Language (XML, for short), and the'),\n",
              " Document(metadata={}, page_content='Language (XML, for short), and the SOAP service is defined in such a way that users can then leverage XML no matter what the underlying communication network is. For this system to work, though, the data transferred by SOAP (commonly referred to as the payload) also needs to be in XML format.'),\n",
              " Document(metadata={}, page_content='also needs to be in XML format. Notice a pattern here? The push to be comprehensive and flexible (or, to be all things to all people) plus the XML payload requirement meant that SOAP ended up being quite complex, making it a lot of work to use properly. As you might guess, many IT people found SOAP'),\n",
              " Document(metadata={}, page_content='might guess, many IT people found SOAP daunting and, consequently, resisted using it. Unit-4 – Programming, Management console and Storage on AWS 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services About a decade ago, a doctoral student defined another web services'),\n",
              " Document(metadata={}, page_content='student defined another web services approach as part of his thesis: REST, or Representational State Transfer. REST, which is far less comprehensive than SOAP, aspires to solve fewer problems. It doesn’t address some aspects of SOAP that seemed important but that, in retrospect, made it more'),\n",
              " Document(metadata={}, page_content='but that, in retrospect, made it more complex to use — security, for example. The most important aspect of REST is that it’s designed to integrate with standard web protocols so that REST services can be called with standard web verbs and URLs. For example, a valid REST call looks like this:'),\n",
              " Document(metadata={}, page_content='a valid REST call looks like this: http://search.examplecompany.com/CompanyDirectory/EmployeeInfo?empname=BernardGolden That’s all it takes to make a query to the REST service of examplecompany to see my personnel information. The HTTP verb that accompanies this request is GET, asking for'),\n",
              " Document(metadata={}, page_content='this request is GET, asking for information to be returned. To delete information, you use the verb DELETE. To insert my information, you use the verb POST. To update my information, you use the verb PUT. For the POST and PUT actions, additional information would accompany the empname and be'),\n",
              " Document(metadata={}, page_content='would accompany the empname and be separated by an ampersand (&) to indicate another argument to be used by the service. REST imposes no particular formatting requirements on the service payloads. In this respect, it differs from SOAP, which requires XML. For simple interactions, a string of bytes'),\n",
              " Document(metadata={}, page_content='simple interactions, a string of bytes is all you need for the payload. For more complex interactions (say, in addition to returning my employee information, I want to place a request for the employee information of all employees whose names start with G), the encoding convention JSON is used. As'),\n",
              " Document(metadata={}, page_content='encoding convention JSON is used. As you might expect, REST’s simpler use model, its alignment with standard web protocols and verbs, and its less restrictive payload formatting made it catch on with developers like a house on fire. AWS originally launched with SOAP support for interactions with'),\n",
              " Document(metadata={}, page_content='with SOAP support for interactions with its API, but it has steadily deprecated its SOAP interface in favor of REST. AWS URL Naming You can access your bucket using the Amazon S3 console. Using the console UI, you can perform almost all bucket operations without having to write any code. If you'),\n",
              " Document(metadata={}, page_content='having to write any code. If you access a bucket programmatically, note that Amazon S3 supports RESTful architecture in which your buckets and objects are resources, each with a resource URI that uniquely identifies the resource. Amazon S3 supports both virtual-hosted–style and path-style URLs to'),\n",
              " Document(metadata={}, page_content='and path-style URLs to access a bucket. In a virtual-hosted–style URL, the bucket name is part of the domain name in the URL. For example: o http://bucket.s3.amazonaws.com o http://bucket.s3-aws-region.amazonaws.com In a virtual-hosted–style URL, you can use either of these endpoints. If you make a'),\n",
              " Document(metadata={}, page_content='of these endpoints. If you make a request to the http://bucket.s3.amazonaws.com endpoint, the DNS has sufficient information to route your request directly to the Region where your bucket resides. In a path-style URL, the bucket name is not part of the domain (unless you use a Region-specific'),\n",
              " Document(metadata={}, page_content='(unless you use a Region-specific endpoint). For example: o US East (N. Virginia) Region endpoint, http://s3.amazonaws.com/bucket o Region-specific endpoint, http://s3-aws-region.amazonaws.com/bucket In a path-style URL, the endpoint you use must match the Region in which the bucket resides. Unit-4'),\n",
              " Document(metadata={}, page_content='in which the bucket resides. Unit-4 – Programming, Management console and Storage on AWS 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services For example, if your bucket is in the South America (São Paulo) Region, you must use the http://s3-sa-'),\n",
              " Document(metadata={}, page_content='Region, you must use the http://s3-sa- east-1.amazonaws.com/bucket endpoint. If your bucket is in the US East (N. Virginia) Region, you must use the http://s3.amazonaws.com/bucket endpoint. Matching Interfaces and Services In the simplest case, the service interfaces are identical apart from name'),\n",
              " Document(metadata={}, page_content='are identical apart from name and category (inbound or outbound), that is, if the outbound service interface and all interface objects, are referenced by this service interface, are copies of the corresponding objects of an inbound service interfaces. If, however, the consumer only wants to call'),\n",
              " Document(metadata={}, page_content='the consumer only wants to call one operation of the inbound service interface, for example, it is not necessary to create the other inbound service interface operations in the outbound service interface as well. Simply put, the operations and corresponding outbound service interface data'),\n",
              " Document(metadata={}, page_content='outbound service interface data structures can be a subset of the operations and corresponding data structures of the inbound interface referenced. The service interface editor provides a check for the service interface pairs to determine the compatibility of the inbound and outbound service'),\n",
              " Document(metadata={}, page_content='of the inbound and outbound service interface. This check is performed in multiple steps to determine compatibility (starting service interfaces, across operations and down to data types). The following section describes the steps to be able to estimate for a service interface assignment whether'),\n",
              " Document(metadata={}, page_content='a service interface assignment whether two service interfaces match. Matching Service Interfaces Two service interfaces match each other if the following conditions are fulfilled: o One service interface is of the outbound category and the other service interface if of the inbound category. Neither'),\n",
              " Document(metadata={}, page_content='if of the inbound category. Neither of the service interfaces can be abstract. o Both of the service interfaces have the same interface pattern. o There is a matching operation in the inbound service interface for each of the operations in the outbound service interface. Matching Operations An'),\n",
              " Document(metadata={}, page_content='interface. Matching Operations An inbound service interface operation matches an outbound service interface operation (and the other way around) if the following conditions are met: o Both operations must have the name mode (asynchronous or synchronous). o Both operations must have the same'),\n",
              " Document(metadata={}, page_content='o Both operations must have the same Operation Pattern. o The message type for the request, which must be referenced by each operation, must have the same name and same XML Namespace. The names of the operations may differ. The same applies for the response with synchronous communication. o If the'),\n",
              " Document(metadata={}, page_content='synchronous communication. o If the inbound service interface operation references a fault message type, the outbound service interface operation must also reference a fault message type with the same name and XML Namespace. o The data types of the message types, which the outbound service'),\n",
              " Document(metadata={}, page_content='types, which the outbound service interface for the request message references (and, if necessary, for the response and fault message) must be compatible with the corresponding inbound service interface data types. Matching Data Types The check whether the corresponding data types are compatible'),\n",
              " Document(metadata={}, page_content='corresponding data types are compatible with each other is sufficient until the comparison of the Facets of an XSD type. Unit-4 – Programming, Management console and Storage on AWS 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services The data types are compared'),\n",
              " Document(metadata={}, page_content='Services The data types are compared using the same method as other objects: The structures are compatible if they contain the same fields (elements and attributes) and if these fields have compatible types, frequencies, details, and default values. There are however a few restraints, for example'),\n",
              " Document(metadata={}, page_content='however a few restraints, for example the target structure can contain attributes or elements that do not appear in the outbound structure, but if these are not required and where the frequency is optional or prohibited (attributes) or minOccurs=0 (elements). o The data structures compared must'),\n",
              " Document(metadata={}, page_content='o The data structures compared must both be correct. For example, not all correct facets are skipped or considered in the compatibility check. o Some XSD schema language elements that can appear in a reference to an external message in the data structure are not supported. Therefore, the elements'),\n",
              " Document(metadata={}, page_content='not supported. Therefore, the elements redefine and any, for example, as well as the attributes blockDefault, finalDefault, and substitutionGroup. o The comparison of structures is, for example, restricted to the following: \\uf0a7 The details white Space and pattern are not checked \\uf0a7 If the facet'),\n",
              " Document(metadata={}, page_content='pattern are not checked \\uf0a7 If the facet pattern is used for the outbound structure field, all the other details are not checked. \\uf0a7 If the order of sub elements if different between the outbound and target field, a warning is displayed. Elastic Block Store Amazon Elastic Block Store is an AWS block'),\n",
              " Document(metadata={}, page_content='Elastic Block Store is an AWS block storage system that is best used for storing persistent data. Often incorrectly referred to as Elastic Block Storage, Amazon EBS provides highly available block level storage volumes for use with Amazon Elastic Compute Cloud (EC2) instances. An EC2 instance is a'),\n",
              " Document(metadata={}, page_content=\"(EC2) instances. An EC2 instance is a virtual server in Amazon's Elastic Compute Cloud (EC2) for running applications on the Amazon Web Services (AWS) infrastructure. To begin, create an EBS volume (General Purpose, Provisioned IOPS or Magnetic), pick a size for it (up to a terabyte of data) and\"),\n",
              " Document(metadata={}, page_content='for it (up to a terabyte of data) and attach that to any one of your EC2 instances. An EBS volume can only be attached to one instance at a time but if you need to have multiple copies of the volume, you can take a snapshot and create another volume from that snapshot and attach it to another'),\n",
              " Document(metadata={}, page_content='that snapshot and attach it to another drive. A snapshot file is equivalent to a backup of whatever the EBS volume looks like at the time. For every snapshot you create, you can make an identical EC2 instance. This will allow you to publish identical content on multiple servers. Amazon EBS is ideal'),\n",
              " Document(metadata={}, page_content='multiple servers. Amazon EBS is ideal if you’re doing any substantial work with EC2, you want to keep data persistently on a file system, and you want to keep that data around even after you shut down your EC2 instance. EC2 instances have local storage that you can use as long as you’re running the'),\n",
              " Document(metadata={}, page_content='can use as long as you’re running the instance, but as soon as you shut down the instance you lose the data that was on there. If you want to save anything, you need to save it on Amazon EBS. Because EC2 is like having a local drive on the machine, you can access and read the EBS volumes anytime'),\n",
              " Document(metadata={}, page_content='access and read the EBS volumes anytime once you attach the file to an EC2 instance. Amazon Simple Storage Service (S3) Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. Amazon S3 is intentionally built'),\n",
              " Document(metadata={}, page_content='web. Amazon S3 is intentionally built with a minimal feature set that focuses on simplicity and robustness. Following are some of advantages of the Amazon S3 service: Unit-4 – Programming, Management console and Storage on AWS 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure'),\n",
              " Document(metadata={}, page_content='| 2180712 – Cloud Infrastructure and Services o Create Buckets – Create and name a bucket that stores data. Buckets are the fundamental container in Amazon S3 for data storage. o Store data in Buckets – Store an infinite amount of data in a bucket. Upload as many objects as you like into an Amazon'),\n",
              " Document(metadata={}, page_content='many objects as you like into an Amazon S3 bucket. Each object can contain up to 5 TB of data. Each object is stored and retrieved using a unique developer-assigned key. o Download data – Download your data any time you like or allow others to do the same. o Permissions – Grant or deny access to'),\n",
              " Document(metadata={}, page_content='o Permissions – Grant or deny access to others who want to upload or download data into your Amazon S3 bucket. o Standard interfaces – Use standards-based REST and SOAP interfaces designed to work with any Internet-development toolkit. Amazon S3 Application Programming Interfaces (API) The Amazon'),\n",
              " Document(metadata={}, page_content='Programming Interfaces (API) The Amazon S3 architecture is designed to be programming language-neutral, using their supported interfaces to store and retrieve objects. Amazon S3 provides a REST and a SOAP interface. They are similar, but there are some differences. For example, in the REST'),\n",
              " Document(metadata={}, page_content='differences. For example, in the REST interface, metadata is returned in HTTP headers. Because we only support HTTP requests of up to 4 KB (not including the body), the amount of metadata you can supply is restricted. The REST Interface The REST API is an HTTP interface to Amazon S3. Using REST,'),\n",
              " Document(metadata={}, page_content='interface to Amazon S3. Using REST, you use standard HTTP requests to create, fetch, and delete buckets and objects. You can use any toolkit that supports HTTP to use the REST API. You can even use a browser to fetch objects, as long as they are anonymously readable. The REST API uses the standard'),\n",
              " Document(metadata={}, page_content='The REST API uses the standard HTTP headers and status codes, so that standard browsers and toolkits work as expected. In some areas, they have added functionality to HTTP (for example, we added headers to support access control). The SOAP Interface SOAP support over HTTP is deprecated, but it is'),\n",
              " Document(metadata={}, page_content='over HTTP is deprecated, but it is still available over HTTPS. New Amazon S3 features will not be supported for SOAP. The SOAP API provides a SOAP 1.1 interface using document literal encoding. The most common way to use SOAP is to download the WSDL, and use a SOAP toolkit such as Apache Axis or'),\n",
              " Document(metadata={}, page_content='a SOAP toolkit such as Apache Axis or Microsoft.NET to create bindings, and then write code that uses the bindings to call Amazon S3. Operations we can execute through API Login into Amazon S3. Uploading. Retrieving. Deleting etc. Amazon Glacier (Now Amazon S3 Glacier) - Content Delivery Platforms'),\n",
              " Document(metadata={}, page_content='Glacier) - Content Delivery Platforms Amazon Glacier is an extremely low-cost storage service that provides secure, durable, and flexible storage for data backup and archival. With Amazon Glacier, customers can reliably store their data for as little as $0.004 per gigabyte per month. Unit-4 –'),\n",
              " Document(metadata={}, page_content='$0.004 per gigabyte per month. Unit-4 – Programming, Management console and Storage on AWS 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Amazon Glacier enables customers to offload the administrative burdens of operating and scaling storage to AWS, so that'),\n",
              " Document(metadata={}, page_content='and scaling storage to AWS, so that they don’t have to worry about capacity planning, hardware provisioning, data replication, hardware failure detection and repair, or time-consuming hardware migrations. Amazon Glacier enables any business or organization to easily and cost effectively retain data'),\n",
              " Document(metadata={}, page_content='easily and cost effectively retain data for months, years, or decades. With Amazon Glacier, customers can now cost effectively retain more of their data for future analysis or reference, and they can focus on their business rather than operating and maintaining their storage infrastructure.'),\n",
              " Document(metadata={}, page_content='their storage infrastructure. Customers seeking compliance storage can deploy compliance controls using Vault Lock to meet regulatory and compliance archiving requirements. Benefits of Glacier Storage Service. 1. Retrievals as Quick as 1-5 Minutes Amazon Glacier provides three retrieval options to'),\n",
              " Document(metadata={}, page_content='provides three retrieval options to fit your use case. Expedited retrievals typically return data in 1-5 minutes, and are great for Active Archive use cases. Standard retrievals typically complete between 3-5 hours’ work, and work well for less time-sensitive needs like backup data, media editing,'),\n",
              " Document(metadata={}, page_content='needs like backup data, media editing, or long-term analytics. Bulk retrievals are the lowest-cost retrieval option, returning large amounts of data within 5-12 hours. 2. Unmatched Durability & Scalability Amazon Glacier runs on the world’s largest global cloud infrastructure, and was designed for'),\n",
              " Document(metadata={}, page_content='infrastructure, and was designed for 99.999999999% of durability. Data is automatically distributed across a minimum of three physical Availability Zones that are geographically separated within an AWS Region, and Amazon Glacier can also automatically replicate data to any other AWS Region. 3. Most'),\n",
              " Document(metadata={}, page_content='data to any other AWS Region. 3. Most Comprehensive Security & Compliance Capabilities Amazon Glacier offers sophisticated integration with AWS CloudTrail to log, monitor and retain storage API call activities for auditing, and supports three different forms of encryption. Amazon Glacier also'),\n",
              " Document(metadata={}, page_content='of encryption. Amazon Glacier also supports security standards and compliance certifications including SEC Rule 17a-4, PCI-DSS, HIPAA/HITECH, FedRAMP, EU Data Protection Directive, and FISMA, and Amazon Glacier Vault Lock enables WORM storage capabilities, helping satisfy compliance requirements'),\n",
              " Document(metadata={}, page_content='helping satisfy compliance requirements for virtually every regulatory agency around the globe. 4. Low Cost Amazon Glacier is designed to be the lowest cost AWS object storage class, allowing you to archive large amounts of data at a very low cost. This makes it feasible to retain all the data you'),\n",
              " Document(metadata={}, page_content='it feasible to retain all the data you want for use cases like data lakes, analytics, IoT, machine learning, compliance, and media asset archiving. You pay only for what you need, with no minimum commitments or up-front fees. 5. Most Supported Platform with the Largest Ecosystem In addition to'),\n",
              " Document(metadata={}, page_content='the Largest Ecosystem In addition to integration with most AWS services, the Amazon object storage ecosystem includes tens of thousands of consulting, systems integrator and independent software vendor partners, with more joining every month. And the AWS Marketplace offers 35 categories and more'),\n",
              " Document(metadata={}, page_content='offers 35 categories and more than 3,500 software listings from over 1,100 ISVs that are pre-configured to deploy on the AWS Cloud. AWS Partner Network partners have adapted their services and software to work with Amazon S3 and Amazon Glacier for solutions like Backup & Recovery, Archiving, and'),\n",
              " Document(metadata={}, page_content='like Backup & Recovery, Archiving, and Disaster Recovery. No other cloud provider has more partners with solutions that are pre-integrated to work with their service. Unit-4 – Programming, Management console and Storage on AWS 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure'),\n",
              " Document(metadata={}, page_content='| 2180712 – Cloud Infrastructure and Services 6. Query in Place Amazon Glacier is the only cloud archive storage service that allows you to query data in place and retrieve only the subset of data you need from within an archive. Amazon Glacier Select helps you reduce the total cost of ownership by'),\n",
              " Document(metadata={}, page_content='reduce the total cost of ownership by extending your data lake into cost-effective archive storage. Unit-5 – AWS identity services, security and compliance 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Identity Management and Access Management (IAM) Identity'),\n",
              " Document(metadata={}, page_content='and Access Management (IAM) Identity and access management (IAM) is a framework for business processes that facilitates the management of electronic or digital identities. The framework includes the organizational policies for managing digital identity as well as the technologies needed to support'),\n",
              " Document(metadata={}, page_content='as the technologies needed to support identity management. With IAM technologies, IT managers can control user access to critical information within their organizations. Identity and access management products offer role-based access control, which lets system administrators regulate access to'),\n",
              " Document(metadata={}, page_content='administrators regulate access to systems or networks based on the roles of individual users within the enterprise. In this context, access is the ability of an individual user to perform a specific task, such as view, create or modify a file. Roles are defined according to job competency,'),\n",
              " Document(metadata={}, page_content='defined according to job competency, authority and responsibility within the enterprise. Systems used for identity and access management include single sign-on systems, multifactor authentication and access management. These technologies also provide the ability to securely store identity and'),\n",
              " Document(metadata={}, page_content='ability to securely store identity and profile data as well as data governance functions to ensure that only data that is necessary and relevant is shared. These products can be deployed on premises, provided by a third party vendor via a cloud-based subscription model or deployed in a hybrid'),\n",
              " Document(metadata={}, page_content='model or deployed in a hybrid cloud. How Does IAM Work? The IAM workflow includes the following six elements: 1. A principal is an entity that can perform actions on an AWS resource. A user, a role or an application can be a principal. 2. Authentication is the process of confirming the identity of'),\n",
              " Document(metadata={}, page_content='process of confirming the identity of the principal trying to access an AWS product. The principal must provide its credentials or required keys for authentication. 3. Request: A principal sends a request to AWS specifying the action and which resource should perform it. 4. Authorization: By'),\n",
              " Document(metadata={}, page_content='should perform it. 4. Authorization: By default, all resources are denied. IAM authorizes a request only if all parts of the request are allowed by a matching policy. After authenticating and authorizing the request, AWS approves the action. 5. Actions are used to view, create, edit or delete a'),\n",
              " Document(metadata={}, page_content='used to view, create, edit or delete a resource. 6. Resources: A set of actions can be performed on a resource related to your AWS account. Identities (Users, Groups, and Roles) IAM identities, which you create to provide authentication for people and processes in your AWS account. IAM groups,'),\n",
              " Document(metadata={}, page_content='in your AWS account. IAM groups, which are collections of IAM users that you can manage as a unit. Identities represent the user, and can be authenticated and then authorized to perform actions in AWS. Each of these can be associated with one or more policies to determine what actions a user, role,'),\n",
              " Document(metadata={}, page_content='to determine what actions a user, role, or member of a group can do with which AWS resources and under what conditions. The AWS Account Root User When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and'),\n",
              " Document(metadata={}, page_content='complete access to all AWS services and resources in the account. Unit-5 – AWS identity services, security and compliance 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services This identity is called the AWS account root user and is accessed by signing in with the'),\n",
              " Document(metadata={}, page_content='and is accessed by signing in with the email address and password that you used to create the account. IAM Users An IAM user is an entity that you create in AWS. The IAM user represents the person or service who uses the IAM user to interact with AWS. A primary use for IAM users is to give people'),\n",
              " Document(metadata={}, page_content='use for IAM users is to give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI. A user in AWS consists of a name, a password to sign into the AWS Management Console, and up to two access keys that'),\n",
              " Document(metadata={}, page_content='Console, and up to two access keys that can be used with the API or CLI. When you create an IAM user, you grant it permissions by making it a member of a group that has appropriate permission policies attached (recommended), or by directly attaching policies to the user. You can also clone the'),\n",
              " Document(metadata={}, page_content='to the user. You can also clone the permissions of an existing IAM user, which automatically makes the new user a member of the same groups and attaches all the same policies. IAM Groups An IAM group is a collection of IAM users. You can use groups to specify permissions for a collection of users,'),\n",
              " Document(metadata={}, page_content='permissions for a collection of users, which can make those permissions easier to manage for those users. For example, you could have a group called Admins and give that group the types of permissions that administrators typically need. Any user in that group automatically has the permissions that'),\n",
              " Document(metadata={}, page_content='automatically has the permissions that are assigned to the group. If a new user joins your organization and should have administrator privileges, you can assign the appropriate permissions by adding the user to that group. Similarly, if a person changes jobs in your organization, instead of editing'),\n",
              " Document(metadata={}, page_content=\"your organization, instead of editing that user's permissions, you can remove him or her from the old groups and add him or her to the appropriate new groups. Note that a group is not truly an identity because it cannot be identified as a Principal in a resource-based or trust policy. It is only a\"),\n",
              " Document(metadata={}, page_content='or trust policy. It is only a way to attach policies to multiple users at one time. IAM Roles An IAM role is very similar to a user, in that it is an identity with permission policies that determine what the identity can and cannot do in AWS. However, a role does not have any credentials (password'),\n",
              " Document(metadata={}, page_content='does not have any credentials (password or access keys) associated with it. Instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. An IAM user can assume a role to temporarily take on different permissions for a specific task. A role can be'),\n",
              " Document(metadata={}, page_content='for a specific task. A role can be assigned to a federated user who signs in by using an external identity provider instead of IAM. AWS uses details passed by the identity provider to determine which role is mapped to the federated user. Temporary Credentials Temporary credentials are primarily'),\n",
              " Document(metadata={}, page_content='Temporary credentials are primarily used with IAM roles, but there are also other uses. You can request temporary credentials that have a more restricted set of permissions than your standard IAM user. This prevents you from accidentally performing tasks that are not permitted by the more'),\n",
              " Document(metadata={}, page_content='that are not permitted by the more restricted credentials. Unit-5 – AWS identity services, security and compliance 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services A benefit of temporary credentials is that they expire automatically after a set period of time.'),\n",
              " Document(metadata={}, page_content='after a set period of time. You have control over the duration that the credentials are valid. Security Policies You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated'),\n",
              " Document(metadata={}, page_content='an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON'),\n",
              " Document(metadata={}, page_content='Most policies are stored in AWS as JSON documents. AWS supports six types of policies. IAM policies define permissions for an action regardless of the method that you use to perform the operation. For example, if a policy allows the GetUser action, then a user with that policy can get user'),\n",
              " Document(metadata={}, page_content='a user with that policy can get user information from the AWS Management Console, the AWS CLI, or the AWS API. When you create an IAM user, you can choose to allow console or programmatic access. If console access is allowed, the IAM user can sign in to the console using a user name and password.'),\n",
              " Document(metadata={}, page_content='console using a user name and password. Or if programmatic access is allowed, the user can use access keys to work with the CLI or API. Policy Types Identity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based'),\n",
              " Document(metadata={}, page_content='users belong, or roles). Identity-based policies grant permissions to an identity. Resource-based policies – Attach inline policies to resources. The most common examples of resource- based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions'),\n",
              " Document(metadata={}, page_content='policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. Permissions boundaries – Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum'),\n",
              " Document(metadata={}, page_content='role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity. Organizations SCPs – Use an AWS Organizations'),\n",
              " Document(metadata={}, page_content='SCPs – Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account,'),\n",
              " Document(metadata={}, page_content='(users or roles) within the account, but do not grant permissions. Access control lists (ACLs) – Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does'),\n",
              " Document(metadata={}, page_content='they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account. Session policies – Pass advanced session policies'),\n",
              " Document(metadata={}, page_content=\"– Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user's identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant\"),\n",
              " Document(metadata={}, page_content='for a created session, but do not grant permissions. For more information, see Session Policies. Unit-5 – AWS identity services, security and compliance 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services IAM Abilities/Features Shared access to the AWS account. The'),\n",
              " Document(metadata={}, page_content='Shared access to the AWS account. The main feature of IAM is that it allows you to create separate usernames and passwords for individual users or resources and delegate access. Granular permissions. Restrictions can be applied to requests. For example, you can allow the user to download'),\n",
              " Document(metadata={}, page_content='you can allow the user to download information, but deny the user the ability to update information through the policies. Multifactor authentication (MFA). IAM supports MFA, in which users provide their username and password plus a one-time password from their phone—a randomly generated number used'),\n",
              " Document(metadata={}, page_content='phone—a randomly generated number used as an additional authentication factor. Identity Federation. If the user is already authenticated, such as through a Facebook or Google account, IAM can be made to trust that authentication method and then allow access based on it. This can also be used to'),\n",
              " Document(metadata={}, page_content='based on it. This can also be used to allow users to maintain just one password for both on-premises and cloud environment work. Free to use. There is no additional charge for IAM security. There is no additional charge for creating additional users, groups or policies. PCI DSS compliance. The'),\n",
              " Document(metadata={}, page_content='or policies. PCI DSS compliance. The Payment Card Industry Data Security Standard is an information security standard for organizations that handle branded credit cards from the major card schemes. IAM complies with this standard. Password policy. The IAM password policy allows you to reset a'),\n",
              " Document(metadata={}, page_content='password policy allows you to reset a password or rotate passwords remotely. You can also set rules, such as how a user should pick a password or how many attempts a user may make to provide a password before being denied access. IAM Limitations Names of all IAM identities and IAM resources can be'),\n",
              " Document(metadata={}, page_content=\"IAM identities and IAM resources can be alphanumeric. They can include common characters such as plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). Names of IAM identities (users, roles, and groups) must be unique within the AWS account. So you can't have two groups\"),\n",
              " Document(metadata={}, page_content=\"account. So you can't have two groups named DEVELOPERS and developers in your AWS account. AWS account ID aliases must be unique across AWS products in your account. It cannot be a 12 digit number. You cannot create more than 100 groups in an AWS account. You cannot create more than 5000 users in\"),\n",
              " Document(metadata={}, page_content='cannot create more than 5000 users in an AWS account. AWS recommends the use of temporary security credentials for adding a large number of users in an AWS account. You cannot create more than 500 roles in an AWS account. An IAM user cannot be a member of more than 10 groups. An IAM user cannot be'),\n",
              " Document(metadata={}, page_content='than 10 groups. An IAM user cannot be assigned more than 2 access keys. An AWS account cannot have more than 1000 customer managed policies. You cannot attach more than 10 managed policies to each IAM entity (user, groups, or roles). You cannot store more than 20 server certificates in an AWS'),\n",
              " Document(metadata={}, page_content='than 20 server certificates in an AWS account. You cannot have more than 100 SAML providers in an AWS account. A policy name should not exceed 128 characters. An alias for an AWS account ID should be between 3 and 63 characters. A username and role name should not exceed 64 characters. A group name'),\n",
              " Document(metadata={}, page_content='not exceed 64 characters. A group name should not exceed 128 characters. AWS Physical and Environmental Security AWS data centers are state of the art, utilizing innovative architectural and engineering approaches. Unit-5 – AWS identity services, security and compliance 5 Prof. Vijay M. Shekhat, CE'),\n",
              " Document(metadata={}, page_content='compliance 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Amazon has many years of experience in designing, constructing, and operating large-scale data centers. This experience has been applied to the AWS platform and infrastructure. AWS data centers are'),\n",
              " Document(metadata={}, page_content='infrastructure. AWS data centers are housed in facilities that are not branded as AWS facilities. Physical access is strictly controlled both at the perimeter and at building ingress points by professional security staff utilizing video surveillance, intrusion detection systems, and other'),\n",
              " Document(metadata={}, page_content='intrusion detection systems, and other electronic means. Authorized staff must pass two-factor authentication a minimum of two times to access data center floors. All visitors are required to present identification and are signed in and continually escorted by authorized staff. AWS only provides'),\n",
              " Document(metadata={}, page_content='by authorized staff. AWS only provides data center access and information to employees and contractors who have a legitimate business need for such privileges. When an employee no longer has a business need for these privileges, his or her access is immediately revoked, even if they continue to be'),\n",
              " Document(metadata={}, page_content='revoked, even if they continue to be an employee of Amazon or Amazon Web Services. All physical access to data centers by AWS employees is logged and audited routinely. Fire Detection and Suppression Automatic fire detection and suppression equipment has been installed to reduce risk. The fire'),\n",
              " Document(metadata={}, page_content='been installed to reduce risk. The fire detection system utilizes smoke detection sensors in all data center environments, mechanical and electrical infrastructure spaces, chiller rooms and generator equipment rooms. These areas are protected by either wet-pipe, double interlocked pre-action, or'),\n",
              " Document(metadata={}, page_content='double interlocked pre-action, or gaseous sprinkler systems. Power The data center electrical power systems are designed to be fully redundant and maintainable without impact to operations, 24 hours a day, and seven days a week. Uninterruptible Power Supply (UPS) units provide back-up power in the'),\n",
              " Document(metadata={}, page_content='units provide back-up power in the event of an electrical failure for critical and essential loads in the facility. Data centers use generators to provide back-up power for the entire facility. Climate and Temperature Climate control is required to maintain a constant operating temperature for'),\n",
              " Document(metadata={}, page_content='a constant operating temperature for servers and other hardware, which prevents overheating and reduces the possibility of service outages. Data centers are conditioned to maintain atmospheric conditions at optimal levels. Personnel and systems monitor and control temperature and humidity at'),\n",
              " Document(metadata={}, page_content='and control temperature and humidity at appropriate levels. Management AWS monitors electrical, mechanical, and life support systems and equipment so that any issues are immediately identified. Preventative maintenance is performed to maintain the continued operability of equipment. Storage Device'),\n",
              " Document(metadata={}, page_content='of equipment. Storage Device Decommissioning When a storage device has reached the end of its useful life, AWS procedures include a decommissioning process that is designed to prevent customer data from being exposed to unauthorized individuals. AWS uses the techniques detailed in NIST 800-88'),\n",
              " Document(metadata={}, page_content='the techniques detailed in NIST 800-88 (“Guidelines for Media Sanitization”) as part of the decommissioning process. Unit-5 – AWS identity services, security and compliance 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS Compliance Initiatives AWS'),\n",
              " Document(metadata={}, page_content='Services AWS Compliance Initiatives AWS Compliance enables customers to understand the robust controls in place at AWS to maintain security and data protection in the cloud. As systems are built on top of AWS cloud infrastructure, compliance responsibilities are shared. By tying together'),\n",
              " Document(metadata={}, page_content='are shared. By tying together governance-focused, audit friendly service features with applicable compliance or audit standards, AWS Compliance enablers build on traditional programs; helping customers to establish and operate in an AWS security control environment. The IT infrastructure that AWS'),\n",
              " Document(metadata={}, page_content='The IT infrastructure that AWS provides to its customers is designed and managed in alignment with security best practices and a variety of IT security standards, including: o SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70) o SOC 2 o SOC 3 o FISMA, DIACAP, and FedRAMP o DOD CSM Levels 1-5 o PCI DSS Level'),\n",
              " Document(metadata={}, page_content='o DOD CSM Levels 1-5 o PCI DSS Level 1 o ISO 9001 / ISO 27001 / ISO 27017 / ISO 27018 o ITAR o FIPS 140-2 o MTCS Level 3 o HITRUST In addition, the flexibility and control that the AWS platform provides allows customers to deploy solutions that meet several industry-specific standards, including: o'),\n",
              " Document(metadata={}, page_content='standards, including: o Criminal Justice Information Services (CJIS) o Cloud Security Alliance (CSA) o Family Educational Rights and Privacy Act (FERPA) o Health Insurance Portability and Accountability Act (HIPAA) o Motion Picture Association of America (MPAA) AWS provides a wide range of'),\n",
              " Document(metadata={}, page_content='(MPAA) AWS provides a wide range of information regarding its IT control environment to customers through white papers, reports, certifications, accreditations, and other third party attestations. Understanding Public/Private Keys Amazon AWS uses keys to encrypt and decrypt login information. At'),\n",
              " Document(metadata={}, page_content='and decrypt login information. At the basic level, a sender uses a public key to encrypt data, which its receiver then decrypts using another private key. These two keys, public and private, are known as a key pair. You need a key pair to be able to connect to your instances. The way this works on'),\n",
              " Document(metadata={}, page_content='your instances. The way this works on Linux and Windows instances is different. First, when you launch a new instance, you assign a key pair to it. Then, when you log in to it, you use the private key. The difference between Linux and Windows instances is that Linux instances do not have a password'),\n",
              " Document(metadata={}, page_content='Linux instances do not have a password already set and you must use the key pair to log in to Linux instances. On the other hand, on Windows instances, you need the key pair to decrypt the administrator password. Using the decrypted password, you can use RDP and then connect to your Windows'),\n",
              " Document(metadata={}, page_content='RDP and then connect to your Windows instance. Amazon EC2 stores only the public key, and you can either generate it inside Amazon EC2 or you can import it. Unit-5 – AWS identity services, security and compliance 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services'),\n",
              " Document(metadata={}, page_content='– Cloud Infrastructure and Services Since the private key is not stored by Amazon, it’s advisable to store it in a secure place as anyone who has this private key can log in on your behalf. AWS API Security API Gateway supports multiple mechanisms of access control, including metering or tracking'),\n",
              " Document(metadata={}, page_content='control, including metering or tracking API uses by clients using API keys. The standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API set or individual methods. Custom authorizers and Amazon Cognito user pools provide customizable'),\n",
              " Document(metadata={}, page_content='Cognito user pools provide customizable authorization and authentication solutions. A. Control Access to an API with IAM Permissions You control access to Amazon API Gateway with IAM permissions by controlling access to the following two API Gateway component processes: o To create, deploy, and'),\n",
              " Document(metadata={}, page_content='processes: o To create, deploy, and manage an API in API Gateway, you must grant the API developer permissions to perform the required actions supported by the API management component of API Gateway. o To call a deployed API or to refresh the API caching, you must grant the API caller permissions'),\n",
              " Document(metadata={}, page_content='must grant the API caller permissions to perform required IAM actions supported by the API execution component of API Gateway. B. Use API Gateway Custom Authorizers An Amazon API Gateway custom authorizer is a Lambda function that you provide to control access to your API methods. A custom'),\n",
              " Document(metadata={}, page_content='access to your API methods. A custom authorizer uses bearer token authentication strategies, such as OAuth or SAML. It can also use information described by headers, paths, query strings, stage variables, or context variables request parameters. When a client calls your API, API Gateway verifies'),\n",
              " Document(metadata={}, page_content='calls your API, API Gateway verifies whether a custom authorizer is configured for the API method. If so, API Gateway calls the Lambda function. In this call, API Gateway supplies the authorization token that is extracted from a specified request header for the token-based authorizer, or passes in'),\n",
              " Document(metadata={}, page_content='token-based authorizer, or passes in the incoming request parameters as the input (for example, the event parameter) to the request parameters-based authorizer function. You can implement various authorization strategies, such as JSON Web Token (JWT) verification and OAuth provider callout. You can'),\n",
              " Document(metadata={}, page_content='and OAuth provider callout. You can also implement a custom scheme based on incoming request parameter values, to return IAM policies that authorize the request. If the returned policy is invalid or the permissions are denied, the API call does not succeed. C. Use Amazon Cognito User Pools In'),\n",
              " Document(metadata={}, page_content='C. Use Amazon Cognito User Pools In addition to using IAM roles and policies or custom authorizers, you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway. To use an Amazon Cognito user pool with your API, you must first create an authorizer of the'),\n",
              " Document(metadata={}, page_content='must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the'),\n",
              " Document(metadata={}, page_content=\"call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have\"),\n",
              " Document(metadata={}, page_content='call because the client did not have credentials that could be authorized. Unit-5 – AWS identity services, security and compliance 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services D. Use Client-Side SSL Certificates for Authentication by the Backend You can use'),\n",
              " Document(metadata={}, page_content='by the Backend You can use API Gateway to generate an SSL certificate and use its public key in the backend to verify that HTTP requests to your backend system are from API Gateway. This allows your HTTP backend to control and accept only requests originating from Amazon API Gateway, even if the'),\n",
              " Document(metadata={}, page_content='from Amazon API Gateway, even if the backend is publicly accessible. The SSL certificates that are generated by API Gateway are self-signed and only the public key of a certificate is visible in the API Gateway console or through the APIs. E. Create and Use API Gateway Usage Plans After you create,'),\n",
              " Document(metadata={}, page_content='Gateway Usage Plans After you create, test, and deploy your APIs, you can use API Gateway usage plans to extend them as product offerings for your customers. You can provide usage plans to allow specified customers to access selected APIs at agreed-upon request rates and quotas that can meet their'),\n",
              " Document(metadata={}, page_content='rates and quotas that can meet their business requirements and budget constraints. AWS Security, Identity, & Compliance services Category Use cases AWS service Identity & access management Securely manage access to services and resources AWS Identity & Access Management Cloud single-sign-on (SSO)'),\n",
              " Document(metadata={}, page_content='Management Cloud single-sign-on (SSO) service AWS Single Sign-On Identity management for your apps Amazon Cognito Managed Microsoft Active Directory AWS Directory Service Simple, secure service to share AWS resources AWS Resource Access Manager Detective controls Unified security and compliance'),\n",
              " Document(metadata={}, page_content='Unified security and compliance center AWS Security Hub Managed threat detection service Amazon GuardDuty Analyze application security Amazon Inspector Investigate potential security issues Amazon Detective Infrastructure protection DDoS protection AWS Shield Filter malicious web traffic AWS Web'),\n",
              " Document(metadata={}, page_content='Filter malicious web traffic AWS Web Application Firewall (WAF) Central management of firewall rules AWS Firewall Manager Data protection Discover and protect your sensitive data at scale Amazon Macie Key storage and management AWS Key Management Service (KMS) Hardware based key storage for'),\n",
              " Document(metadata={}, page_content='(KMS) Hardware based key storage for regulatory compliance AWS CloudHSM Provision, manage, and deploy public and private SSL/TLS certificates AWS Certificate Manager Rotate, manage, and retrieve secrets AWS Secrets Manager Compliance No cost, self-service portal for on-demand access to AWS’'),\n",
              " Document(metadata={}, page_content='portal for on-demand access to AWS’ compliance reports AWS Artifact Dark Web The dark web is a general term for the seedier corners of the web, where people can interact online without worrying about the watchful eye of the authorities. Usually, these sites are guarded by encryption mechanisms such'),\n",
              " Document(metadata={}, page_content=\"guarded by encryption mechanisms such as Tor that allow users to visit them anonymously. But there are also sites that don't rely on Tor, such as password-protected forums where hackers trade secrets and stolen credit card numbers, that can also be considered part of the dark web. Unit-5 – AWS\"),\n",
              " Document(metadata={}, page_content='part of the dark web. Unit-5 – AWS identity services, security and compliance 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services People use the dark web for a variety of purposes: buying and selling drugs, discussing hacking techniques and selling hacking services'),\n",
              " Document(metadata={}, page_content='techniques and selling hacking services and so forth. It\\'s important to remember that the technologies used to facilitate \"dark web\" activities aren\\'t inherently good or bad. The same technologies used by drug dealers to hide their identity can also be used by authorized informers to securely pass'),\n",
              " Document(metadata={}, page_content='authorized informers to securely pass information to government agencies. Unit-6 – AWS computing and marketplace 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Elastic Cloud Compute (EC2) Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides'),\n",
              " Document(metadata={}, page_content='EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete'),\n",
              " Document(metadata={}, page_content='friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment. Advantages of EC2 In less than 10 minutes you can rent a slice of Amazon’s vast cloud network and put those computing resources to work on anything from data'),\n",
              " Document(metadata={}, page_content='resources to work on anything from data science to bitcoin mining. EC2 offers a number of benefits and advantages over alternatives. Most notably: Affordability EC2 allows you to take advantage of Amazon’s enormous scale. You can pay a very low rate for the resources you use. The smallest EC2'),\n",
              " Document(metadata={}, page_content='the resources you use. The smallest EC2 instance can be rented for as little as $.0058 per hour which works out to about $4.18 per month. Of course, instances with more resources are more expensive but this gives you a sense of how affordable EC2 instances are. With EC2 instances, you’re only'),\n",
              " Document(metadata={}, page_content='are. With EC2 instances, you’re only paying for what you use in terms of compute hours and bandwidth so there’s little wasted expense. Ease of use Amazon’s goal with EC2 was to make accessing compute resources low friction and, by and large, they’ve succeeded. Launching an instance is simply a'),\n",
              " Document(metadata={}, page_content='Launching an instance is simply a matter of logging into the AWS Console, selecting your operating system, instance type, and storage options. At most, it’s a 10 minute process and there aren’t any major technical barriers preventing anyone from spinning up an instance, though it may take some'),\n",
              " Document(metadata={}, page_content='up an instance, though it may take some technical knowledge to leverage those resources after launch. Scalability You can easily add EC2 instances as needed, creating your own private cloud of computer resources that perfectly matches your needs. Here at Pagely a common configuration is an EC2'),\n",
              " Document(metadata={}, page_content='Pagely a common configuration is an EC2 instance to run a WordPress app, an instance to run RDS (a database service), and an EBS so that data can easily be moved and shared between instances as they’re added. AWS offers built-in, rules-based auto scaling so that you can automatically turn instances'),\n",
              " Document(metadata={}, page_content='you can automatically turn instances on or off based on demand. This helps you ensure that you’re never wasting resources but you also have enough resources available to do the job. Integration Perhaps the biggest advantage of EC2, and something no competing solution can claim, is its native'),\n",
              " Document(metadata={}, page_content='solution can claim, is its native integration with the vast ecosystem of AWS services. Unit-6 – AWS computing and marketplace 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Currently there are over 170 services. No other cloud network can claim the breadth,'),\n",
              " Document(metadata={}, page_content='cloud network can claim the breadth, depth, and flexibility AWS can. EC2 Image Builder EC2 Image Builder simplifies the creation, maintenance, validation, sharing, and deployment of Linux or Windows Server images for use with Amazon EC2 and on-premises. Keeping server images up-to-date can be time'),\n",
              " Document(metadata={}, page_content='server images up-to-date can be time consuming, resource intensive, and error-prone. Currently, customers either manually update and snapshot VMs or have teams that build automation scripts to maintain images. Image Builder significantly reduces the effort of keeping images up-to-date and secure by'),\n",
              " Document(metadata={}, page_content='keeping images up-to-date and secure by providing a simple graphical interface, built-in automation, and AWS-provided security settings. With Image Builder, there are no manual steps for updating an image nor do you have to build your own automation pipeline. Image Builder is offered at no cost,'),\n",
              " Document(metadata={}, page_content='Image Builder is offered at no cost, other than the cost of the underlying AWS resources used to create, store, and share the images. Auto Scaling AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost.'),\n",
              " Document(metadata={}, page_content='at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple services in minutes. The service provides a simple, powerful user interface that lets you build scaling plans for resources including Amazon EC2 instances and Spot'),\n",
              " Document(metadata={}, page_content='including Amazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon DynamoDB tables and indexes, and Amazon Aurora Replicas. AWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, costs, or balance between them. If you’re already using Amazon EC2'),\n",
              " Document(metadata={}, page_content='If you’re already using Amazon EC2 Auto Scaling to dynamically scale your Amazon EC2 instances, you can now combine it with AWS Auto Scaling to scale additional resources for other AWS services. With AWS Auto Scaling, your applications always have the right resources at the right time. It’s easy to'),\n",
              " Document(metadata={}, page_content='at the right time. It’s easy to get started with AWS Auto Scaling using the AWS Management Console, Command Line Interface (CLI), or SDK. AWS Auto Scaling is available at no additional charge. You pay only for the AWS resources needed to run your applications and Amazon CloudWatch monitoring fees.'),\n",
              " Document(metadata={}, page_content='and Amazon CloudWatch monitoring fees. Elastic Load Balancing Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single'),\n",
              " Document(metadata={}, page_content='of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancing offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant. o'),\n",
              " Document(metadata={}, page_content='your applications fault tolerant. o Application Load Balancers, o Network Load Balancers, and o Classic Load Balancers. Unit-6 – AWS computing and marketplace 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Application Load Balancer Application Load Balancer is'),\n",
              " Document(metadata={}, page_content='Balancer Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including micro services and containers. Operating at the individual request level (Layer 7),'),\n",
              " Document(metadata={}, page_content='the individual request level (Layer 7), Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request. Network Load Balancer Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is'),\n",
              " Document(metadata={}, page_content='traffic where extreme performance is required. Operating at the connection level (Layer 4), Network Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) and is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load'),\n",
              " Document(metadata={}, page_content='ultra-low latencies. Network Load Balancer is also optimized to handle sudden and volatile traffic patterns. Classic Load Balancer Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load'),\n",
              " Document(metadata={}, page_content='and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. Benefits of Elastic Load Balancing for reducing workload Highly Available Elastic Load Balancing automatically distributes incoming traffic across multiple targets – Amazon EC2'),\n",
              " Document(metadata={}, page_content='across multiple targets – Amazon EC2 instances, containers, and IP addresses – in multiple Availability Zones and ensures only healthy targets receive traffic. Elastic Load Balancing can also load balance across a Region, routing traffic to healthy targets in different Availability Zones. Secure'),\n",
              " Document(metadata={}, page_content='in different Availability Zones. Secure Elastic Load Balancing works with Amazon Virtual Private Cloud (VPC) to provide robust security features, including integrated certificate management and SSL decryption. Together, they give you the flexibility to centrally manage SSL settings and offload CPU'),\n",
              " Document(metadata={}, page_content='manage SSL settings and offload CPU intensive workloads from your applications. Elastic Elastic Load Balancing is capable of handling rapid changes in network traffic patterns. Additionally, deep integration with Auto Scaling ensures sufficient application capacity to meet varying levels of'),\n",
              " Document(metadata={}, page_content='capacity to meet varying levels of application load without requiring manual intervention. Flexible Elastic Load Balancing also allows you to use IP addresses to route requests to application targets. This offers you flexibility in how you virtualize your application targets, allowing you to host'),\n",
              " Document(metadata={}, page_content='targets, allowing you to host more applications on the same instance. This also enables these applications to have individual security groups and use the same network port to further simplify inter-application communication in microservices based architecture. Robust Monitoring and Auditing Elastic'),\n",
              " Document(metadata={}, page_content='Robust Monitoring and Auditing Elastic Load Balancing allows you to monitor your applications and their performance in real time with Amazon CloudWatch metrics, logging, and request tracing. This improves visibility into the behavior of Unit-6 – AWS computing and marketplace 4 Prof. Vijay M.'),\n",
              " Document(metadata={}, page_content='and marketplace 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services your applications, uncovering issues and identifying performance bottlenecks in your application stack at the granularity of an individual request. Hybrid Load Balancing Elastic Load Balancing'),\n",
              " Document(metadata={}, page_content='Load Balancing Elastic Load Balancing offers ability to load balance across AWS and on-premises resources using the same load balancer. This makes it easy for you to migrate, burst, or failover on-premises applications to the cloud. AMIs An Amazon Machine Image (AMI) provides the information'),\n",
              " Document(metadata={}, page_content='Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need'),\n",
              " Document(metadata={}, page_content='AMIs to launch instances when you need instances with different configurations. An AMI includes the following: o One or more EBS snapshots, or, for instance-store-backed AMIs, a template for the root volume of the instance (for example, an operating system, an application server, and applications).'),\n",
              " Document(metadata={}, page_content=\"application server, and applications). o Launch permissions that control which AWS accounts can use the AMI to launch instances. o A block device mapping that specifies the volumes to attach to the instance when it's launched. Using an AMI Fig.: The AMI lifecycle (create, register, launch, copy,\"),\n",
              " Document(metadata={}, page_content='(create, register, launch, copy, and deregister) The following diagram summarizes the AMI lifecycle. After you create and register an AMI, you can use it to launch new instances. (You can also launch instances from an AMI if the AMI owner grants you launch permissions.) You can copy an AMI within'),\n",
              " Document(metadata={}, page_content='You can copy an AMI within the same Region or to different Regions. When you no longer require an AMI, you can deregister it. You can search for an AMI that meets the criteria for your instance. You can search for AMIs provided by AWS or AMIs provided by the community. After you launch an instance'),\n",
              " Document(metadata={}, page_content='community. After you launch an instance from an AMI, you can connect to it. When you are connected to an instance, you can use it just like you use any other server. For information about launching, connecting, and using your instance, see Amazon EC2 instances. Unit-6 – AWS computing and'),\n",
              " Document(metadata={}, page_content='instances. Unit-6 – AWS computing and marketplace 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Multi Tenancy In cloud computing, multi tenancy means that multiple customers of a cloud vendor are using the same computing resources. Despite the fact that they'),\n",
              " Document(metadata={}, page_content=\"resources. Despite the fact that they share resources, cloud customers aren't aware of each other, and their data is kept totally separate. Multi tenancy is a crucial component of cloud computing; without it, cloud services would be far less practical. Multitenant architecture is a feature in many\"),\n",
              " Document(metadata={}, page_content='architecture is a feature in many types of public cloud computing, including IaaS, PaaS, SaaS, containers, and server less computing. To understand multi tenancy, think of how banking works. Multiple people can store their money in one bank, and their assets are completely separate even though'),\n",
              " Document(metadata={}, page_content=\"are completely separate even though they're stored in the same place. Customers of the bank don't interact with each other, don't have access to other customers' money, and aren't even aware of each other. Similarly, in public cloud computing, customers of the cloud vendor use the same\"),\n",
              " Document(metadata={}, page_content='of the cloud vendor use the same infrastructure – the same servers, typically – while still keeping their data and their business logic separate and secure. The classic definition of multi tenancy was a single software instance that served multiple users, or tenants. However, in modern cloud'),\n",
              " Document(metadata={}, page_content='or tenants. However, in modern cloud computing, the term has taken on a broader meaning, referring to shared cloud infrastructure instead of just a shared software instance. Cataloging the Marketplace AWS Marketplace is a curated digital catalog customers can use to find, buy, deploy, and manage'),\n",
              " Document(metadata={}, page_content='use to find, buy, deploy, and manage third- party software, data, and services that customers need to build solutions and run their businesses. AWS Marketplace includes thousands of software listings from popular categories such as security, networking, storage, machine learning, business'),\n",
              " Document(metadata={}, page_content='storage, machine learning, business intelligence, database, and DevOps. AWS Marketplace also simplifies software licensing and procurement with flexible pricing options and multiple deployment methods. In addition, AWS Marketplace includes data products available from AWS Data Exchange. Customers'),\n",
              " Document(metadata={}, page_content='from AWS Data Exchange. Customers can quickly launch preconfigured software with just a few clicks, and choose software solutions in Amazon Machine Images (AMIs), software as a service (SaaS), and other formats. You can browse and subscribe to data products. Flexible pricing options include free'),\n",
              " Document(metadata={}, page_content='Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL, and get billed from one source. AWS handles billing and payments, and charges appear on customers’ AWS bill. You can use AWS Marketplace as a buyer (subscriber), seller (provider), or both. Anyone with an'),\n",
              " Document(metadata={}, page_content='(provider), or both. Anyone with an AWS account can use AWS Marketplace as a buyer, and can register to become a seller. A seller can be an independent software vendor (ISV), value-added reseller, or individual who has something to offer that works with AWS products and services. Every software'),\n",
              " Document(metadata={}, page_content='products and services. Every software product on AWS Marketplace has been through a curation process. On the product page, there can be one or more offerings for the product. When the seller submits a product in AWS Marketplace, they define the price of the product and the terms and conditions of'),\n",
              " Document(metadata={}, page_content='product and the terms and conditions of use. Unit-6 – AWS computing and marketplace 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services When a consumer subscribes to a product offering, they agree to the pricing and terms and conditions set for the offer. The'),\n",
              " Document(metadata={}, page_content='and conditions set for the offer. The product can be free to use or it can have an associated charge. The charge becomes part of your AWS bill, and after you pay, AWS Marketplace pays the seller. Products can take many forms. For example, a product can be offered as an Amazon Machine Image (AMI)'),\n",
              " Document(metadata={}, page_content='as an Amazon Machine Image (AMI) that is instantiated using your AWS account. The product can also be configured to use AWS CloudFormation templates for delivery to the consumer. The product can also be software as a service (SaaS) offerings from an ISV, web ACL, set of rules, or conditions for AWS'),\n",
              " Document(metadata={}, page_content='set of rules, or conditions for AWS WAF. Software products can be purchased at the listed price using the ISV’s standard end user license agreement (EULA) or offered with customer pricing and EULA. Products can also be purchased under a contract with specified time or usage boundaries. After the'),\n",
              " Document(metadata={}, page_content='time or usage boundaries. After the product subscriptions are in place, the consumer can copy the product to their AWS Service Catalog to manage how the product is accessed and used in the consumer’s organization. Selling On the Marketplace. As a seller, go to the AWS Marketplace Management Portal'),\n",
              " Document(metadata={}, page_content=\"the AWS Marketplace Management Portal to register. If you're providing a data product or you're charging for use of your software product, you must also provide tax and banking information as part of your registration. When you register, you create a profile for your company or for yourself that is\"),\n",
              " Document(metadata={}, page_content='your company or for yourself that is discoverable on AWS Marketplace. You also use the AWS Marketplace Management Portal to create and manage product pages for your products. Eligible partners can programmatically list AWS Marketplace products outside of AWS Marketplace. For information about'),\n",
              " Document(metadata={}, page_content='AWS Marketplace. For information about becoming an eligible partner, contact your AWS Marketplace business development partner. Unit-7 – AWS networking and databases 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Virtual private clouds A virtual private cloud'),\n",
              " Document(metadata={}, page_content='private clouds A virtual private cloud (VPC) is an on-demand configurable pool of shared computing resources allocated within a public cloud environment, providing a certain level of isolation between the different organizations. You already know that there are three major types of clouds: Public,'),\n",
              " Document(metadata={}, page_content='three major types of clouds: Public, Private and Hybrid. Now, there’s a newer player in the game: Virtual Private Clouds. What makes these different from public and private clouds, and what is the benefit? Is it just a fancy name for public cloud, or is it a private one? VPC are related to the'),\n",
              " Document(metadata={}, page_content='a private one? VPC are related to the public cloud, but they are not the same. Instead of sharing resources and space in a public infrastructure, you get a changeable allotment of resources to configure. There is a certain level of isolation between you and other users, via a private IP subnet and'),\n",
              " Document(metadata={}, page_content='users, via a private IP subnet and virtual communication construct (such as a VLAN) on a per user basis. This ensures a secure method of remotely accessing your cloud resources. This isolation within a public cloud lends the name “virtual private” because you are essentially operating a private'),\n",
              " Document(metadata={}, page_content='you are essentially operating a private cloud within a public cloud. That also doesn’t mean Virtual Private Clouds and private clouds are the same. Private clouds are entirely dedicated to your organization, and that includes the hardware. Virtual Private clouds do not have the same hardware'),\n",
              " Document(metadata={}, page_content='clouds do not have the same hardware dedication; it just creates a more secure environment on public infrastructure. Think of it as operating like a VPN: You use them to send messages over the public internet in a secure way as if you had your own personal network, but it’s not the same as actually'),\n",
              " Document(metadata={}, page_content='but it’s not the same as actually having your own. What’s the benefit to this? Wouldn’t it just be easier to have a private cloud? Not necessarily. Private clouds are expensive to operate, and because the hardware as well as the resources required to run it belong to you alone, there is no one to'),\n",
              " Document(metadata={}, page_content='belong to you alone, there is no one to share that cost with. Virtual Private Clouds give you the best of both worlds: A private cloud for security and compliance purposes, reduced infrastructure costs that come with public clouds. The allotment of resources is yours to use, so there is no worry'),\n",
              " Document(metadata={}, page_content='is yours to use, so there is no worry about running out or having to share with others. You simply are sharing the infrastructure. Virtual Private Clouds are commonly used with Infrastructure as a Service (IaaS) providers. Because the shared resources (CPU, RAM, etc.) are not always the'),\n",
              " Document(metadata={}, page_content='(CPU, RAM, etc.) are not always the responsibility of the hardware provider, it is possible to have different infrastructure and VPC providers. However, having the same VPC and infrastructure provider can help cut down on the confusion and communication process between you and your vendor. Amazon'),\n",
              " Document(metadata={}, page_content=\"between you and your vendor. Amazon Route 53 Announces Private DNS within Amazon VPC You can now use Amazon Route 53, AWS's highly available and scalable DNS service, to easily manage your internal domain names with the same simplicity, security, and cost effectiveness that Route 53 already\"),\n",
              " Document(metadata={}, page_content='effectiveness that Route 53 already provides for external DNS names. You can use the Route 53 Private DNS feature to manage authoritative DNS within your Virtual Private Clouds (VPCs), so you can use custom domain names for your internal AWS resources without exposing DNS data to the public'),\n",
              " Document(metadata={}, page_content='without exposing DNS data to the public Internet. You can use Route 53 Private DNS to manage internal DNS hostnames for resources like application servers, database servers, and web servers. Unit-7 – AWS networking and databases 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud'),\n",
              " Document(metadata={}, page_content='CE Department | 2180712 – Cloud Infrastructure and Services Route 53 will only respond to queries for these names when the queries originate from within the VPC(s) that you authorize. Using custom internal DNS names (rather than IP addresses or AWS-provided names such as ec2-10-1-2-'),\n",
              " Document(metadata={}, page_content='AWS-provided names such as ec2-10-1-2- 3.us-west-2.compute.amazonaws.com) has a variety of benefits, for example, being able to flip from one database to another just by changing the mapping of a domain name such as internal.example.com to point to a new IP address. Route 53 also supports'),\n",
              " Document(metadata={}, page_content='new IP address. Route 53 also supports split-view DNS, so you can configure public and private hosted zones to return different external and internal IP addresses for the same domain names. Relational Database Service Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate,'),\n",
              " Document(metadata={}, page_content='RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. It frees you to focus on your applications'),\n",
              " Document(metadata={}, page_content='frees you to focus on your applications so you can give them the fast performance, high availability, security and compatibility they need. Amazon RDS is available on several database instance types optimized for memory, performance or I/O and provides you with six familiar database engines to'),\n",
              " Document(metadata={}, page_content='with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. You can use the AWS Database Migration Service to easily migrate or replicate your existing databases to Amazon RDS. Advantages/Benefits of Relational Database'),\n",
              " Document(metadata={}, page_content='of Relational Database Service (i) Easy to Administer Amazon RDS makes it easy to go from project conception to deployment. Use the AWS Management Console, the AWS RDS Command-Line Interface, or simple API calls to access the capabilities of a production-ready relational database in minutes. No'),\n",
              " Document(metadata={}, page_content=\"relational database in minutes. No need for infrastructure provisioning, and no need for installing and maintaining database software. (ii) Highly Scalable We can scale our database's compute and storage resources with only a few mouse clicks or an API call, often with no downtime. Many Amazon RDS\"),\n",
              " Document(metadata={}, page_content='often with no downtime. Many Amazon RDS engine types allow you to launch one or more Read Replicas to offload read traffic from your primary database instance. (iii) Available and Durable Amazon RDS runs on the same highly reliable infrastructure used by other Amazon Web Services. When you'),\n",
              " Document(metadata={}, page_content='by other Amazon Web Services. When you provision a Multi-AZ DB Instance, Amazon RDS synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Amazon RDS has many other features that enhance reliability for critical production databases, including automated'),\n",
              " Document(metadata={}, page_content='databases, including automated backups, database snapshots, and automatic host replacement. (iv) Fast Amazon RDS supports the most demanding database applications. You can choose between two SSD- backed storage options: one optimized for high-performance OLTP applications, and the other for cost-'),\n",
              " Document(metadata={}, page_content='applications, and the other for cost- effective general-purpose use. In addition, Amazon Aurora provides performance on par with commercial databases at 1/10th the cost. Unit-7 – AWS networking and databases 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services (v)'),\n",
              " Document(metadata={}, page_content='– Cloud Infrastructure and Services (v) Secure Amazon RDS makes it easy to control network access to your database. Amazon RDS also lets you run your database instances in Amazon Virtual Private Cloud (Amazon VPC), which enables you to isolate your database instances and to connect to your existing'),\n",
              " Document(metadata={}, page_content='and to connect to your existing IT infrastructure through an industry-standard encrypted IPsec VPN. Many Amazon RDS engine types offer encryption at rest and encryption in transit. (vi) Inexpensive You pay very low rates and only for the resources you actually consume. In addition, you benefit from'),\n",
              " Document(metadata={}, page_content='consume. In addition, you benefit from the option of On-Demand pricing with no up-front or long-term commitments, or even lower hourly rates via Reserved Instance pricing. DynamoDB Amazon DynamoDB -- also known as Dynamo Database or DDB -- is a fully managed NoSQL database service provided by'),\n",
              " Document(metadata={}, page_content='NoSQL database service provided by Amazon Web Services. DynamoDB is known for low latencies and scalability. According to AWS, DynamoDB makes it simple and cost-effective to store and retrieve any amount of data, as well as serve any level of request traffic. All data items are stored on'),\n",
              " Document(metadata={}, page_content='traffic. All data items are stored on solid-state drives, which provide high I/O performance and can more efficiently handle high-scale requests. An AWS user interacts with the service by using the AWS Management Console or a DynamoDB API. DynamoDB uses a NoSQL database model, which is'),\n",
              " Document(metadata={}, page_content='uses a NoSQL database model, which is nonrelational, allowing documents, graphs and columnar among its data models. A user stores data in DynamoDB tables, then interacts with it via GET and PUT queries, which are read and write operations, respectively. DynamoDB supports basic CRUD operations and'),\n",
              " Document(metadata={}, page_content='supports basic CRUD operations and conditional operations. Each DynamoDB query is executed by a primary key identified by the user, which uniquely identifies each item. Scalability, Availability and Durability DynamoDB enforces replication across three availability zones for high availability,'),\n",
              " Document(metadata={}, page_content='zones for high availability, durability and read consistency. A user can also opt for cross-region replication, which creates a backup copy of a DynamoDB table in one or more global geographic locations. The DynamoDB scan API provides two consistency options when reading DynamoDB data: o Eventually'),\n",
              " Document(metadata={}, page_content='reading DynamoDB data: o Eventually consistent reads o Strongly consistent reads The former, which is the AWS default setting, maximizes throughput at the potential expense of not having a read reflect the latest write or update. The latter reflects all writes and updates. There are no DynamoDB'),\n",
              " Document(metadata={}, page_content='and updates. There are no DynamoDB limits on data storage per user, nor a maximum throughput per table. Security Amazon DynamoDB offers Fine-Grained Access Control (FGAC) for an administrator to protect data in a table. The admin or table owner can specify who can access which items or attributes'),\n",
              " Document(metadata={}, page_content='can access which items or attributes in a table and what actions that person can perform. FGAC is based on the AWS Identity and Access Management service, which manages credentials and permissions. Unit-7 – AWS networking and databases 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud'),\n",
              " Document(metadata={}, page_content='CE Department | 2180712 – Cloud Infrastructure and Services As with other AWS products, the cloud provider recommends a policy of least privilege when granting access to items and attributes. An admin can view usage metrics for DynamoDB with Amazon CloudWatch. Additional DynamoDB Features The'),\n",
              " Document(metadata={}, page_content='Additional DynamoDB Features The DynamoDB Triggers feature integrates with AWS Lambda to allow a developer to code actions based on updates to items in a DynamoDB table, such as sending a notification or connecting a table to another data source. The developer associates a Lambda function, which'),\n",
              " Document(metadata={}, page_content='associates a Lambda function, which stores the logic code, with the stream on a DynamoDB table. AWS Lambda then reads updates to a table from a stream and executes the function. The DynamoDB Streams feature provides a 24-hour chronological sequence of updates to items in a table. An admin can'),\n",
              " Document(metadata={}, page_content='to items in a table. An admin can access the stream via an API call to take action based on updates, such as synchronizing information with another data store. An admin enables DynamoDB Streams on a per-table basis. Advantages of DynamoDB Performance at scale DynamoDB supports some of the world’s'),\n",
              " Document(metadata={}, page_content='DynamoDB supports some of the world’s largest scale applications by providing consistent, single-digit millisecond response times at any scale. You can build applications with virtually unlimited throughput and storage. DynamoDB global tables replicate your data across multiple AWS Regions to give'),\n",
              " Document(metadata={}, page_content='across multiple AWS Regions to give you fast, local access to data for your globally distributed applications. For use cases that require even faster access with microsecond latency, DynamoDB Accelerator (DAX) provides a fully managed in-memory cache. No servers to manage DynamoDB is server less'),\n",
              " Document(metadata={}, page_content='to manage DynamoDB is server less with no servers to provision, patch, or manage and no software to install, maintain, or operate. DynamoDB automatically scales tables up and down to adjust for capacity and maintain performance. Availability and fault tolerance are built in, eliminating the need to'),\n",
              " Document(metadata={}, page_content='are built in, eliminating the need to architect your applications for these capabilities. DynamoDB provides both provisioned and on-demand capacity modes so that you can optimize costs by specifying capacity per workload, or paying for only the resources you consume. Enterprise ready DynamoDB'),\n",
              " Document(metadata={}, page_content='you consume. Enterprise ready DynamoDB supports ACID transactions to enable you to build business-critical applications at scale. DynamoDB encrypts all data by default and provides fine-grained identity and access control on all your tables. You can create full backups of hundreds of terabytes of'),\n",
              " Document(metadata={}, page_content='backups of hundreds of terabytes of data instantly with no performance impact to your tables, and recover to any point in time in the preceding 35 days with no downtime. DynamoDB is also backed by a service level agreement for guaranteed availability. ElastiCache ElastiCache is a web service that'),\n",
              " Document(metadata={}, page_content='ElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud. Unit-7 – AWS networking and databases 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services It provides a'),\n",
              " Document(metadata={}, page_content='and Services It provides a high-performance, scalable, and cost-effective caching solution, while removing the complexity associated with deploying and managing a distributed cache environment. With ElastiCache, you can quickly deploy your cache environment, without having to provision hardware or'),\n",
              " Document(metadata={}, page_content='without having to provision hardware or install software. You can choose from Memcached or Redis protocol-compliant cache engine software, and let ElastiCache perform software upgrades and patch management for you. For enhanced security, ElastiCache can be run in the Amazon Virtual Private Cloud'),\n",
              " Document(metadata={}, page_content='run in the Amazon Virtual Private Cloud (Amazon VPC) environment, giving you complete control over network access to your clusters. With just a few clicks in the AWS Management Console, you can add or remove resources such as nodes, clusters, or read replicas to your ElastiCache environment to meet'),\n",
              " Document(metadata={}, page_content='to your ElastiCache environment to meet your business needs and application requirements. Existing applications that use Memcached or Redis can use ElastiCache with almost no modification. Your applications simply need to know the host names and port numbers of the ElastiCache nodes that you have'),\n",
              " Document(metadata={}, page_content='of the ElastiCache nodes that you have deployed. The ElastiCache Auto Discovery feature for Memcached lets your applications identify all of the nodes in a cache cluster and connect to them, rather than having to maintain a list of available host names and port numbers. In this way, your'),\n",
              " Document(metadata={}, page_content='and port numbers. In this way, your applications are effectively insulated from changes to node membership in a cluster. ElastiCache has multiple features to enhance reliability for critical production deployments: o Automatic detection and recovery from cache node failures. o Multi-AZ with'),\n",
              " Document(metadata={}, page_content='cache node failures. o Multi-AZ with Automatic Failover of a failed primary cluster to a read replica in Redis clusters that support replication (called replication groups in the ElastiCache API and AWS CLI. o Flexible Availability Zone placement of nodes and clusters. o Integration with other AWS'),\n",
              " Document(metadata={}, page_content='clusters. o Integration with other AWS services such as Amazon EC2, Amazon CloudWatch, AWS CloudTrail, and Amazon SNS to provide a secure, high-performance, managed in-memory caching solution. ElastiCache Nodes A node is the smallest building block of an ElastiCache deployment. A node can exist in'),\n",
              " Document(metadata={}, page_content='deployment. A node can exist in isolation from or in some relationship to other nodes. A node is a fixed-size chunk of secure, network-attached RAM. Each node runs an instance of the engine and version that was chosen when you created your cluster. If necessary, you can scale the nodes in a cluster'),\n",
              " Document(metadata={}, page_content='you can scale the nodes in a cluster up or down to a different instance type. Every node within a cluster is the same instance type and runs the same cache engine. Each cache node has its own Domain Name Service (DNS) name and port. Multiple types of cache nodes are supported, each with varying'),\n",
              " Document(metadata={}, page_content='nodes are supported, each with varying amounts of associated memory. You can purchase nodes on a pay-as-you-go basis, where you only pay for your use of a node. Or you can purchase reserved nodes at a much-reduced hourly rate. If your usage rate is high, purchasing reserved nodes can save you'),\n",
              " Document(metadata={}, page_content='purchasing reserved nodes can save you money. ElastiCache for Redis Shards A Redis shard (called a node group in the API and CLI) is a grouping of one to six related nodes. A Redis (cluster mode disabled) cluster always has one shard. A Redis (cluster mode enabled) cluster can have 1–90 shards. A'),\n",
              " Document(metadata={}, page_content='cluster can have 1–90 shards. A multiple node shard implements replication by having one read/write primary node and 1–5 replica nodes. Unit-7 – AWS networking and databases 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services ElastiCache for Redis Clusters A Redis'),\n",
              " Document(metadata={}, page_content='ElastiCache for Redis Clusters A Redis cluster is a logical grouping of one or more ElastiCache for Redis Shards. Data is partitioned across the shards in a Redis (cluster mode enabled) cluster. Many ElastiCache operations are targeted at clusters: o Creating a cluster o Modifying a cluster o'),\n",
              " Document(metadata={}, page_content='a cluster o Modifying a cluster o Taking snapshots of a cluster (all versions of Redis) o Deleting a cluster o Viewing the elements in a cluster o Adding or removing cost allocation tags to and from a cluster Redshift Perhaps one of the most exciting outcomes of the public cloud was addressing the'),\n",
              " Document(metadata={}, page_content='of the public cloud was addressing the shortcomings of traditional enterprise data warehouse (EDW) storage and processing. The fast provisioning, commodity costs, infinite scale, and pay-as-you-grow pricing of public cloud are a natural fit for EDW needs, providing even the smallest of users the'),\n",
              " Document(metadata={}, page_content='even the smallest of users the ability to now get valuable answers to business intelligence (BI) questions. Amazon Redshift is one such system built to address EDW needs, and it boasts low costs, an easy SQL- based access model, easy integration to other Amazon Web Services (AWS) solutions, and'),\n",
              " Document(metadata={}, page_content='Web Services (AWS) solutions, and most importantly, high query performance. Amazon Redshift gets its name from the astronomical phenomenon noticed by Hubble, which explained the expansion of the universe. By adopting the Amazon Redshift moniker, AWS wanted to relay to customers that the service was'),\n",
              " Document(metadata={}, page_content='relay to customers that the service was built to handle the perpetual expansion of their data. Fig.: Amazon Redshift Architecture An Amazon Redshift cluster consists of one leader node (which clients submit queries to) and one or more follower (or “compute”) nodes, which actually perform the'),\n",
              " Document(metadata={}, page_content='nodes, which actually perform the queries on locally stored data. By allowing for unlimited expansion of follower nodes, Amazon Redshift ensures that customers can continue to grow their cluster as their data needs grow. Unit-7 – AWS networking and databases 7 Prof. Vijay M. Shekhat, CE Department'),\n",
              " Document(metadata={}, page_content='7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Customers can start with a “cluster” as small as a single node (acting as both leader and follower), and for the smallest supported instance type (a DW2), that could be as low cost as $0.25/hour or about'),\n",
              " Document(metadata={}, page_content='be as low cost as $0.25/hour or about $180/month. By using “Reservations” (paying an up-front fee in exchange for a lower hourly running cost) for the underlying instances, Amazon Redshift can cost as little as $1,000/TB/year — upwards of one-fifth to one-tenth of the cost of a traditional EDW.'),\n",
              " Document(metadata={}, page_content='of the cost of a traditional EDW. Because Amazon Redshift provides native Open Database Connectivity (ODBC) and Database Connectivity (JDBC) connectivity (in addition to PostgresSQL driver support), most third-party BI tools (like Tableu, Qlikview, and MicroStrategy) work right out of the box.'),\n",
              " Document(metadata={}, page_content='work right out of the box. Amazon Redshift also uses the ubiquitous Structured Query Language (SQL) language for queries, ensuring that your current resources can quickly and easily become productive with the technology. Amazon Redshift was custom designed from the ParAccel engine — an analytic'),\n",
              " Document(metadata={}, page_content='from the ParAccel engine — an analytic database which used columnar storage and parallel processing to achieve very fast I/O. Columns of data in Amazon Redshift are stored physically adjacent on disk, meaning that queries and scans on those columns (common in online analytical processing [OLAP]'),\n",
              " Document(metadata={}, page_content='in online analytical processing [OLAP] queries) run very fast. Additionally, Amazon Redshift uses 10GB Ethernet interconnects, and specialized EC2 instances (with between three and 24 spindles per node) to achieve high throughput and low latency. For even faster queries, Amazon Redshift allows'),\n",
              " Document(metadata={}, page_content='faster queries, Amazon Redshift allows customers to use column-level compression to both greatly reduce the amount of data that needs stored, and reduce the amount of disk I/O. Amazon Redshift, like many of AWS’s most popular services, is also fully managed, meaning that low-level, time-consuming'),\n",
              " Document(metadata={}, page_content='meaning that low-level, time-consuming administrative tasks like OS patching, backups, replacing failed hardware, and software upgrades are handled automatically and transparently. With Amazon Redshift, users simply provision a cluster, load it with their data, and begin executing queries. All data'),\n",
              " Document(metadata={}, page_content='and begin executing queries. All data is continuously, incrementally, automatically backed up in the highly durable S3, and enabling disaster recovery across regions can be accomplished with just a few clicks. Spinning a cluster up can be as simple as a few mouse clicks, and as fast as a few'),\n",
              " Document(metadata={}, page_content='few mouse clicks, and as fast as a few minutes. A very exciting aspect of Amazon Redshift, and something that is not possible in traditional EDWs, is the ability to easily scale a provisioned cluster up and down. In Amazon Redshift, this scaling is transparent to the customer—when a resize is'),\n",
              " Document(metadata={}, page_content='to the customer—when a resize is requested, data is copied in parallel from the source cluster (which continues to function in read-only mode) to a new cluster, and once all data is live migrated, DNS is flipped to the new cluster and the old cluster is de-provisioned. This allows customers to'),\n",
              " Document(metadata={}, page_content='This allows customers to easily scale up and down, and each scaling event nicely re-stripes the data across the new cluster for a balanced workload. Amazon Redshift offers mature, native, and tunable security. Clusters can be deployed into a Virtual Private Cloud (VPC), and encryption of data is'),\n",
              " Document(metadata={}, page_content='Cloud (VPC), and encryption of data is supported via hardware accelerated AES-256 (for data at rest) and SSL (for data on the wire). Compliance teams will be pleased to learn that users can manage their own encryption keys via AWS’s Hardware Security Module (HSM) service, and that Amazon Redshift'),\n",
              " Document(metadata={}, page_content='(HSM) service, and that Amazon Redshift provides a full audit trail of all SQL connection attempts, queries, and modifications of the cluster. Advantages of Amazon Redshift Exceptionally fast Redshift is very fast when it comes to loading data and querying it for analytical and reporting purposes.'),\n",
              " Document(metadata={}, page_content='for analytical and reporting purposes. Redshift has Massively Parallel Processing (MPP) Architecture which allows you to load data at blazing fast speed. In addition, using this architecture, Redshift distributes and parallelize your queries across multiple nodes. Unit-7 – AWS networking and'),\n",
              " Document(metadata={}, page_content='nodes. Unit-7 – AWS networking and databases 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Redshift gives you an option to use Dense Compute nodes which are SSD based data warehouses. Using this you can run most complex queries in very less time. High'),\n",
              " Document(metadata={}, page_content='complex queries in very less time. High Performance As discussed in the previous point, Redshift gains high performance using massive parallelism, efficient data compression, query optimization, and distribution. MPP enables Redshift to parallelize data loading, backup and restore operation.'),\n",
              " Document(metadata={}, page_content='loading, backup and restore operation. Furthermore, queries that you execute get distributed across multiple nodes. Redshift is a columnar storage database, which is optimized for huge and repetitive type of data. Using columnar storage, reduces the I/O operations on disk drastically, improving'),\n",
              " Document(metadata={}, page_content='on disk drastically, improving performance as a result. Redshift gives you an option to define column-based encoding for data compression. If not specified by the user, redshift automatically assigns compression encoding. Data compression helps in reducing memory footprint and significantly'),\n",
              " Document(metadata={}, page_content='memory footprint and significantly improves the I/O speed. Horizontally Scalable Scalability is a very crucial point for any Data warehousing solution and Redshift does pretty well job in that. Redshift is horizontally scalable. Whenever you need to increase the storage or need it to run faster,'),\n",
              " Document(metadata={}, page_content='the storage or need it to run faster, just add more nodes using AWS console or Cluster API and it will upscale immediately. During this process, your existing cluster will remain available for read operations so your application stays uninterrupted. During the scaling operation, Redshift moves data'),\n",
              " Document(metadata={}, page_content='scaling operation, Redshift moves data parallel between compute nodes of old and new clusters. Therefore enabling the transition to complete smoothly and as quickly as possible. Massive Storage capacity As expected from a Data warehousing solution, Redshift provides massive storage capacity. A'),\n",
              " Document(metadata={}, page_content='provides massive storage capacity. A basic setup can give you a petabyte range of data storage. In addition, Redshift gives you an option to choose Dense Storage type of compute nodes which can provide large storage space using Hard Disk Drives for a very low price. You can further increase the'),\n",
              " Document(metadata={}, page_content='low price. You can further increase the storage by adding more nodes to your cluster and it can go well beyond petabyte of data range. Attractive and transparent pricing Pricing is a very strong point in favor of Redshift, it is considerably cheaper than alternatives or an on premise solution.'),\n",
              " Document(metadata={}, page_content='alternatives or an on premise solution. Redshift has 2 pricing models, pay as you go and reserved instance. Hence this gives you the flexibility to categorize this expense as an operational expense or capital expense. If your use case requires more data storage, then with 3 years reserved instance'),\n",
              " Document(metadata={}, page_content='then with 3 years reserved instance Dense Storage plan, effective price per terabyte per year can be as low as $935. Comparing this to traditional on premise storage, which roughly costs around $19k-$25k per terabyte, Redshift is significantly cheaper. SQL interface Redshift Query Engine is based'),\n",
              " Document(metadata={}, page_content='Redshift Query Engine is based on ParAccel which has the same interface as PostgreSQL If you are already familiar with SQL, you don’t need to learn a lot of new techs to start using query module of Redshift. Since Redshift uses SQL, it works with existing Postgres JDBC/ODBC drivers, readily'),\n",
              " Document(metadata={}, page_content='Postgres JDBC/ODBC drivers, readily connecting to most of the Business Intelligence tools. Unit-7 – AWS networking and databases 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS ecosystem Many businesses are running their infrastructure on AWS already, EC2'),\n",
              " Document(metadata={}, page_content='infrastructure on AWS already, EC2 for servers, S3 for long-term storage, RDS for database and this number is constantly increasing. Redshift works very well if the rest of your infra is already on AWS and you get the benefit of data locality and cost of data transport is comparatively low. For a'),\n",
              " Document(metadata={}, page_content='transport is comparatively low. For a lot of businesses, S3 has become the de-facto destination for cloud storage. Since Redshift is virtually co-located with S3 and it can access formatted data on S3 with single COPY command. When loading or dumping data on S3, Redshift uses Massive Parallel'),\n",
              " Document(metadata={}, page_content='on S3, Redshift uses Massive Parallel Processing which can move data at a very fast speed. Security Amazon Redshift comes packed with various security features. There are options like VPC for network isolation, various ways to handle access control, data encryption etc. Data encryption option is'),\n",
              " Document(metadata={}, page_content='etc. Data encryption option is available at multiple places in Redshift. To encrypt data stored in your cluster you can enable cluster encryption at the time of launching the cluster. Also, to encrypt data in transit, you can enable SSL encryption. When loading data from S3, redshift allows you to'),\n",
              " Document(metadata={}, page_content='data from S3, redshift allows you to use either server-side encryption or client-side encryption. Finally, at the time of loading data, S3 or Redshift copy command handles the decryption respectively. Amazon Redshift clusters can be launched inside your infrastructure Virtual Private Cloud (VPC).'),\n",
              " Document(metadata={}, page_content='Virtual Private Cloud (VPC). Hence you can define VPC security groups to restrict inbound or outbound access to your redshift clusters. Using the robust Access Control system of AWS, you can grant privilege to specific users or maintain access on specific database level. Additionally, you can even'),\n",
              " Document(metadata={}, page_content='level. Additionally, you can even define users and groups to have access to specific data in tables. Amazon Redshift Limitations Doesn’t enforce uniqueness There is no way in redshift to enforce uniqueness on inserted data. Hence, if you have a distributed system and it writes data on Redshift, you'),\n",
              " Document(metadata={}, page_content='and it writes data on Redshift, you will have to handle the uniqueness yourself either on the application layer or by using some method of data de-duplication. Only S3, DynamoDB and Amazon EMR support for parallel upload If your data is in Amazon S3 or relational DynamoDB or on Amazon EMR, Redshift'),\n",
              " Document(metadata={}, page_content='DynamoDB or on Amazon EMR, Redshift can load it using Massively Parallel Processing which is very fast. But for all other sources, parallel loading is not supported. You will either have to use JDBC inserts or some scripts to load data into Redshift. Alternatively, you can use an ETL solution like'),\n",
              " Document(metadata={}, page_content='you can use an ETL solution like Hevo which can load your data into Redshift parallel from 100s of sources. Requires a good understanding of Sort and Distribution keys Sort keys and Distribution keys decide how data is stored and indexed across all Redshift nodes. Therefore, you need to have a'),\n",
              " Document(metadata={}, page_content='nodes. Therefore, you need to have a solid understanding of these concepts and you need to properly set them on your tables for optimal performance. Unit-7 – AWS networking and databases 10 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services There can be only one'),\n",
              " Document(metadata={}, page_content='and Services There can be only one distribution key for a table and that cannot be changed later on, which means you have to think carefully and anticipate future workloads before deciding Distribution key. Can’t be used as live app database While Redshift is very fast when running queries on a'),\n",
              " Document(metadata={}, page_content='is very fast when running queries on a huge amount of data or running reporting and analytics, but it is not fast enough for live web apps. So you will have to pull data into a caching layer or a vanilla Postgres instance to serve redshift data to web apps. Data on Cloud Though it is a good thing'),\n",
              " Document(metadata={}, page_content='Data on Cloud Though it is a good thing for most of the people, in some use cases it could be a point of concern. So if you are concerned with the privacy of data or your data has extremely sensitive content, you may not be comfortable putting it on the cloud. High performance AWS Networking. High'),\n",
              " Document(metadata={}, page_content='High performance AWS Networking. High performance AWS Networking is nothing but use of various network services provided by AWS for better performance. AWS Networking include following services: 1. Private DNS Servers o The Private DNS are name servers that reflect your domain name rather than our'),\n",
              " Document(metadata={}, page_content='your domain name rather than our default ones. o Having private nameservers could be useful if you intend to resell hosting services or want to brand your business. o Also, when using Private DNS, if a domain name is migrated to another server, there is no need to change any nameservers and the'),\n",
              " Document(metadata={}, page_content='need to change any nameservers and the domain names will automatically point to the new location. 2. Virtual Private Clouds (Explain Earlier) 3. Cloud Models (Explain Earlier) etc. Unit-8 – Other AWS Services & Management Services 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud'),\n",
              " Document(metadata={}, page_content='CE Department | 2180712 – Cloud Infrastructure and Services Big Data Analytics Big data analytics is the often complex process of examining large and varied data sets, or big data, to uncover information -- such as hidden patterns, unknown correlations, market trends and customer preferences --'),\n",
              " Document(metadata={}, page_content='trends and customer preferences -- that can help organizations make informed business decisions. AWS Analytics Services Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure'),\n",
              " Document(metadata={}, page_content='so there is no infrastructure to manage, and you pay only for the queries that you run. Athena is easy to use. Simply point to your data in Amazon S3, deﬁne the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, there’s no need for complex'),\n",
              " Document(metadata={}, page_content='Athena, there’s no need for complex extract, transform, and load (ETL) jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets. Athena is out-of-the-box integrated with AWS Glue Data Catalog, allowing you to create a unified'),\n",
              " Document(metadata={}, page_content='allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning. You can also use Glue’s fully-managed ETL capabilities to transform'),\n",
              " Document(metadata={}, page_content='ETL capabilities to transform data or convert it into columnar formats to optimize cost and improve performance. Amazon EMR Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-eﬀective to process vast amounts of data across dynamically scalable Amazon EC2 instances.'),\n",
              " Document(metadata={}, page_content='scalable Amazon EC2 instances. You can also run other popular distributed frameworks such as Apache Spark, HBase, Presto, and Flink in Amazon EMR, and interact with data in other AWS data stores such as Amazon S3 and Amazon DynamoDB. EMR Notebooks, based on the popular Jupyter Notebook, provide a'),\n",
              " Document(metadata={}, page_content='the popular Jupyter Notebook, provide a development and collaboration environment for ad hoc querying and exploratory analysis. Amazon EMR securely and reliably handles a broad set of big data use cases, including log analysis, web indexing, data transformations (ETL), machine learning, ﬁnancial'),\n",
              " Document(metadata={}, page_content='(ETL), machine learning, ﬁnancial analysis, scientiﬁc simulation, and bioinformatics. Amazon CloudSearch Amazon CloudSearch is a managed service in the AWS Cloud that makes it simple and cost-effective to set up, manage, and scale a search solution for your website or application. Amazon'),\n",
              " Document(metadata={}, page_content='for your website or application. Amazon CloudSearch supports 34 languages and popular search features such as highlighting, autocomplete, and geospatial search. Amazon Elasticsearch Service Amazon Elasticsearch Service makes it easy to deploy, secure, operate, and scale Elasticsearch to search,'),\n",
              " Document(metadata={}, page_content='and scale Elasticsearch to search, analyze, and visualize data in real-time. With Amazon Elasticsearch Service, you get easy-to-use APIs and real-time analytics capabilities to power use-cases such as log analytics, full-text search, application monitoring, and clickstream analytics, with'),\n",
              " Document(metadata={}, page_content='and clickstream analytics, with enterprise-grade availability, scalability, and security. Unit-8 – Other AWS Services & Management Services 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services The service oﬀers integrations with open-source tools like Kibana and'),\n",
              " Document(metadata={}, page_content='with open-source tools like Kibana and Logstash for data ingestion and visualization. It also integrates seamlessly with other AWS services such as Amazon Virtual Private Cloud (Amazon VPC), AWS Key Management Service (AWS KMS), Amazon Kinesis Data Firehose, AWS Lambda, AWS Identity and Access'),\n",
              " Document(metadata={}, page_content='AWS Lambda, AWS Identity and Access Management (IAM), Amazon Cognito, and Amazon CloudWatch, so that you can go from raw data to actionable insights quickly. Amazon Kinesis Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and'),\n",
              " Document(metadata={}, page_content='data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can'),\n",
              " Document(metadata={}, page_content='With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of'),\n",
              " Document(metadata={}, page_content='and respond instantly instead of having to wait until all your data is collected before the processing can begin. Amazon Kinesis currently oﬀers four services: Kinesis Data Firehose, Kinesis Data Analytics, Kinesis Data Streams, and Kinesis Video Streams. Amazon Kinesis Data Firehose o Amazon'),\n",
              " Document(metadata={}, page_content='Amazon Kinesis Data Firehose o Amazon Kinesis Firehose is the easiest way to reliably load streaming data into data stores and analytics tools. o It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time'),\n",
              " Document(metadata={}, page_content='and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. o It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. o It can also batch, compress,'),\n",
              " Document(metadata={}, page_content='o It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. o You can easily create a Firehose delivery stream from the AWS Management Console, conﬁgure it with a few clicks, and start sending'),\n",
              " Document(metadata={}, page_content='it with a few clicks, and start sending data to the stream from hundreds of thousands of data sources to be loaded continuously to AWS—all in just a few minutes. o You can also configure your delivery stream to automatically convert the incoming data to columnar formats like Apache Parquet and'),\n",
              " Document(metadata={}, page_content='formats like Apache Parquet and Apache ORC, before the data is delivered to Amazon S3, for cost- effective storage and analytics. Amazon Kinesis Data Analytics o Amazon Kinesis Data Analytics is the easiest way to analyze streaming data, gain actionable insights, and respond to your business and'),\n",
              " Document(metadata={}, page_content='and respond to your business and customer needs in real time. o Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating streaming applications with other AWS services. o SQL users can easily query streaming data or build entire streaming applications using'),\n",
              " Document(metadata={}, page_content='entire streaming applications using templates and an interactive SQL editor. o Java developers can quickly build sophisticated streaming applications using open source Java libraries and AWS integrations to transform and analyze data in real-time. o Amazon Kinesis Data Analytics takes care of'),\n",
              " Document(metadata={}, page_content='Kinesis Data Analytics takes care of everything required to run your queries continuously and scales automatically to match the volume and throughput rate of your incoming data. Unit-8 – Other AWS Services & Management Services 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud'),\n",
              " Document(metadata={}, page_content='CE Department | 2180712 – Cloud Infrastructure and Services Amazon Kinesis Data Streams o Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. o KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as'),\n",
              " Document(metadata={}, page_content='of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. o The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly'),\n",
              " Document(metadata={}, page_content='real-time dashboards, real-time anomaly detection, dynamic pricing, and more. Amazon Kinesis Video Streams o Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), playback, and other processing. o Kinesis Video'),\n",
              " Document(metadata={}, page_content='and other processing. o Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices. o It also durably stores, encrypts, and indexes video data in your streams, and allows you to access your data through'),\n",
              " Document(metadata={}, page_content='allows you to access your data through easy-to-use APIs. o Kinesis Video Streams enables you to playback video for live and on-demand viewing, and quickly build applications that take advantage of computer vision and video analytics through integration with Amazon Recognition Video, and libraries'),\n",
              " Document(metadata={}, page_content='Amazon Recognition Video, and libraries for ML frameworks such as Apache MxNet, TensorFlow, and OpenCV. Amazon Redshift Amazon Redshift is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake. Redshift delivers ten'),\n",
              " Document(metadata={}, page_content='and data lake. Redshift delivers ten times faster performance than other data warehouses by using machine learning, massively parallel query execution, and columnar storage on high-performance disk. You can setup and deploy a new data warehouse in minutes, and run queries across petabytes of data'),\n",
              " Document(metadata={}, page_content='run queries across petabytes of data in your Redshift data warehouse, and exabytes of data in your data lake built on Amazon S3. You can start small for just $0.25 per hour and scale to $250 per terabyte per year, less than one-tenth the cost of other solutions. Amazon QuickSight Amazon QuickSight'),\n",
              " Document(metadata={}, page_content='Amazon QuickSight Amazon QuickSight is a fast, cloud-powered business intelligence (BI) service that makes it easy for you to deliver insights to everyone in your organization. QuickSight lets you create and publish interactive dashboards that can be accessed from browsers or mobile devices. You'),\n",
              " Document(metadata={}, page_content='from browsers or mobile devices. You can embed dashboards into your applications, providing your customers with powerful self-service analytics. QuickSight easily scales to tens of thousands of users without any software to install, servers to deploy, or infrastructure to manage. AWS Data Pipeline'),\n",
              " Document(metadata={}, page_content='to manage. AWS Data Pipeline AWS Data Pipeline is a web service that helps you reliably process and move data between diﬀerent AWS compute and storage services, as well as on-premises data sources, at speciﬁed intervals. With AWS Data Pipeline, you can regularly access your data where it’s stored,'),\n",
              " Document(metadata={}, page_content='access your data where it’s stored, transform and process it at scale, and eﬃciently transfer the results to AWS services such as Amazon S3, Amazon RDS, Amazon DynamoDB, and Amazon EMR. Unit-8 – Other AWS Services & Management Services 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud'),\n",
              " Document(metadata={}, page_content='CE Department | 2180712 – Cloud Infrastructure and Services AWS Data Pipeline helps you easily create complex data processing workloads that are fault tolerant, repeatable, and highly available. You don’t have to worry about ensuring resource availability, managing inter-task dependencies, retrying'),\n",
              " Document(metadata={}, page_content='inter-task dependencies, retrying transient failures or timeouts in individual tasks, or creating a failure notification system. AWS Data Pipeline also allows you to move and process data that was previously locked up in on-premises data silos. AWS Glue AWS Glue is a fully managed extract,'),\n",
              " Document(metadata={}, page_content='AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You can create and run an ETL job with a few clicks in the AWS Management Console. You simply point AWS Glue to your data stored on AWS, and AWS Glue'),\n",
              " Document(metadata={}, page_content='your data stored on AWS, and AWS Glue discovers your data and stores the associated metadata (e.g. table definition and schema) in the AWS Glue Data Catalog. Once cataloged, your data is immediately searchable, queryable, and available for ETL. AWS Lake Formation AWS Lake Formation is a service'),\n",
              " Document(metadata={}, page_content='AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. A data lake enables you to break down data silos and combine'),\n",
              " Document(metadata={}, page_content='to break down data silos and combine different types of analytics to gain insights and guide better business decisions. However, setting up and managing data lakes today involves a lot of manual, complicated, and time- consuming tasks. This work includes loading data from diverse sources,'),\n",
              " Document(metadata={}, page_content='loading data from diverse sources, monitoring those data flows, setting up partitions, turning on encryption and managing keys, defining transformation jobs and monitoring their operation, re-organizing data into a columnar format, configuring access control settings, deduplicating redundant data,'),\n",
              " Document(metadata={}, page_content='settings, deduplicating redundant data, matching linked records, granting access to data sets, and auditing access over time. Creating a data lake with Lake Formation is as simple as defining where your data resides and what data access and security policies you want to apply. Lake Formation then'),\n",
              " Document(metadata={}, page_content='you want to apply. Lake Formation then collects and catalogs data from databases and object storage, moves the data into your new Amazon S3 data lake, cleans and classifies data using machine learning algorithms, and secures access to your sensitive data. Your users can then access a centralized'),\n",
              " Document(metadata={}, page_content='users can then access a centralized catalog of data which describes available data sets and their appropriate usage. Your users then leverage these data sets with their choice of analytics and machine learning services, like Amazon EMR for Apache Spark, Amazon Redshift, Amazon Athena, Amazon'),\n",
              " Document(metadata={}, page_content='Amazon Redshift, Amazon Athena, Amazon SageMaker, and Amazon QuickSight. Amazon Managed Streaming for Kafka (MSK) Amazon Managed Streaming for Kafka (MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Apache'),\n",
              " Document(metadata={}, page_content='Kafka to process streaming data. Apache Kafka is an open-source platform for building real-time streaming data pipelines and applications. With Amazon MSK, you can use Apache Kafka APIs to populate data lakes, stream changes to and from databases, and power machine learning and analytics'),\n",
              " Document(metadata={}, page_content='power machine learning and analytics applications. Apache Kafka clusters are challenging to setup, scale, and manage in production. Unit-8 – Other AWS Services & Management Services 5 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services When you run Apache Kafka on'),\n",
              " Document(metadata={}, page_content='Services When you run Apache Kafka on your own, you need to provision servers, configure Apache Kafka manually, replace servers when they fail, orchestrate server patches and upgrades, architect the cluster for high availability, ensure data is durably stored and secured, setup monitoring and'),\n",
              " Document(metadata={}, page_content='and secured, setup monitoring and alarms, and carefully plan scaling events to support load changes. Amazon Managed Streaming for Kafka makes it easy for you to build and run production applications on Apache Kafka without needing Apache Kafka infrastructure management expertise. That means you'),\n",
              " Document(metadata={}, page_content='management expertise. That means you spend less time managing infrastructure and more time building applications. With a few clicks in the Amazon MSK console you can create highly available Apache Kafka clusters with settings and configuration based on Apache Kafka’s deployment best practices.'),\n",
              " Document(metadata={}, page_content='Kafka’s deployment best practices. Amazon MSK automatically provisions and runs your Apache Kafka clusters. Amazon MSK continuously monitors cluster health and automatically replaces unhealthy nodes with no downtime to your application. In addition, Amazon MSK secures your Apache Kafka cluster by'),\n",
              " Document(metadata={}, page_content='secures your Apache Kafka cluster by encrypting data at rest. Application Services Tracking Software Licenses with AWS Service Catalog and AWS Step Functions Enterprises have many business requirements for tracking how software product licenses are used in their organization for financial,'),\n",
              " Document(metadata={}, page_content='in their organization for financial, governance, and compliance reasons. By tracking license usage, organizations can stay within budget, track expenditures, and avoid unplanned true-up bills from their vendors’ true-up processes. The goal is to track the usage licenses as resources are deployed.'),\n",
              " Document(metadata={}, page_content='licenses as resources are deployed. In this post, you learn how to use AWS Service Catalog to deploy services and applications while tracking the licenses being consumed by end users, and how to prevent license overruns on AWS. This solution uses the following AWS services. Most of the resources'),\n",
              " Document(metadata={}, page_content='AWS services. Most of the resources are set up for you with an AWS CloudFormation stack: o AWS Service Catalog o AWS Lambda o AWS Step Functions o AWS CloudFormation o Amazon DynamoDB o Amazon SES Secure Serverless Development Using AWS Service Catalog Serverless computing allows you to build and'),\n",
              " Document(metadata={}, page_content='computing allows you to build and run applications and services without having to manage servers. AWS Service Catalog allows you to create and manage catalogs of services that are approved for use on AWS. Combining Serverless and Service Catalog together is a great way to safely allow developers to'),\n",
              " Document(metadata={}, page_content='great way to safely allow developers to create products and services in the cloud. In this post, I demonstrate how to combine the controls of Service Catalog with AWS Lambda and Amazon API Gateway and allow your developers to build a Serverless application without full AWS access. How to secure'),\n",
              " Document(metadata={}, page_content='without full AWS access. How to secure infrequently used EC2 instances with AWS Systems Manager Many organizations have predictable spikes in the usage of their applications and services. For example, retailers see large spikes in usage during Black Friday or Cyber Monday. Unit-8 – Other AWS'),\n",
              " Document(metadata={}, page_content='or Cyber Monday. Unit-8 – Other AWS Services & Management Services 6 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services The beauty of Amazon Elastic Compute Cloud (Amazon EC2) is that it allows customers to quickly scale up their compute power to meet these demands.'),\n",
              " Document(metadata={}, page_content='compute power to meet these demands. However, some customers might require more time-consuming setup for their software running on EC2 instances. Instead of creating and terminating instances to meet demand, these customers turn off instances and then turn them on again when they are needed.'),\n",
              " Document(metadata={}, page_content='them on again when they are needed. Eventually the patches on those instances become out of date, and they require updates. How Cloudticity Automates Security Patches for Linux and Windows using Amazon EC2 Systems Manager and AWS Step Functions As a provider of HIPAA-compliant solutions using AWS,'),\n",
              " Document(metadata={}, page_content='of HIPAA-compliant solutions using AWS, Cloudticity always has security as the base of everything we do. HIPAA breaches would be an end-of-life event for most of our customers. Having been born in the cloud with automation in our DNA, Cloudticity embeds automation into all levels of infrastructure'),\n",
              " Document(metadata={}, page_content='into all levels of infrastructure management including security, monitoring, and continuous compliance. As mandated by the HIPAA Security Rule (45 CFR Part 160 and Subparts A and C of Part 164), patches at the operating system and application level are required to prevent security vulnerabilities.'),\n",
              " Document(metadata={}, page_content='to prevent security vulnerabilities. As a result, patches are a major component of infrastructure management. Cloudticity strives to provide consistent and reliable services to all of our customers. As such, we needed to create a custom patching solution that supports both Linux and Windows. The'),\n",
              " Document(metadata={}, page_content='supports both Linux and Windows. The minimum requirements for such a solution were to read from a manifest file that contains instance names and a list of knowledge base articles (KBs) or security packages to apply to each instance. Below is a simplified, high-level process overview. Fig.:'),\n",
              " Document(metadata={}, page_content='high-level process overview. Fig.: High-Level Process Overview Unit-8 – Other AWS Services & Management Services 7 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services There were a few guidelines to be considered when designing the solution: o Each customer has a'),\n",
              " Document(metadata={}, page_content='the solution: o Each customer has a defined maintenance window that patches can be completed within. As such, the solution must be able to perform the updates within the specified maintenance window. o The solution must be able to provide patches to one or many instances and finish within the'),\n",
              " Document(metadata={}, page_content='or many instances and finish within the maintenance window. o The solution should use as many AWS services as possible to reduce time-to-market and take advantage of the built-in scaling that many AWS services provide. o Code reusability is essential. Cloud Security A number of security threats are'),\n",
              " Document(metadata={}, page_content='A number of security threats are associated with cloud data services: not only traditional security threats, such as network eavesdropping, illegal invasion, and denial of service attacks, but also specific cloud computing threats, such as side channel attacks, virtualization vulnerabilities, and'),\n",
              " Document(metadata={}, page_content='virtualization vulnerabilities, and abuse of cloud services. The following security requirements limit the threats if we achieve that requirement than we can say our data is safe on cloud. Identity management o Every enterprise will have its own identity management system to control access to'),\n",
              " Document(metadata={}, page_content='management system to control access to information and computing resources. o Cloud providers either integrate the customer’s identity management system into their own infrastructure, using federation or SSO technology, or a biometric-based identification system, or provide an identity management'),\n",
              " Document(metadata={}, page_content='or provide an identity management system of their own. o CloudID, for instance, provides privacy-preserving cloud-based and cross-enterprise biometric identification. o It links the confidential information of the users to their biometrics and stores it in an encrypted fashion. o Making use of a'),\n",
              " Document(metadata={}, page_content='an encrypted fashion. o Making use of a searchable encryption technique, biometric identification is performed in encrypted domain to make sure that the cloud provider or potential attackers do not gain access to any sensitive data or even the contents of the individual queries. Physical security o'),\n",
              " Document(metadata={}, page_content='individual queries. Physical security o Cloud service providers physically secure the IT hardware (servers, routers, cables etc.) against unauthorized access, interference, theft, fires, floods etc. and ensure that essential supplies (such as electricity) are sufficiently robust to minimize the'),\n",
              " Document(metadata={}, page_content=\"are sufficiently robust to minimize the possibility of disruption. o This is normally achieved by serving cloud applications from 'world-class' (i.e. professionally specified, designed, constructed, managed, monitored and maintained) data centers. Personnel security o Various information security\"),\n",
              " Document(metadata={}, page_content='security o Various information security concerns relating to the IT and other professionals associated with cloud services are typically handled through pre-, para- and post-employment activities such as security screening potential recruits, security awareness and training programs, proactive.'),\n",
              " Document(metadata={}, page_content='and training programs, proactive. Privacy o Providers ensure that all critical data (credit card numbers, for example) are masked or encrypted and that only authorized users have access to data in its entirety. Moreover, digital identities and credentials must be protected as should any data that'),\n",
              " Document(metadata={}, page_content='be protected as should any data that the provider collects or produces about customer activity in the cloud. Confidentiality o Data confidentiality is the property that data contents are not made available or disclosed to illegal users. Unit-8 – Other AWS Services & Management Services 8 Prof.'),\n",
              " Document(metadata={}, page_content=\"Services & Management Services 8 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services o Outsourced data is stored in a cloud and out of the owners' direct control. Only authorized users can access the sensitive data while others, including CSPs, should not gain any\"),\n",
              " Document(metadata={}, page_content='including CSPs, should not gain any information of the data. o Meanwhile, data owners expect to fully utilize cloud data services, e.g., data search, data computation, and data sharing, without the leakage of the data contents to CSPs or other adversaries. Access controllability o Access'),\n",
              " Document(metadata={}, page_content='Access controllability o Access controllability means that a data owner can perform the selective restriction of access to her or his data outsourced to cloud. o Legal users can be authorized by the owner to access the data, while others cannot access it without permissions. o Further, it is'),\n",
              " Document(metadata={}, page_content='without permissions. o Further, it is desirable to enforce fine-grained access control to the outsourced data, i.e., different users should be granted different access privileges with regard to different data pieces. o The access authorization must be controlled only by the owner in untrusted cloud'),\n",
              " Document(metadata={}, page_content='only by the owner in untrusted cloud environments. Integrity o Data integrity demands maintaining and assuring the accuracy and completeness of data. o A data owner always expects that her or his data in a cloud can be stored correctly and trustworthily. o It means that the data should not be'),\n",
              " Document(metadata={}, page_content='o It means that the data should not be illegally tampered, improperly modified, deliberately deleted, or maliciously fabricated. o If any undesirable operations corrupt or delete the data, the owner should be able to detect the corruption or loss. o Further, when a portion of the outsourced data is'),\n",
              " Document(metadata={}, page_content='a portion of the outsourced data is corrupted or lost, it can still be retrieved by the data users. CloudWatch Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log'),\n",
              " Document(metadata={}, page_content='track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources. Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications'),\n",
              " Document(metadata={}, page_content='metrics generated by your applications and services, and any log files your applications generate. You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application'),\n",
              " Document(metadata={}, page_content='to react and keep your application running smoothly. CloudFormation AWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated'),\n",
              " Document(metadata={}, page_content='to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This file serves as the single source of truth for your cloud environment. AWS CloudFormation is available at no additional charge, and you pay only for the AWS'),\n",
              " Document(metadata={}, page_content='charge, and you pay only for the AWS resources needed to run your applications. Unit-8 – Other AWS Services & Management Services 9 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Advantage of Cloud Formation Model it all AWS CloudFormation allows you to model'),\n",
              " Document(metadata={}, page_content='AWS CloudFormation allows you to model your entire infrastructure in a text file. This template becomes the single source of truth for your infrastructure. This helps you to standardize infrastructure components used across your organization, enabling configuration compliance and faster'),\n",
              " Document(metadata={}, page_content='configuration compliance and faster troubleshooting. Automate and deploy AWS CloudFormation provisions your resources in a safe, repeatable manner, allowing you to build and rebuild your infrastructure and applications, without having to perform manual actions or write custom scripts.'),\n",
              " Document(metadata={}, page_content=\"manual actions or write custom scripts. CloudFormation takes care of determining the right operations to perform when managing your stack, and rolls back changes automatically if errors are detected. It's just code Codifying your infrastructure allows you to treat your infrastructure as just code.\"),\n",
              " Document(metadata={}, page_content='treat your infrastructure as just code. You can author it with any code editor, check it into a version control system, and review the files with team members before deploying into production. CloudTrail AWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational'),\n",
              " Document(metadata={}, page_content='governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. CloudTrail is enabled'),\n",
              " Document(metadata={}, page_content='SDKs and APIs. CloudTrail is enabled on your AWS account when you create it. When activity occurs in your AWS account, that activity is recorded in a CloudTrail event. You can easily view recent events in the CloudTrail console by going to Event history. For an ongoing record of activity and events'),\n",
              " Document(metadata={}, page_content='ongoing record of activity and events in your AWS account, create a trail. Visibility into your AWS account activity is a key aspect of security and operational best practices. You can use CloudTrail to view, search, download, archive, analyze, and respond to account activity across your AWS'),\n",
              " Document(metadata={}, page_content='to account activity across your AWS infrastructure. You can identify who or what took which action, what resources were acted upon, when the event occurred, and other details to help you analyze and respond to activity in your AWS account. Optionally, you can enable AWS CloudTrail Insights on a'),\n",
              " Document(metadata={}, page_content='can enable AWS CloudTrail Insights on a trail to help you identify and respond to unusual activity. You can integrate CloudTrail into applications using the API, automate trail creation for your organization, check the status of trails you create, and control how users view CloudTrail events.'),\n",
              " Document(metadata={}, page_content='how users view CloudTrail events. Working of CloudTrail You can create two types of trails for an AWS account: A trail that applies to all regions o When you create a trail that applies to all regions, CloudTrail records events in each region and delivers the CloudTrail event log files to an S3'),\n",
              " Document(metadata={}, page_content='the CloudTrail event log files to an S3 bucket that you specify. o If a region is added after you create a trail that applies to all regions that new region is automatically included, and events in that region are logged. o This is the default option when you create a trail in the CloudTrail'),\n",
              " Document(metadata={}, page_content='you create a trail in the CloudTrail console. Unit-8 – Other AWS Services & Management Services 10 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services A trail that applies to one region o When you create a trail that applies to one region, CloudTrail records the'),\n",
              " Document(metadata={}, page_content='to one region, CloudTrail records the events in that region only. o It then delivers the CloudTrail event log files to an Amazon S3 bucket that you specify. o If you create additional single trails, you can have those trails deliver CloudTrail event log files to the same Amazon S3 bucket or to'),\n",
              " Document(metadata={}, page_content='to the same Amazon S3 bucket or to separate buckets. o This is the default option when you create a trail using the AWS CLI or the CloudTrail API. Beginning on April 12, 2019, trails will be viewable only in the AWS Regions where they log events. If you create a trail that logs events in all AWS'),\n",
              " Document(metadata={}, page_content='a trail that logs events in all AWS Regions, it will appear in the console in all AWS Regions. If you create a trail that only logs events in a single AWS Region, you can view and manage it only in that AWS Region. If you have created an organization in AWS Organizations, you can also create a'),\n",
              " Document(metadata={}, page_content='Organizations, you can also create a trail that will log all events for all AWS accounts in that organization. This is referred to as an organization trail. Organization trails can apply to all AWS Regions or one Region. Organization trails must be created in the master account, and when specified'),\n",
              " Document(metadata={}, page_content='the master account, and when specified as applying to an organization, are automatically applied to all member accounts in the organization. Member accounts will be able to see the organization trail, but cannot modify or delete it. By default, member accounts will not have access to the log files'),\n",
              " Document(metadata={}, page_content='will not have access to the log files for the organization trail in the Amazon S3 bucket. You can change the configuration of a trail after you create it, including whether it logs events in one region or all regions. You can also change whether it logs data or CloudTrail Insights events. Changing'),\n",
              " Document(metadata={}, page_content='or CloudTrail Insights events. Changing whether a trail logs events in one region or in all regions affects which events are logged. By default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE). You can also choose to encrypt your log files with an AWS Key'),\n",
              " Document(metadata={}, page_content='encrypt your log files with an AWS Key Management Service (AWS KMS) key. You can store your log files in your bucket for as long as you want. You can also define Amazon S3 lifecycle rules to archive or delete log files automatically. If you want notifications about log file delivery and validation,'),\n",
              " Document(metadata={}, page_content='about log file delivery and validation, you can set up Amazon SNS notifications. CloudTrail typically delivers log files within 15 minutes of account activity. In addition, CloudTrail publishes log files multiple times an hour, about every five minutes. These log files contain API calls from'),\n",
              " Document(metadata={}, page_content='These log files contain API calls from services in the account that support CloudTrail. Benefits of CloudTrail Simplified compliance With AWS CloudTrail, simplify your compliance audits by automatically recording and storing event logs for actions made within your AWS account. Integration with'),\n",
              " Document(metadata={}, page_content='your AWS account. Integration with Amazon CloudWatch Logs provides a convenient way to search through log data, identify out-of-compliance events, accelerate incident investigations, and expedite responses to auditor requests. Security analysis and troubleshooting With AWS CloudTrail, you can'),\n",
              " Document(metadata={}, page_content='With AWS CloudTrail, you can discover and troubleshoot security and operational issues by capturing a comprehensive history of changes that occurred in your AWS account within a specified period of time. Visibility into user and resource activity AWS CloudTrail increases visibility into your user'),\n",
              " Document(metadata={}, page_content='increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. Unit-8 – Other AWS Services & Management Services 11 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services You can identify which users and accounts'),\n",
              " Document(metadata={}, page_content='can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred. Security automation AWS CloudTrail allows you track and automatically respond to account activity threatening the security of your AWS resources. With Amazon'),\n",
              " Document(metadata={}, page_content='of your AWS resources. With Amazon CloudWatch Events integration, you can define workflows that execute when events that can result in security vulnerabilities are detected. For example, you can create a workflow to add a specific policy to an Amazon S3 bucket when CloudTrail logs an API call that'),\n",
              " Document(metadata={}, page_content='when CloudTrail logs an API call that makes that bucket public. OpsWorks AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers.'),\n",
              " Document(metadata={}, page_content='the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. OpsWorks has three offerings, AWS Opsworks for Chef Automate, AWS OpsWorks for Puppet'),\n",
              " Document(metadata={}, page_content='Chef Automate, AWS OpsWorks for Puppet Enterprise, and AWS OpsWorks Stacks. AWS OpsWorks for Chef Automate AWS OpsWorks for Chef Automate is a fully managed configuration management service that hosts Chef Automate, a suite of automation tools from Chef for configuration management, compliance and'),\n",
              " Document(metadata={}, page_content='management, compliance and security, and continuous deployment. OpsWorks also maintains your Chef server by automatically patching, updating, and backing up your server. OpsWorks eliminates the need to operate your own configuration management systems or worry about maintaining its infrastructure.'),\n",
              " Document(metadata={}, page_content='about maintaining its infrastructure. OpsWorks gives you access to all of the Chef Automate features, such as configuration and compliance management, which you manage through the Chef Console or command line tools like Knife. It also works seamlessly with your existing Chef cookbooks. Choose AWS'),\n",
              " Document(metadata={}, page_content='existing Chef cookbooks. Choose AWS OpsWorks for Chef Automate if you are an existing Chef user. AWS OpsWorks for Puppet Enterprise AWS OpsWorks for Puppet Enterprise is a fully managed configuration management service that hosts Puppet Enterprise, a set of automation tools from Puppet for'),\n",
              " Document(metadata={}, page_content='set of automation tools from Puppet for infrastructure and application management. OpsWorks also maintains your Puppet master server by automatically patching, updating, and backing up your server. OpsWorks eliminates the need to operate your own configuration management systems or worry about'),\n",
              " Document(metadata={}, page_content='management systems or worry about maintaining its infrastructure. OpsWorks gives you access to all of the Puppet Enterprise features, which you manage through the Puppet console. It also works seamlessly with your existing Puppet code. Choose AWS OpsWorks for Puppet Enterprise if you are an'),\n",
              " Document(metadata={}, page_content='for Puppet Enterprise if you are an existing Puppet user. Unit-8 – Other AWS Services & Management Services 12 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS OpsWorks Stacks AWS OpsWorks Stacks is an application and server management service. With OpsWorks'),\n",
              " Document(metadata={}, page_content='management service. With OpsWorks Stacks, you can model your application as a stack containing different layers, such as load balancing, database, and application server. Within each layer, you can provision Amazon EC2 instances, enable automatic scaling, and configure your instances with Chef'),\n",
              " Document(metadata={}, page_content='and configure your instances with Chef recipes using Chef Solo. This allows you to automate tasks such as installing packages and programming languages or frameworks, configuring software, and more. Choose AWS OpsWorks Stacks if you need a solution for application modeling and management. OpenID'),\n",
              " Document(metadata={}, page_content='modeling and management. OpenID Connect (OIDC) IAM OIDC identity providers are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Salesforce. You use an IAM OIDC identity provider when you want to establish'),\n",
              " Document(metadata={}, page_content=\"provider when you want to establish trust between an OIDC-compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign-in code or manage your own user identities. You can create and\"),\n",
              " Document(metadata={}, page_content='own user identities. You can create and manage an IAM OIDC identity provider using the AWS Management Console, the AWS Command Line Interface, the Tools for Windows PowerShell, or the IAM API. When you create an OpenID Connect (OIDC) identity provider in IAM, you must supply a thumbprint. IAM'),\n",
              " Document(metadata={}, page_content=\"IAM, you must supply a thumbprint. IAM requires the thumbprint for the root certificate authority (CA) that signed the certificate used by the external identity provider (IdP). The thumbprint is a signature for the CA's certificate that was used to issue the certificate for the OIDC- compatible\"),\n",
              " Document(metadata={}, page_content=\"certificate for the OIDC- compatible IdP. When you create an IAM OIDC identity provider, you are trusting identities authenticated by that IdP to have access to your AWS account. By supplying the CA's certificate thumbprint, you trust any certificate issued by that CA with the same DNS name as the\"),\n",
              " Document(metadata={}, page_content=\"that CA with the same DNS name as the one registered. This eliminates the need to update trusts in each account when you renew the IdP's signing certificate. You can create an IAM OIDC identity provider with the AWS Command Line Interface, the Tools for Windows PowerShell, or the IAM API. When you\"),\n",
              " Document(metadata={}, page_content='PowerShell, or the IAM API. When you use these methods, you must obtain the thumbprint manually and supply it to AWS. When you create an OIDC identity provider with the IAM console, the console attempts to fetch the thumbprint for you. We recommend that you also obtain the thumbprint for your OIDC'),\n",
              " Document(metadata={}, page_content='obtain the thumbprint for your OIDC IdP manually and verify that the console fetched the correct thumbprint. Unit-9 – AWS Billing & Dealing with Disaster 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Managing Costs, Utilization and Tracking The cloud allows'),\n",
              " Document(metadata={}, page_content='and Tracking The cloud allows you to trade capital expenses (such as data centers and physical servers) for variable expenses, and only pay for IT as you consume it. And, because of the economies of scale, the variable expenses are much lower than what you would pay to do it yourself. Whether you'),\n",
              " Document(metadata={}, page_content='pay to do it yourself. Whether you were born in the cloud, or you are just starting your migration journey to the cloud, AWS has a set of solutions to help you manage and optimize your spend. During this unprecedented time, many businesses and organizations are facing disruption to their'),\n",
              " Document(metadata={}, page_content='are facing disruption to their operations, budgets, and revenue. AWS has a set of solutions to help you with cost management and optimization. This includes services, tools, and resources to organize and track cost and usage data, enhance control through consolidated billing and access permission,'),\n",
              " Document(metadata={}, page_content='billing and access permission, enable better planning through budgeting and forecasts, and further lower cost with resources and pricing optimizations. AWS Cost Management Solutions Organize and Report Cost and Usage Based on User-Defined Methods You need complete, near real-time visibility of your'),\n",
              " Document(metadata={}, page_content='near real-time visibility of your cost and usage information to make informed decisions. AWS equips you with tools to organize your resources based on your needs, visualize and analyze cost and usage data in a single pane of glass, and accurately chargeback to appropriate entities (e.g. department,'),\n",
              " Document(metadata={}, page_content='appropriate entities (e.g. department, project, and product). Rather than centrally policing the cost, you can provide real-time cost data that makes sense to your engineering, application, and business teams. The detailed, allocable cost data allows teams to have the visibility and details to be'),\n",
              " Document(metadata={}, page_content='have the visibility and details to be accountable of their own spend. Billing with Built-in Control Business and organization leaders need a simple and easy way to access AWS billing information, including a spend summary, a breakdown of all service costs incurred by accounts across the'),\n",
              " Document(metadata={}, page_content='costs incurred by accounts across the organization, along with discounts and credits. Customer can choose to consolidate your bills and take advantage of higher volume discounts based on aggregated usage across your bills. Leaders also need to set appropriate guardrails in place so you can maintain'),\n",
              " Document(metadata={}, page_content='guardrails in place so you can maintain control over cost, governance, and security. AWS helps organizations balance freedom and control by enabling the governance of granular user permission. Improved Planning with Flexible Forecasting and Budgeting Businesses and organizations need to plan and'),\n",
              " Document(metadata={}, page_content='and organizations need to plan and set expectations around cloud costs for your projects, applications, and more. The emergence of the cloud allowed teams to acquire and deprecate resources on an ongoing basis, without relying on teams to approve, procure and install infrastructure. However, this'),\n",
              " Document(metadata={}, page_content='install infrastructure. However, this flexibility requires organizations to adapt to the new, dynamic forecasting and budgeting process. Unit-9 – AWS Billing & Dealing with Disaster 2 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services AWS provides forecasts based on'),\n",
              " Document(metadata={}, page_content='AWS provides forecasts based on your cost and usage history and allows you to set budget threshold and alerts, so you can stay informed whenever cost and usage is forecasted to, or exceeds the threshold limit. You can also set reservation utilization and/or coverage targets for your Reserved'),\n",
              " Document(metadata={}, page_content='coverage targets for your Reserved Instances and Savings Plans and monitor how they are progressing towards your target. Optimize Costs with Resource and Pricing Recommendations With AWS, customers can take control of your cost and continuously optimize your spend. There are a variety of AWS'),\n",
              " Document(metadata={}, page_content='your spend. There are a variety of AWS pricing models and resources you can choose from to meet requirements for both performance and cost efficiency, and adjust as needed. When evaluating AWS services for your architectural and business needs, you will have the flexibility to choose from a variety'),\n",
              " Document(metadata={}, page_content='flexibility to choose from a variety of elements, such as operating systems, instance types, availability zones, and purchase options. AWS offers resources optimization recommendations to simplify the evaluation process so you can efficiently select the cost-optimized resources. We also provide'),\n",
              " Document(metadata={}, page_content='resources. We also provide recommendations around pricing models (up to 72% with Reserved Instances and Savings Plans and up to 90% with Spot Instances) based on your utilization patterns, so you can further drive down your cost without compromising workload performance. Monitor, Track, and Analyze'),\n",
              " Document(metadata={}, page_content='Monitor, Track, and Analyze Your AWS Costs & Usage Appropriate management, tracking and measurement are fundamental in achieving the full benefits of cost optimization. Amazon CloudWatch Amazon CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing'),\n",
              " Document(metadata={}, page_content='of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS and on- premises servers. AWS Trusted Advisor AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS'),\n",
              " Document(metadata={}, page_content='provision your resources following AWS best practices. AWS Cost Explorer AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. Bottom Line Impact As AWS provide large range of service and we can utilize it for our business'),\n",
              " Document(metadata={}, page_content='and we can utilize it for our business on pay as you go basis so it will save our cost and time. Due to that company can reduce their cost and increase revenue by focusing on core work and other service management is done by cloud providers. It will create bottom line impact for organization.'),\n",
              " Document(metadata={}, page_content='bottom line impact for organization. Geographic Concerns The AWS Global Cloud Infrastructure is the most secure, extensive, and reliable cloud platform, offering over 175 fully featured services from data centers globally. Unit-9 – AWS Billing & Dealing with Disaster 3 Prof. Vijay M. Shekhat, CE'),\n",
              " Document(metadata={}, page_content='Disaster 3 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Whether you need to deploy your application workloads across the globe in a single click, or you want to build and deploy specific applications closer to your end-users with single-digit millisecond'),\n",
              " Document(metadata={}, page_content='end-users with single-digit millisecond latency, AWS provides you the cloud infrastructure where and when you need it. With millions of active customers and tens of thousands of partners globally, AWS has the largest and most dynamic ecosystem. Customers across virtually every industry and of every'),\n",
              " Document(metadata={}, page_content='virtually every industry and of every size, including start-ups, enterprises, and public sector organizations, are running every imaginable use case on AWS. Failure plans / Disaster Recovery (DR) Our data is the most precious asset that we have and protecting it is our top priority. Creating'),\n",
              " Document(metadata={}, page_content='it is our top priority. Creating backups of our data to an off shore data center, so that in the event of an on premise failure we can switch over to our backup, is a prime focus for business continuity. As AWS says, ‘Disaster recovery is a continual process of analysis and improvement, as business'),\n",
              " Document(metadata={}, page_content='analysis and improvement, as business and systems evolve. For each business service, customers need to establish an acceptable recovery point and time, and then build an appropriate DR solution.’ Backup and DR on Cloud reduces costs by half as compared to maintaining your own redundant data'),\n",
              " Document(metadata={}, page_content='to maintaining your own redundant data centers. And if you think about it, it’s really not that surprising. Imagine the kind of cost you would entail in buying and maintaining servers and data centers, providing secure and stable connectivity and not to mention keeping them secure. You would also'),\n",
              " Document(metadata={}, page_content='keeping them secure. You would also be underutilizing severs; and in times of unpredictable traffic rise it would be strenuous to set up new ones. To all these cloud provides a seamless transition reducing cost dramatically. 4 Standard Approaches of Backup and Disaster Recovery Using Amazon Cloud'),\n",
              " Document(metadata={}, page_content='Disaster Recovery Using Amazon Cloud 1. Backup and Recovery To recover your data in the event of any disaster, you must first have your data periodically backed up from your system to AWS. Backing up of data can be done through various mechanisms and your choice will be based on the RPO (Recovery'),\n",
              " Document(metadata={}, page_content='will be based on the RPO (Recovery Point Objective- So if your disaster struck at 2 pm and your RPO is 1 hr, your Backup & DR will restore all data till 1 pm.) that will suit your business needs. AWS offers AWS Direct connect and Import Export services that allow for faster backup. For example, if'),\n",
              " Document(metadata={}, page_content='for faster backup. For example, if you have a frequently changing database like say a stock market, then you will need a very high RPO. However if your data is mostly static with a low frequency of changes, you can opt for periodic incremental backup. Once your backup mechanisms are activated you'),\n",
              " Document(metadata={}, page_content='backup mechanisms are activated you can pre-configure AMIs (operating systems & application software). Now when a disaster strikes, EC2 (Elastic Compute Capacity) instances in the Cloud using EBS (Elastic Block Store) coupled with AMIs can access your data from the S3 (Simple Storage Service)'),\n",
              " Document(metadata={}, page_content='from the S3 (Simple Storage Service) buckets to revive your system and keep it going. 2. Pilot Light Approach The name pilot light comes from the gas heater analogy. Just as in a heater you have a small flame that is always on, and can quickly ignite the entire furnace; a similar approach can be'),\n",
              " Document(metadata={}, page_content='furnace; a similar approach can be thought of about your data system. In the preparatory phase your on premise database server mirrors data to data volumes on AWS. The database server on cloud is always activated for frequent or continuous incremental backup. Unit-9 – AWS Billing & Dealing with'),\n",
              " Document(metadata={}, page_content='Unit-9 – AWS Billing & Dealing with Disaster 4 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services This core area is the pilot from our gas heater analogy. The application and caching server replica environments are created on cloud and kept in standby mode as very'),\n",
              " Document(metadata={}, page_content='cloud and kept in standby mode as very few changes take place over time. These AMIs can be updated periodically. This is the entire furnace from our example. If the on premise system fails, then the application and caching servers get activated; further users are rerouted using elastic IP addresses'),\n",
              " Document(metadata={}, page_content='are rerouted using elastic IP addresses to the ad hoc environment on cloud. Your Recovery takes just a few minutes. 3. Warm Standby Approach This Technique is the next level of the pilot light, reducing recovery time to almost zero. Your application and caching servers are set up and always'),\n",
              " Document(metadata={}, page_content='caching servers are set up and always activated based on your business critical activities but only a minimum sized fleet of EC2 instances are dedicated. The backup system is not capable of handling production load, but can be used for testing, quality assurance and other internal uses. In the'),\n",
              " Document(metadata={}, page_content='and other internal uses. In the event of a disaster, when your on premise data center fails, two things happen. Firstly multiple EC2 instances are dedicated (vertical and horizontal scaling) to bring your application and caching environment up to production load. ELB and Auto Scaling (for'),\n",
              " Document(metadata={}, page_content='load. ELB and Auto Scaling (for distributing traffic) are used to ease scaling up. Secondly using Amazon Route 53 user traffic is rerouted instantly using elastic IP addresses and there is instant recovery of your system with almost zero down time. 4. Multi-Site Approach Well this is the optimum'),\n",
              " Document(metadata={}, page_content='Approach Well this is the optimum technique in backup and DR and is the next step after warm standby. All activities in the preparatory stage are similar to a warm standby; except that AWS backup on Cloud is also used to handle some portions of the user traffic using Route 53. When a disaster'),\n",
              " Document(metadata={}, page_content='traffic using Route 53. When a disaster strikes, the rest of the traffic that was pointing to the on premise servers are rerouted to AWS and using auto scaling techniques multiple EC2 instances are deployed to handle full production capacity. You can further increase the availability of your'),\n",
              " Document(metadata={}, page_content='increase the availability of your multi-site solution by designing Multi-AZ architectures. Examining Logs It is necessary to examine the log files in order to locate an error code or other indication of the issue that your cluster experienced. It may take some investigative work to determine what'),\n",
              " Document(metadata={}, page_content='investigative work to determine what happened. Hadoop runs the work of the jobs in task attempts on various nodes in the cluster. Amazon EMR can initiate speculative task attempts, terminating the other task attempts that do not complete first. This generates significant activity that is logged to'),\n",
              " Document(metadata={}, page_content='significant activity that is logged to the controller, stderr and syslog log files as it happens. In addition, multiple tasks attempts are running simultaneously, but a log file can only display results linearly. Start by checking the bootstrap action logs for errors or unexpected configuration'),\n",
              " Document(metadata={}, page_content='for errors or unexpected configuration changes during the launch of the cluster. From there, look in the step logs to identify Hadoop jobs launched as part of a step with errors. Examine the Hadoop job logs to identify the failed task attempts. The task attempt log will contain details about what'),\n",
              " Document(metadata={}, page_content='log will contain details about what caused a task attempt to fail. References 1 Prof. Vijay M. Shekhat, CE Department | 2180712 – Cloud Infrastructure and Services Book 1. Cloud Computing Bible, Barrie Sosinsky, John Wiley & Sons, ISBN-13: 978-0470903568. 2. Mastering AWS Security, Albert Anthony,'),\n",
              " Document(metadata={}, page_content='Mastering AWS Security, Albert Anthony, Packt Publishing Ltd., ISBN 978-1-78829-372-3. 3. Amazon Web Services for Dummies, Bernard Golden, For Dummies, ISBN-13: 978- 1118571835. Websites 1. www.aws.amazon.com 2. www.docs.aws.amazon.com 3. www.bluepiit.com 4. www.inforisktoday.com 5.'),\n",
              " Document(metadata={}, page_content='4. www.inforisktoday.com 5. www.techno-pulse.com 6. www.exelanz.com 7. www.ibm.com 8. www.iarjset.com/upload/2017/july-17/IARJSET%2018.pdf 9. www.searchservervirtualization.techtarget.com 10. www.docs.eucalyptus.com 11. www.cloudacademy.com 12. www.searchaws.techtarget.com 13.'),\n",
              " Document(metadata={}, page_content='12. www.searchaws.techtarget.com 13. www.searchsecurity.techtarget.com 14. www.en.wikipedia.org/wiki/Cloud_computing_security 15. www.znetlive.com 16. www.en.wikipedia.org/wiki/Virtual_private_cloud 17. www.resource.onlinetech.com 18. www.globalknowledge.com 19.'),\n",
              " Document(metadata={}, page_content='18. www.globalknowledge.com 19. www.blog.blazeclan.com/4-approaches-backup-disaster-recovery-explained-amazon-cloud 20. www.zdnet.com/article/what-is-cloud-computing-everything-you-need-to-know-about-the-cloud 21. www.javatpoint.com/introduction-to-cloud-computing 22.'),\n",
              " Document(metadata={}, page_content='22. www.javatpoint.com/history-of-cloud-computing 23. www.allcloud.io/blog/6-cloud-computing-concerns-facing-2018 24. www.searchitchannel.techtarget.com/definition/cloud-marketplace 25. www.en.wikipedia.org/wiki/Amazon_Web_Services 26.'),\n",
              " Document(metadata={}, page_content='26. www.msystechnologies.com/blog/cloud-orchestration-everything-you-want-to-know 27. www.linuxacademy.com/blog/linux-academy/elasticity-cloud-computing 28. www.searchitchannel.techtarget.com/definition/Eucalyptus 29. www.geeksforgeeks.org/virtualization-cloud-computing-types 30.'),\n",
              " Document(metadata={}, page_content='30. www.cloudsearch.blogspot.com 31. www.simplilearn.com/tutorials/aws-tutorial/aws-iam 32. www.d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf 33. www.resources.intenseschool.com/amazon-aws-understanding-ec2-key-pairs-and-how-they-are-used- for-windows-and-linux-instances/ 34.'),\n",
              " Document(metadata={}, page_content='for-windows-and-linux-instances/ 34. www.pagely.com/blog/amazon-ec2/ 35. www.cloudflare.com/learning/cloud/what-is-multitenancy/ 36. www.hevodata.com/blog/amazon-redshift-pros-and-cons/')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=40,\n",
        "     length_function=len\n",
        "    )\n",
        "\n",
        "document = text_splitter.create_documents([cleaned_text])\n",
        "document\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUNCFT_Jfwej"
      },
      "source": [
        "# **Convert Text Chunks into Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "lxSfT8BChCUv",
        "outputId": "b833e58c-6707-47d9-d1cc-d751a054febf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_google_genai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-140ec577e10e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_google_genai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleGenerativeAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_google_genai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\",google_api_key=\"AIzaSyBW_ps9Mjmx kjvnqKQyDnj6FS3m9N-4sok\")\n",
        "embedding.client.timeout = 120"
      ],
      "metadata": {
        "id": "kczjZbQAiDRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "0yAJKw6V-KFr",
        "outputId": "8d0247c1-873b-49a9-cab4-f87ae6abaf55",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_chroma'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b05b69c68ac6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_chroma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_chroma'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain_chroma import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma.from_documents(document, embedding)"
      ],
      "metadata": {
        "id": "sUoVDKsii3RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwT69_uxld2U",
        "outputId": "1d29eb2a-be70-4385-f99e-71e1a870a349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7fdfce77a450>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw1rQiMv_Zn9"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ✅ Load SentenceTransformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ✅ Example documents\n",
        "documents = [\n",
        "    Document(page_content=\"This is the first document.\"),\n",
        "    Document(page_content=\"Here is another document.\"),\n",
        "    Document(page_content=\"ChromaDB is useful for storing vectors.\"),\n",
        "]\n",
        "\n",
        "# ✅ Generate embeddings for each document\n",
        "embeddings = [model.encode(doc.page_content).tolist() for doc in document]\n",
        "\n",
        "# ✅ Extract texts and metadata\n",
        "texts = [doc.page_content for doc in documents]\n",
        "metadatas = [{\"source\": f\"doc_{i}\"} for i in range(len(document))]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wdFxeKHmVy35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uvp13rGVZGBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}